{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21b541e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1d01a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd3cdd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 149855 entries, 0 to 149854\n",
      "Data columns (total 86 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   index              149855 non-null  int64  \n",
      " 1   ID                 149855 non-null  int64  \n",
      " 2   S15                149855 non-null  float64\n",
      " 3   S17                149855 non-null  float64\n",
      " 4   S13                149855 non-null  float64\n",
      " 5   S5                 149855 non-null  float64\n",
      " 6   S16                149855 non-null  float64\n",
      " 7   S19                149855 non-null  float64\n",
      " 8   S18                149855 non-null  float64\n",
      " 9   EQUIPMENT_FAILURE  149855 non-null  int64  \n",
      " 10  S8                 149855 non-null  float64\n",
      " 11  AGE_OF_EQUIPMENT   149855 non-null  int64  \n",
      " 12  TIME_SINCE_START   149855 non-null  float64\n",
      " 13  too_soon           149855 non-null  int32  \n",
      " 14  S15_mean           149855 non-null  float64\n",
      " 15  S17_mean           149855 non-null  float64\n",
      " 16  S13_mean           149855 non-null  float64\n",
      " 17  S5_mean            149855 non-null  float64\n",
      " 18  S16_mean           149855 non-null  float64\n",
      " 19  S19_mean           149855 non-null  float64\n",
      " 20  S18_mean           149855 non-null  float64\n",
      " 21  S8_mean            149855 non-null  float64\n",
      " 22  S15_median         149855 non-null  float64\n",
      " 23  S17_median         149855 non-null  float64\n",
      " 24  S13_median         149855 non-null  float64\n",
      " 25  S5_median          149855 non-null  float64\n",
      " 26  S16_median         149855 non-null  float64\n",
      " 27  S19_median         149855 non-null  float64\n",
      " 28  S18_median         149855 non-null  float64\n",
      " 29  S8_median          149855 non-null  float64\n",
      " 30  S15_max            149855 non-null  float64\n",
      " 31  S17_max            149855 non-null  float64\n",
      " 32  S13_max            149855 non-null  float64\n",
      " 33  S5_max             149855 non-null  float64\n",
      " 34  S16_max            149855 non-null  float64\n",
      " 35  S19_max            149855 non-null  float64\n",
      " 36  S18_max            149855 non-null  float64\n",
      " 37  S8_max             149855 non-null  float64\n",
      " 38  S15_min            149855 non-null  float64\n",
      " 39  S17_min            149855 non-null  float64\n",
      " 40  S13_min            149855 non-null  float64\n",
      " 41  S5_min             149855 non-null  float64\n",
      " 42  S16_min            149855 non-null  float64\n",
      " 43  S19_min            149855 non-null  float64\n",
      " 44  S18_min            149855 non-null  float64\n",
      " 45  S8_min             149855 non-null  float64\n",
      " 46  S15_chg            149855 non-null  float64\n",
      " 47  S17_chg            149855 non-null  float64\n",
      " 48  S13_chg            149855 non-null  float64\n",
      " 49  S5_chg             149855 non-null  float64\n",
      " 50  S16_chg            149855 non-null  float64\n",
      " 51  S19_chg            149855 non-null  float64\n",
      " 52  S18_chg            149855 non-null  float64\n",
      " 53  S8_chg             149855 non-null  float64\n",
      " 54  CLUSTER_A          149855 non-null  uint8  \n",
      " 55  CLUSTER_B          149855 non-null  uint8  \n",
      " 56  CLUSTER_C          149855 non-null  uint8  \n",
      " 57  CLUSTER_D          149855 non-null  uint8  \n",
      " 58  CLUSTER_E          149855 non-null  uint8  \n",
      " 59  CLUSTER_F          149855 non-null  uint8  \n",
      " 60  CLUSTER_G          149855 non-null  uint8  \n",
      " 61  CLUSTER_H          149855 non-null  uint8  \n",
      " 62  MV_I               149855 non-null  uint8  \n",
      " 63  MV_J               149855 non-null  uint8  \n",
      " 64  MV_K               149855 non-null  uint8  \n",
      " 65  MV_L               149855 non-null  uint8  \n",
      " 66  MV_M               149855 non-null  uint8  \n",
      " 67  MV_N               149855 non-null  uint8  \n",
      " 68  MV_O               149855 non-null  uint8  \n",
      " 69  MV_P               149855 non-null  uint8  \n",
      " 70  MN_Q               149855 non-null  uint8  \n",
      " 71  MN_R               149855 non-null  uint8  \n",
      " 72  MN_S               149855 non-null  uint8  \n",
      " 73  MN_T               149855 non-null  uint8  \n",
      " 74  MN_U               149855 non-null  uint8  \n",
      " 75  MN_V               149855 non-null  uint8  \n",
      " 76  MN_W               149855 non-null  uint8  \n",
      " 77  MN_X               149855 non-null  uint8  \n",
      " 78  MN_Y               149855 non-null  uint8  \n",
      " 79  MN_Z               149855 non-null  uint8  \n",
      " 80  WG_1               149855 non-null  uint8  \n",
      " 81  WG_2               149855 non-null  uint8  \n",
      " 82  WG_3               149855 non-null  uint8  \n",
      " 83  WG_4               149855 non-null  uint8  \n",
      " 84  TIME_TO_FAILURE    149855 non-null  float64\n",
      " 85  FAILURE_TARGET     149855 non-null  int32  \n",
      "dtypes: float64(50), int32(2), int64(4), uint8(30)\n",
      "memory usage: 67.2 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31cfe8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[x for x in df.columns if x not in ['FAILURE_TARGET','EQUIPMENT_FAILURE', 'TIME_TO_FAILURE']]]\n",
    "y = df[['FAILURE_TARGET']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00e59c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy().ravel()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4edfc6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.8)\n",
    "#Now since we want the valid and test size to be equal (10% each of overall data).\n",
    "test_size = 0.5\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_rem,y_rem, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "106929f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e8ade1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "def test(y_true,y_pred):\n",
    "    print(accuracy_score(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    print('tn',tn, 'fp',fp, 'fn',fn, 'tp',tp)\n",
    "    print('f1', f1_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e068897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.968968968968969\n",
      "[[14428     8]\n",
      " [  457    92]]\n",
      "tn 14428 fp 8 fn 457 tp 92\n",
      "f1 0.2835130970724192\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(gamma='auto')\n",
    "svc.fit(X_train_scaled,y_train)\n",
    "y_pred = svc.predict(X_val_scaled)\n",
    "test(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0532e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "X_oversampled, y_oversampled = resample(X_train_scaled[y_train == 1],\n",
    "                                        y_train[y_train == 1],\n",
    "                                        replace=True,\n",
    "                                        n_samples=X_train_scaled[y_train == 0].shape[0])\n",
    "X_balanced = np.vstack((X_train_scaled[y_train == 0], X_oversampled))\n",
    "y_balanced = np.hstack((y_train[y_train == 0], y_oversampled))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e6a4afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8894227560894228\n",
      "[[12791  1645]\n",
      " [   12   537]]\n",
      "tn 12791 fp 1645 fn 12 tp 537\n",
      "f1 0.3932625411937019\n"
     ]
    }
   ],
   "source": [
    "svc_balanced = SVC(gamma='auto')\n",
    "svc_balanced.fit(X_balanced[::2],y_balanced[::2])\n",
    "y_pred_balanced = svc_balanced.predict(X_val_scaled)\n",
    "test(y_val,y_pred_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c55c29d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9960627293960628\n",
      "[[14415    21]\n",
      " [   38   511]]\n",
      "tn 14415 fp 21 fn 38 tp 511\n",
      "f1 0.9454209065679926\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_balanced,y_balanced)\n",
    "y_pred_rfc = rfc.predict(X_val_scaled)\n",
    "test(y_val,y_pred_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1011937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9882549215882549\n",
      "[[14290   146]\n",
      " [   30   519]]\n",
      "tn 14290 fp 146 fn 30 tp 519\n",
      "f1 0.8550247116968698\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "nn = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(40, 20))\n",
    "nn.fit(X_balanced,y_balanced)\n",
    "y_pred_nn = nn.predict(X_val_scaled)\n",
    "test(y_val,y_pred_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e030209f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9804484185239557\n",
      "[[14126   263]\n",
      " [   30   567]]\n",
      "tn 14126 fp 263 fn 30 tp 567\n",
      "f1 0.7946741415557114\n"
     ]
    }
   ],
   "source": [
    "y_pred_nn = nn.predict(X_test_scaled)\n",
    "test(y_test,y_pred_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "511493c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9966635526491392\n",
      "[[14378    11]\n",
      " [   39   558]]\n",
      "tn 14378 fp 11 fn 39 tp 558\n",
      "f1 0.9571183533447685\n"
     ]
    }
   ],
   "source": [
    "y_pred_rfc = rfc.predict(X_test_scaled)\n",
    "test(y_test,y_pred_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "870f4611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "print(X_train_scaled.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2288e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##NEURAL NETWORK WITH PYTORCH\n",
    "n_input, n_hidden, n_out = X_train_scaled.shape[1], (40,20), 1, \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_input,n_hidden[0])\n",
    "        self.fc2 = nn.Linear(n_hidden[0], n_hidden[1])\n",
    "        self.fc3 = nn.Linear(n_hidden[1], n_out)\n",
    "        self.ReLU = nn.ReLU()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.ReLU(self.fc1(x))\n",
    "        x = self.ReLU(self.fc2(x))\n",
    "        x = self.ReLU(self.fc3(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return torch.flatten(x)#torch.reshape(x,(1,))\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "tensor_x = torch.Tensor(X_balanced) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(y_balanced)\n",
    "\n",
    "my_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "my_dataloader = DataLoader(my_dataset,batch_size=256, shuffle=True) # create your dataloader\n",
    "\n",
    "tensor_x = torch.Tensor(X_val_scaled) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(y_val)\n",
    "my_dataset = TensorDataset(tensor_x,tensor_y)\n",
    "validation_loader = DataLoader(my_dataset,batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "63da1919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "Epoch [1/50], Step [0/901], Loss: 0.4748\n",
      "Epoch [1/50], Step [10/901], Loss: 0.4529\n",
      "Epoch [1/50], Step [20/901], Loss: 0.4562\n",
      "Epoch [1/50], Step [30/901], Loss: 0.4891\n",
      "Epoch [1/50], Step [40/901], Loss: 0.4871\n",
      "Epoch [1/50], Step [50/901], Loss: 0.5055\n",
      "Epoch [1/50], Step [60/901], Loss: 0.4604\n",
      "Epoch [1/50], Step [70/901], Loss: 0.4891\n",
      "Epoch [1/50], Step [80/901], Loss: 0.4749\n",
      "Epoch [1/50], Step [90/901], Loss: 0.4829\n",
      "Epoch [1/50], Step [100/901], Loss: 0.4584\n",
      "Epoch [1/50], Step [110/901], Loss: 0.4791\n",
      "Epoch [1/50], Step [120/901], Loss: 0.4678\n",
      "Epoch [1/50], Step [130/901], Loss: 0.4900\n",
      "Epoch [1/50], Step [140/901], Loss: 0.4601\n",
      "Epoch [1/50], Step [150/901], Loss: 0.4484\n",
      "Epoch [1/50], Step [160/901], Loss: 0.4581\n",
      "Epoch [1/50], Step [170/901], Loss: 0.4681\n",
      "Epoch [1/50], Step [180/901], Loss: 0.4245\n",
      "Epoch [1/50], Step [190/901], Loss: 0.4360\n",
      "Epoch [1/50], Step [200/901], Loss: 0.5490\n",
      "Epoch [1/50], Step [210/901], Loss: 0.4222\n",
      "Epoch [1/50], Step [220/901], Loss: 0.4585\n",
      "Epoch [1/50], Step [230/901], Loss: 0.4609\n",
      "Epoch [1/50], Step [240/901], Loss: 0.4673\n",
      "Epoch [1/50], Step [250/901], Loss: 0.4508\n",
      "Epoch [1/50], Step [260/901], Loss: 0.4908\n",
      "Epoch [1/50], Step [270/901], Loss: 0.4381\n",
      "Epoch [1/50], Step [280/901], Loss: 0.4713\n",
      "Epoch [1/50], Step [290/901], Loss: 0.5100\n",
      "Epoch [1/50], Step [300/901], Loss: 0.4756\n",
      "Epoch [1/50], Step [310/901], Loss: 0.4338\n",
      "Epoch [1/50], Step [320/901], Loss: 0.4578\n",
      "Epoch [1/50], Step [330/901], Loss: 0.4511\n",
      "Epoch [1/50], Step [340/901], Loss: 0.4659\n",
      "Epoch [1/50], Step [350/901], Loss: 0.4551\n",
      "Epoch [1/50], Step [360/901], Loss: 0.4831\n",
      "Epoch [1/50], Step [370/901], Loss: 0.4278\n",
      "Epoch [1/50], Step [380/901], Loss: 0.5100\n",
      "Epoch [1/50], Step [390/901], Loss: 0.4386\n",
      "Epoch [1/50], Step [400/901], Loss: 0.4753\n",
      "Epoch [1/50], Step [410/901], Loss: 0.4569\n",
      "Epoch [1/50], Step [420/901], Loss: 0.4592\n",
      "Epoch [1/50], Step [430/901], Loss: 0.5044\n",
      "Epoch [1/50], Step [440/901], Loss: 0.4731\n",
      "Epoch [1/50], Step [450/901], Loss: 0.5231\n",
      "Epoch [1/50], Step [460/901], Loss: 0.4703\n",
      "Epoch [1/50], Step [470/901], Loss: 0.5018\n",
      "Epoch [1/50], Step [480/901], Loss: 0.4985\n",
      "Epoch [1/50], Step [490/901], Loss: 0.4704\n",
      "Epoch [1/50], Step [500/901], Loss: 0.4396\n",
      "Epoch [1/50], Step [510/901], Loss: 0.4438\n",
      "Epoch [1/50], Step [520/901], Loss: 0.4569\n",
      "Epoch [1/50], Step [530/901], Loss: 0.4804\n",
      "Epoch [1/50], Step [540/901], Loss: 0.5038\n",
      "Epoch [1/50], Step [550/901], Loss: 0.4620\n",
      "Epoch [1/50], Step [560/901], Loss: 0.4833\n",
      "Epoch [1/50], Step [570/901], Loss: 0.4994\n",
      "Epoch [1/50], Step [580/901], Loss: 0.4304\n",
      "Epoch [1/50], Step [590/901], Loss: 0.4599\n",
      "Epoch [1/50], Step [600/901], Loss: 0.4633\n",
      "Epoch [1/50], Step [610/901], Loss: 0.5134\n",
      "Epoch [1/50], Step [620/901], Loss: 0.4847\n",
      "Epoch [1/50], Step [630/901], Loss: 0.4795\n",
      "Epoch [1/50], Step [640/901], Loss: 0.4473\n",
      "Epoch [1/50], Step [650/901], Loss: 0.5139\n",
      "Epoch [1/50], Step [660/901], Loss: 0.4615\n",
      "Epoch [1/50], Step [670/901], Loss: 0.4706\n",
      "Epoch [1/50], Step [680/901], Loss: 0.4612\n",
      "Epoch [1/50], Step [690/901], Loss: 0.4436\n",
      "Epoch [1/50], Step [700/901], Loss: 0.5471\n",
      "Epoch [1/50], Step [710/901], Loss: 0.4330\n",
      "Epoch [1/50], Step [720/901], Loss: 0.4660\n",
      "Epoch [1/50], Step [730/901], Loss: 0.4289\n",
      "Epoch [1/50], Step [740/901], Loss: 0.4516\n",
      "Epoch [1/50], Step [750/901], Loss: 0.4457\n",
      "Epoch [1/50], Step [760/901], Loss: 0.4716\n",
      "Epoch [1/50], Step [770/901], Loss: 0.4398\n",
      "Epoch [1/50], Step [780/901], Loss: 0.4688\n",
      "Epoch [1/50], Step [790/901], Loss: 0.4842\n",
      "Epoch [1/50], Step [800/901], Loss: 0.4500\n",
      "Epoch [1/50], Step [810/901], Loss: 0.4510\n",
      "Epoch [1/50], Step [820/901], Loss: 0.4707\n",
      "Epoch [1/50], Step [830/901], Loss: 0.4365\n",
      "Epoch [1/50], Step [840/901], Loss: 0.4594\n",
      "Epoch [1/50], Step [850/901], Loss: 0.5020\n",
      "Epoch [1/50], Step [860/901], Loss: 0.4611\n",
      "Epoch [1/50], Step [870/901], Loss: 0.4752\n",
      "Epoch [1/50], Step [880/901], Loss: 0.4843\n",
      "Epoch [1/50], Step [890/901], Loss: 0.4380\n",
      "Epoch [1/50], Step [900/901], Loss: 0.4467\n",
      "\n",
      "train loss: 0.4681, train acc: 86.3879\n",
      "validation loss: 0.7113, validation acc: 95.5222\n",
      "\n",
      "Epoch 2\n",
      "\n",
      "Epoch [2/50], Step [0/901], Loss: 0.4426\n",
      "Epoch [2/50], Step [10/901], Loss: 0.4497\n",
      "Epoch [2/50], Step [20/901], Loss: 0.4614\n",
      "Epoch [2/50], Step [30/901], Loss: 0.4495\n",
      "Epoch [2/50], Step [40/901], Loss: 0.4706\n",
      "Epoch [2/50], Step [50/901], Loss: 0.4633\n",
      "Epoch [2/50], Step [60/901], Loss: 0.4539\n",
      "Epoch [2/50], Step [70/901], Loss: 0.4970\n",
      "Epoch [2/50], Step [80/901], Loss: 0.4894\n",
      "Epoch [2/50], Step [90/901], Loss: 0.4242\n",
      "Epoch [2/50], Step [100/901], Loss: 0.4924\n",
      "Epoch [2/50], Step [110/901], Loss: 0.4668\n",
      "Epoch [2/50], Step [120/901], Loss: 0.5056\n",
      "Epoch [2/50], Step [130/901], Loss: 0.4708\n",
      "Epoch [2/50], Step [140/901], Loss: 0.4250\n",
      "Epoch [2/50], Step [150/901], Loss: 0.4874\n",
      "Epoch [2/50], Step [160/901], Loss: 0.4611\n",
      "Epoch [2/50], Step [170/901], Loss: 0.4921\n",
      "Epoch [2/50], Step [180/901], Loss: 0.4919\n",
      "Epoch [2/50], Step [190/901], Loss: 0.4314\n",
      "Epoch [2/50], Step [200/901], Loss: 0.5202\n",
      "Epoch [2/50], Step [210/901], Loss: 0.4382\n",
      "Epoch [2/50], Step [220/901], Loss: 0.4887\n",
      "Epoch [2/50], Step [230/901], Loss: 0.4352\n",
      "Epoch [2/50], Step [240/901], Loss: 0.4378\n",
      "Epoch [2/50], Step [250/901], Loss: 0.4206\n",
      "Epoch [2/50], Step [260/901], Loss: 0.4809\n",
      "Epoch [2/50], Step [270/901], Loss: 0.4649\n",
      "Epoch [2/50], Step [280/901], Loss: 0.4433\n",
      "Epoch [2/50], Step [290/901], Loss: 0.4687\n",
      "Epoch [2/50], Step [300/901], Loss: 0.4952\n",
      "Epoch [2/50], Step [310/901], Loss: 0.4362\n",
      "Epoch [2/50], Step [320/901], Loss: 0.4251\n",
      "Epoch [2/50], Step [330/901], Loss: 0.4340\n",
      "Epoch [2/50], Step [340/901], Loss: 0.4508\n",
      "Epoch [2/50], Step [350/901], Loss: 0.4432\n",
      "Epoch [2/50], Step [360/901], Loss: 0.4704\n",
      "Epoch [2/50], Step [370/901], Loss: 0.4193\n",
      "Epoch [2/50], Step [380/901], Loss: 0.4799\n",
      "Epoch [2/50], Step [390/901], Loss: 0.4389\n",
      "Epoch [2/50], Step [400/901], Loss: 0.4393\n",
      "Epoch [2/50], Step [410/901], Loss: 0.4929\n",
      "Epoch [2/50], Step [420/901], Loss: 0.4407\n",
      "Epoch [2/50], Step [430/901], Loss: 0.4527\n",
      "Epoch [2/50], Step [440/901], Loss: 0.4775\n",
      "Epoch [2/50], Step [450/901], Loss: 0.4123\n",
      "Epoch [2/50], Step [460/901], Loss: 0.4432\n",
      "Epoch [2/50], Step [470/901], Loss: 0.5060\n",
      "Epoch [2/50], Step [480/901], Loss: 0.5063\n",
      "Epoch [2/50], Step [490/901], Loss: 0.4668\n",
      "Epoch [2/50], Step [500/901], Loss: 0.4713\n",
      "Epoch [2/50], Step [510/901], Loss: 0.4634\n",
      "Epoch [2/50], Step [520/901], Loss: 0.4433\n",
      "Epoch [2/50], Step [530/901], Loss: 0.4821\n",
      "Epoch [2/50], Step [540/901], Loss: 0.4959\n",
      "Epoch [2/50], Step [550/901], Loss: 0.4196\n",
      "Epoch [2/50], Step [560/901], Loss: 0.4480\n",
      "Epoch [2/50], Step [570/901], Loss: 0.4893\n",
      "Epoch [2/50], Step [580/901], Loss: 0.4784\n",
      "Epoch [2/50], Step [590/901], Loss: 0.4747\n",
      "Epoch [2/50], Step [600/901], Loss: 0.4872\n",
      "Epoch [2/50], Step [610/901], Loss: 0.4735\n",
      "Epoch [2/50], Step [620/901], Loss: 0.4492\n",
      "Epoch [2/50], Step [630/901], Loss: 0.4953\n",
      "Epoch [2/50], Step [640/901], Loss: 0.4343\n",
      "Epoch [2/50], Step [650/901], Loss: 0.4465\n",
      "Epoch [2/50], Step [660/901], Loss: 0.5292\n",
      "Epoch [2/50], Step [670/901], Loss: 0.4875\n",
      "Epoch [2/50], Step [680/901], Loss: 0.4777\n",
      "Epoch [2/50], Step [690/901], Loss: 0.4895\n",
      "Epoch [2/50], Step [700/901], Loss: 0.4723\n",
      "Epoch [2/50], Step [710/901], Loss: 0.4634\n",
      "Epoch [2/50], Step [720/901], Loss: 0.4837\n",
      "Epoch [2/50], Step [730/901], Loss: 0.4062\n",
      "Epoch [2/50], Step [740/901], Loss: 0.4501\n",
      "Epoch [2/50], Step [750/901], Loss: 0.4456\n",
      "Epoch [2/50], Step [760/901], Loss: 0.4885\n",
      "Epoch [2/50], Step [770/901], Loss: 0.4814\n",
      "Epoch [2/50], Step [780/901], Loss: 0.4444\n",
      "Epoch [2/50], Step [790/901], Loss: 0.3969\n",
      "Epoch [2/50], Step [800/901], Loss: 0.4978\n",
      "Epoch [2/50], Step [810/901], Loss: 0.4769\n",
      "Epoch [2/50], Step [820/901], Loss: 0.4385\n",
      "Epoch [2/50], Step [830/901], Loss: 0.4714\n",
      "Epoch [2/50], Step [840/901], Loss: 0.4492\n",
      "Epoch [2/50], Step [850/901], Loss: 0.4665\n",
      "Epoch [2/50], Step [860/901], Loss: 0.4656\n",
      "Epoch [2/50], Step [870/901], Loss: 0.4684\n",
      "Epoch [2/50], Step [880/901], Loss: 0.4936\n",
      "Epoch [2/50], Step [890/901], Loss: 0.4679\n",
      "Epoch [2/50], Step [900/901], Loss: 0.4470\n",
      "\n",
      "train loss: 0.4668, train acc: 86.6542\n",
      "validation loss: 0.7175, validation acc: 94.8882\n",
      "\n",
      "Epoch 3\n",
      "\n",
      "Epoch [3/50], Step [0/901], Loss: 0.4895\n",
      "Epoch [3/50], Step [10/901], Loss: 0.4908\n",
      "Epoch [3/50], Step [20/901], Loss: 0.5063\n",
      "Epoch [3/50], Step [30/901], Loss: 0.4534\n",
      "Epoch [3/50], Step [40/901], Loss: 0.4919\n",
      "Epoch [3/50], Step [50/901], Loss: 0.4415\n",
      "Epoch [3/50], Step [60/901], Loss: 0.4573\n",
      "Epoch [3/50], Step [70/901], Loss: 0.4445\n",
      "Epoch [3/50], Step [80/901], Loss: 0.4148\n",
      "Epoch [3/50], Step [90/901], Loss: 0.5027\n",
      "Epoch [3/50], Step [100/901], Loss: 0.4982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Step [110/901], Loss: 0.4733\n",
      "Epoch [3/50], Step [120/901], Loss: 0.4854\n",
      "Epoch [3/50], Step [130/901], Loss: 0.4830\n",
      "Epoch [3/50], Step [140/901], Loss: 0.4421\n",
      "Epoch [3/50], Step [150/901], Loss: 0.4635\n",
      "Epoch [3/50], Step [160/901], Loss: 0.4840\n",
      "Epoch [3/50], Step [170/901], Loss: 0.4605\n",
      "Epoch [3/50], Step [180/901], Loss: 0.4854\n",
      "Epoch [3/50], Step [190/901], Loss: 0.4442\n",
      "Epoch [3/50], Step [200/901], Loss: 0.4905\n",
      "Epoch [3/50], Step [210/901], Loss: 0.4401\n",
      "Epoch [3/50], Step [220/901], Loss: 0.4679\n",
      "Epoch [3/50], Step [230/901], Loss: 0.4473\n",
      "Epoch [3/50], Step [240/901], Loss: 0.5351\n",
      "Epoch [3/50], Step [250/901], Loss: 0.4518\n",
      "Epoch [3/50], Step [260/901], Loss: 0.4560\n",
      "Epoch [3/50], Step [270/901], Loss: 0.5060\n",
      "Epoch [3/50], Step [280/901], Loss: 0.4426\n",
      "Epoch [3/50], Step [290/901], Loss: 0.4562\n",
      "Epoch [3/50], Step [300/901], Loss: 0.5268\n",
      "Epoch [3/50], Step [310/901], Loss: 0.4224\n",
      "Epoch [3/50], Step [320/901], Loss: 0.4608\n",
      "Epoch [3/50], Step [330/901], Loss: 0.4834\n",
      "Epoch [3/50], Step [340/901], Loss: 0.4429\n",
      "Epoch [3/50], Step [350/901], Loss: 0.4943\n",
      "Epoch [3/50], Step [360/901], Loss: 0.4640\n",
      "Epoch [3/50], Step [370/901], Loss: 0.4545\n",
      "Epoch [3/50], Step [380/901], Loss: 0.4670\n",
      "Epoch [3/50], Step [390/901], Loss: 0.4911\n",
      "Epoch [3/50], Step [400/901], Loss: 0.4285\n",
      "Epoch [3/50], Step [410/901], Loss: 0.5065\n",
      "Epoch [3/50], Step [420/901], Loss: 0.4908\n",
      "Epoch [3/50], Step [430/901], Loss: 0.4429\n",
      "Epoch [3/50], Step [440/901], Loss: 0.4363\n",
      "Epoch [3/50], Step [450/901], Loss: 0.4825\n",
      "Epoch [3/50], Step [460/901], Loss: 0.4510\n",
      "Epoch [3/50], Step [470/901], Loss: 0.4800\n",
      "Epoch [3/50], Step [480/901], Loss: 0.5152\n",
      "Epoch [3/50], Step [490/901], Loss: 0.4717\n",
      "Epoch [3/50], Step [500/901], Loss: 0.4266\n",
      "Epoch [3/50], Step [510/901], Loss: 0.4438\n",
      "Epoch [3/50], Step [520/901], Loss: 0.4469\n",
      "Epoch [3/50], Step [530/901], Loss: 0.4726\n",
      "Epoch [3/50], Step [540/901], Loss: 0.4155\n",
      "Epoch [3/50], Step [550/901], Loss: 0.4398\n",
      "Epoch [3/50], Step [560/901], Loss: 0.4541\n",
      "Epoch [3/50], Step [570/901], Loss: 0.4555\n",
      "Epoch [3/50], Step [580/901], Loss: 0.4557\n",
      "Epoch [3/50], Step [590/901], Loss: 0.4494\n",
      "Epoch [3/50], Step [600/901], Loss: 0.4876\n",
      "Epoch [3/50], Step [610/901], Loss: 0.4781\n",
      "Epoch [3/50], Step [620/901], Loss: 0.4426\n",
      "Epoch [3/50], Step [630/901], Loss: 0.4160\n",
      "Epoch [3/50], Step [640/901], Loss: 0.5182\n",
      "Epoch [3/50], Step [650/901], Loss: 0.5018\n",
      "Epoch [3/50], Step [660/901], Loss: 0.4625\n",
      "Epoch [3/50], Step [670/901], Loss: 0.4752\n",
      "Epoch [3/50], Step [680/901], Loss: 0.4833\n",
      "Epoch [3/50], Step [690/901], Loss: 0.4192\n",
      "Epoch [3/50], Step [700/901], Loss: 0.4817\n",
      "Epoch [3/50], Step [710/901], Loss: 0.4425\n",
      "Epoch [3/50], Step [720/901], Loss: 0.4985\n",
      "Epoch [3/50], Step [730/901], Loss: 0.4731\n",
      "Epoch [3/50], Step [740/901], Loss: 0.3744\n",
      "Epoch [3/50], Step [750/901], Loss: 0.4716\n",
      "Epoch [3/50], Step [760/901], Loss: 0.4334\n",
      "Epoch [3/50], Step [770/901], Loss: 0.4834\n",
      "Epoch [3/50], Step [780/901], Loss: 0.4473\n",
      "Epoch [3/50], Step [790/901], Loss: 0.4306\n",
      "Epoch [3/50], Step [800/901], Loss: 0.4356\n",
      "Epoch [3/50], Step [810/901], Loss: 0.5069\n",
      "Epoch [3/50], Step [820/901], Loss: 0.5037\n",
      "Epoch [3/50], Step [830/901], Loss: 0.4499\n",
      "Epoch [3/50], Step [840/901], Loss: 0.4454\n",
      "Epoch [3/50], Step [850/901], Loss: 0.4656\n",
      "Epoch [3/50], Step [860/901], Loss: 0.4089\n",
      "Epoch [3/50], Step [870/901], Loss: 0.4611\n",
      "Epoch [3/50], Step [880/901], Loss: 0.4484\n",
      "Epoch [3/50], Step [890/901], Loss: 0.4698\n",
      "Epoch [3/50], Step [900/901], Loss: 0.4722\n",
      "\n",
      "train loss: 0.4653, train acc: 86.8588\n",
      "validation loss: 0.7144, validation acc: 96.2896\n",
      "\n",
      "Epoch 4\n",
      "\n",
      "Epoch [4/50], Step [0/901], Loss: 0.4533\n",
      "Epoch [4/50], Step [10/901], Loss: 0.4916\n",
      "Epoch [4/50], Step [20/901], Loss: 0.4476\n",
      "Epoch [4/50], Step [30/901], Loss: 0.4950\n",
      "Epoch [4/50], Step [40/901], Loss: 0.4286\n",
      "Epoch [4/50], Step [50/901], Loss: 0.4871\n",
      "Epoch [4/50], Step [60/901], Loss: 0.4714\n",
      "Epoch [4/50], Step [70/901], Loss: 0.4172\n",
      "Epoch [4/50], Step [80/901], Loss: 0.4125\n",
      "Epoch [4/50], Step [90/901], Loss: 0.5013\n",
      "Epoch [4/50], Step [100/901], Loss: 0.5025\n",
      "Epoch [4/50], Step [110/901], Loss: 0.4588\n",
      "Epoch [4/50], Step [120/901], Loss: 0.5088\n",
      "Epoch [4/50], Step [130/901], Loss: 0.5159\n",
      "Epoch [4/50], Step [140/901], Loss: 0.4726\n",
      "Epoch [4/50], Step [150/901], Loss: 0.4462\n",
      "Epoch [4/50], Step [160/901], Loss: 0.4181\n",
      "Epoch [4/50], Step [170/901], Loss: 0.4600\n",
      "Epoch [4/50], Step [180/901], Loss: 0.4288\n",
      "Epoch [4/50], Step [190/901], Loss: 0.4795\n",
      "Epoch [4/50], Step [200/901], Loss: 0.4596\n",
      "Epoch [4/50], Step [210/901], Loss: 0.5030\n",
      "Epoch [4/50], Step [220/901], Loss: 0.4793\n",
      "Epoch [4/50], Step [230/901], Loss: 0.4471\n",
      "Epoch [4/50], Step [240/901], Loss: 0.4542\n",
      "Epoch [4/50], Step [250/901], Loss: 0.4703\n",
      "Epoch [4/50], Step [260/901], Loss: 0.4392\n",
      "Epoch [4/50], Step [270/901], Loss: 0.4432\n",
      "Epoch [4/50], Step [280/901], Loss: 0.4652\n",
      "Epoch [4/50], Step [290/901], Loss: 0.4422\n",
      "Epoch [4/50], Step [300/901], Loss: 0.4623\n",
      "Epoch [4/50], Step [310/901], Loss: 0.4835\n",
      "Epoch [4/50], Step [320/901], Loss: 0.4468\n",
      "Epoch [4/50], Step [330/901], Loss: 0.4308\n",
      "Epoch [4/50], Step [340/901], Loss: 0.4494\n",
      "Epoch [4/50], Step [350/901], Loss: 0.4753\n",
      "Epoch [4/50], Step [360/901], Loss: 0.4558\n",
      "Epoch [4/50], Step [370/901], Loss: 0.4415\n",
      "Epoch [4/50], Step [380/901], Loss: 0.4742\n",
      "Epoch [4/50], Step [390/901], Loss: 0.4388\n",
      "Epoch [4/50], Step [400/901], Loss: 0.4294\n",
      "Epoch [4/50], Step [410/901], Loss: 0.4381\n",
      "Epoch [4/50], Step [420/901], Loss: 0.4855\n",
      "Epoch [4/50], Step [430/901], Loss: 0.4671\n",
      "Epoch [4/50], Step [440/901], Loss: 0.4052\n",
      "Epoch [4/50], Step [450/901], Loss: 0.4273\n",
      "Epoch [4/50], Step [460/901], Loss: 0.4369\n",
      "Epoch [4/50], Step [470/901], Loss: 0.4499\n",
      "Epoch [4/50], Step [480/901], Loss: 0.4286\n",
      "Epoch [4/50], Step [490/901], Loss: 0.4538\n",
      "Epoch [4/50], Step [500/901], Loss: 0.4744\n",
      "Epoch [4/50], Step [510/901], Loss: 0.4355\n",
      "Epoch [4/50], Step [520/901], Loss: 0.4498\n",
      "Epoch [4/50], Step [530/901], Loss: 0.4144\n",
      "Epoch [4/50], Step [540/901], Loss: 0.4243\n",
      "Epoch [4/50], Step [550/901], Loss: 0.4439\n",
      "Epoch [4/50], Step [560/901], Loss: 0.4746\n",
      "Epoch [4/50], Step [570/901], Loss: 0.4412\n",
      "Epoch [4/50], Step [580/901], Loss: 0.4663\n",
      "Epoch [4/50], Step [590/901], Loss: 0.4236\n",
      "Epoch [4/50], Step [600/901], Loss: 0.4725\n",
      "Epoch [4/50], Step [610/901], Loss: 0.4651\n",
      "Epoch [4/50], Step [620/901], Loss: 0.4919\n",
      "Epoch [4/50], Step [630/901], Loss: 0.4002\n",
      "Epoch [4/50], Step [640/901], Loss: 0.4609\n",
      "Epoch [4/50], Step [650/901], Loss: 0.5021\n",
      "Epoch [4/50], Step [660/901], Loss: 0.4584\n",
      "Epoch [4/50], Step [670/901], Loss: 0.4719\n",
      "Epoch [4/50], Step [680/901], Loss: 0.4551\n",
      "Epoch [4/50], Step [690/901], Loss: 0.4446\n",
      "Epoch [4/50], Step [700/901], Loss: 0.4292\n",
      "Epoch [4/50], Step [710/901], Loss: 0.5021\n",
      "Epoch [4/50], Step [720/901], Loss: 0.4974\n",
      "Epoch [4/50], Step [730/901], Loss: 0.4686\n",
      "Epoch [4/50], Step [740/901], Loss: 0.4117\n",
      "Epoch [4/50], Step [750/901], Loss: 0.4203\n",
      "Epoch [4/50], Step [760/901], Loss: 0.4170\n",
      "Epoch [4/50], Step [770/901], Loss: 0.5060\n",
      "Epoch [4/50], Step [780/901], Loss: 0.4376\n",
      "Epoch [4/50], Step [790/901], Loss: 0.5087\n",
      "Epoch [4/50], Step [800/901], Loss: 0.4611\n",
      "Epoch [4/50], Step [810/901], Loss: 0.4477\n",
      "Epoch [4/50], Step [820/901], Loss: 0.4161\n",
      "Epoch [4/50], Step [830/901], Loss: 0.4510\n",
      "Epoch [4/50], Step [840/901], Loss: 0.4445\n",
      "Epoch [4/50], Step [850/901], Loss: 0.4543\n",
      "Epoch [4/50], Step [860/901], Loss: 0.4820\n",
      "Epoch [4/50], Step [870/901], Loss: 0.4051\n",
      "Epoch [4/50], Step [880/901], Loss: 0.4834\n",
      "Epoch [4/50], Step [890/901], Loss: 0.5145\n",
      "Epoch [4/50], Step [900/901], Loss: 0.4831\n",
      "\n",
      "train loss: 0.4640, train acc: 86.9911\n",
      "validation loss: 0.7142, validation acc: 95.8559\n",
      "\n",
      "Epoch 5\n",
      "\n",
      "Epoch [5/50], Step [0/901], Loss: 0.4694\n",
      "Epoch [5/50], Step [10/901], Loss: 0.4411\n",
      "Epoch [5/50], Step [20/901], Loss: 0.4580\n",
      "Epoch [5/50], Step [30/901], Loss: 0.4616\n",
      "Epoch [5/50], Step [40/901], Loss: 0.4746\n",
      "Epoch [5/50], Step [50/901], Loss: 0.4528\n",
      "Epoch [5/50], Step [60/901], Loss: 0.4696\n",
      "Epoch [5/50], Step [70/901], Loss: 0.4490\n",
      "Epoch [5/50], Step [80/901], Loss: 0.4353\n",
      "Epoch [5/50], Step [90/901], Loss: 0.4767\n",
      "Epoch [5/50], Step [100/901], Loss: 0.4422\n",
      "Epoch [5/50], Step [110/901], Loss: 0.4513\n",
      "Epoch [5/50], Step [120/901], Loss: 0.4308\n",
      "Epoch [5/50], Step [130/901], Loss: 0.4930\n",
      "Epoch [5/50], Step [140/901], Loss: 0.4053\n",
      "Epoch [5/50], Step [150/901], Loss: 0.4420\n",
      "Epoch [5/50], Step [160/901], Loss: 0.4462\n",
      "Epoch [5/50], Step [170/901], Loss: 0.4587\n",
      "Epoch [5/50], Step [180/901], Loss: 0.4999\n",
      "Epoch [5/50], Step [190/901], Loss: 0.4634\n",
      "Epoch [5/50], Step [200/901], Loss: 0.4652\n",
      "Epoch [5/50], Step [210/901], Loss: 0.4563\n",
      "Epoch [5/50], Step [220/901], Loss: 0.4411\n",
      "Epoch [5/50], Step [230/901], Loss: 0.4370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Step [240/901], Loss: 0.4665\n",
      "Epoch [5/50], Step [250/901], Loss: 0.4426\n",
      "Epoch [5/50], Step [260/901], Loss: 0.4697\n",
      "Epoch [5/50], Step [270/901], Loss: 0.4888\n",
      "Epoch [5/50], Step [280/901], Loss: 0.4817\n",
      "Epoch [5/50], Step [290/901], Loss: 0.4990\n",
      "Epoch [5/50], Step [300/901], Loss: 0.4718\n",
      "Epoch [5/50], Step [310/901], Loss: 0.4511\n",
      "Epoch [5/50], Step [320/901], Loss: 0.4767\n",
      "Epoch [5/50], Step [330/901], Loss: 0.4651\n",
      "Epoch [5/50], Step [340/901], Loss: 0.4727\n",
      "Epoch [5/50], Step [350/901], Loss: 0.4750\n",
      "Epoch [5/50], Step [360/901], Loss: 0.4405\n",
      "Epoch [5/50], Step [370/901], Loss: 0.5234\n",
      "Epoch [5/50], Step [380/901], Loss: 0.4434\n",
      "Epoch [5/50], Step [390/901], Loss: 0.4206\n",
      "Epoch [5/50], Step [400/901], Loss: 0.4542\n",
      "Epoch [5/50], Step [410/901], Loss: 0.4721\n",
      "Epoch [5/50], Step [420/901], Loss: 0.5029\n",
      "Epoch [5/50], Step [430/901], Loss: 0.4272\n",
      "Epoch [5/50], Step [440/901], Loss: 0.4290\n",
      "Epoch [5/50], Step [450/901], Loss: 0.4584\n",
      "Epoch [5/50], Step [460/901], Loss: 0.4497\n",
      "Epoch [5/50], Step [470/901], Loss: 0.4655\n",
      "Epoch [5/50], Step [480/901], Loss: 0.4580\n",
      "Epoch [5/50], Step [490/901], Loss: 0.4604\n",
      "Epoch [5/50], Step [500/901], Loss: 0.4892\n",
      "Epoch [5/50], Step [510/901], Loss: 0.4253\n",
      "Epoch [5/50], Step [520/901], Loss: 0.4306\n",
      "Epoch [5/50], Step [530/901], Loss: 0.4966\n",
      "Epoch [5/50], Step [540/901], Loss: 0.4366\n",
      "Epoch [5/50], Step [550/901], Loss: 0.4310\n",
      "Epoch [5/50], Step [560/901], Loss: 0.4288\n",
      "Epoch [5/50], Step [570/901], Loss: 0.4132\n",
      "Epoch [5/50], Step [580/901], Loss: 0.4517\n",
      "Epoch [5/50], Step [590/901], Loss: 0.4651\n",
      "Epoch [5/50], Step [600/901], Loss: 0.4873\n",
      "Epoch [5/50], Step [610/901], Loss: 0.3793\n",
      "Epoch [5/50], Step [620/901], Loss: 0.4704\n",
      "Epoch [5/50], Step [630/901], Loss: 0.4748\n",
      "Epoch [5/50], Step [640/901], Loss: 0.5003\n",
      "Epoch [5/50], Step [650/901], Loss: 0.4951\n",
      "Epoch [5/50], Step [660/901], Loss: 0.4167\n",
      "Epoch [5/50], Step [670/901], Loss: 0.4615\n",
      "Epoch [5/50], Step [680/901], Loss: 0.4781\n",
      "Epoch [5/50], Step [690/901], Loss: 0.4752\n",
      "Epoch [5/50], Step [700/901], Loss: 0.4624\n",
      "Epoch [5/50], Step [710/901], Loss: 0.4559\n",
      "Epoch [5/50], Step [720/901], Loss: 0.4182\n",
      "Epoch [5/50], Step [730/901], Loss: 0.4720\n",
      "Epoch [5/50], Step [740/901], Loss: 0.4752\n",
      "Epoch [5/50], Step [750/901], Loss: 0.5006\n",
      "Epoch [5/50], Step [760/901], Loss: 0.4403\n",
      "Epoch [5/50], Step [770/901], Loss: 0.4951\n",
      "Epoch [5/50], Step [780/901], Loss: 0.4335\n",
      "Epoch [5/50], Step [790/901], Loss: 0.4606\n",
      "Epoch [5/50], Step [800/901], Loss: 0.4454\n",
      "Epoch [5/50], Step [810/901], Loss: 0.4466\n",
      "Epoch [5/50], Step [820/901], Loss: 0.5016\n",
      "Epoch [5/50], Step [830/901], Loss: 0.5142\n",
      "Epoch [5/50], Step [840/901], Loss: 0.4181\n",
      "Epoch [5/50], Step [850/901], Loss: 0.5146\n",
      "Epoch [5/50], Step [860/901], Loss: 0.4849\n",
      "Epoch [5/50], Step [870/901], Loss: 0.4571\n",
      "Epoch [5/50], Step [880/901], Loss: 0.4009\n",
      "Epoch [5/50], Step [890/901], Loss: 0.4773\n",
      "Epoch [5/50], Step [900/901], Loss: 0.4523\n",
      "\n",
      "train loss: 0.4630, train acc: 87.0336\n",
      "validation loss: 0.7148, validation acc: 95.8292\n",
      "\n",
      "Epoch 6\n",
      "\n",
      "Epoch [6/50], Step [0/901], Loss: 0.4139\n",
      "Epoch [6/50], Step [10/901], Loss: 0.4503\n",
      "Epoch [6/50], Step [20/901], Loss: 0.4160\n",
      "Epoch [6/50], Step [30/901], Loss: 0.4360\n",
      "Epoch [6/50], Step [40/901], Loss: 0.4518\n",
      "Epoch [6/50], Step [50/901], Loss: 0.4767\n",
      "Epoch [6/50], Step [60/901], Loss: 0.4456\n",
      "Epoch [6/50], Step [70/901], Loss: 0.4572\n",
      "Epoch [6/50], Step [80/901], Loss: 0.4785\n",
      "Epoch [6/50], Step [90/901], Loss: 0.4790\n",
      "Epoch [6/50], Step [100/901], Loss: 0.5257\n",
      "Epoch [6/50], Step [110/901], Loss: 0.4625\n",
      "Epoch [6/50], Step [120/901], Loss: 0.4284\n",
      "Epoch [6/50], Step [130/901], Loss: 0.4876\n",
      "Epoch [6/50], Step [140/901], Loss: 0.4372\n",
      "Epoch [6/50], Step [150/901], Loss: 0.4581\n",
      "Epoch [6/50], Step [160/901], Loss: 0.4199\n",
      "Epoch [6/50], Step [170/901], Loss: 0.4119\n",
      "Epoch [6/50], Step [180/901], Loss: 0.4396\n",
      "Epoch [6/50], Step [190/901], Loss: 0.5087\n",
      "Epoch [6/50], Step [200/901], Loss: 0.4433\n",
      "Epoch [6/50], Step [210/901], Loss: 0.4862\n",
      "Epoch [6/50], Step [220/901], Loss: 0.4829\n",
      "Epoch [6/50], Step [230/901], Loss: 0.4468\n",
      "Epoch [6/50], Step [240/901], Loss: 0.4923\n",
      "Epoch [6/50], Step [250/901], Loss: 0.4532\n",
      "Epoch [6/50], Step [260/901], Loss: 0.4478\n",
      "Epoch [6/50], Step [270/901], Loss: 0.4608\n",
      "Epoch [6/50], Step [280/901], Loss: 0.4633\n",
      "Epoch [6/50], Step [290/901], Loss: 0.4263\n",
      "Epoch [6/50], Step [300/901], Loss: 0.4614\n",
      "Epoch [6/50], Step [310/901], Loss: 0.4559\n",
      "Epoch [6/50], Step [320/901], Loss: 0.4617\n",
      "Epoch [6/50], Step [330/901], Loss: 0.4293\n",
      "Epoch [6/50], Step [340/901], Loss: 0.4614\n",
      "Epoch [6/50], Step [350/901], Loss: 0.4847\n",
      "Epoch [6/50], Step [360/901], Loss: 0.5146\n",
      "Epoch [6/50], Step [370/901], Loss: 0.4340\n",
      "Epoch [6/50], Step [380/901], Loss: 0.4923\n",
      "Epoch [6/50], Step [390/901], Loss: 0.4796\n",
      "Epoch [6/50], Step [400/901], Loss: 0.4754\n",
      "Epoch [6/50], Step [410/901], Loss: 0.4301\n",
      "Epoch [6/50], Step [420/901], Loss: 0.4178\n",
      "Epoch [6/50], Step [430/901], Loss: 0.4383\n",
      "Epoch [6/50], Step [440/901], Loss: 0.3999\n",
      "Epoch [6/50], Step [450/901], Loss: 0.4734\n",
      "Epoch [6/50], Step [460/901], Loss: 0.4786\n",
      "Epoch [6/50], Step [470/901], Loss: 0.4723\n",
      "Epoch [6/50], Step [480/901], Loss: 0.4236\n",
      "Epoch [6/50], Step [490/901], Loss: 0.4557\n",
      "Epoch [6/50], Step [500/901], Loss: 0.4453\n",
      "Epoch [6/50], Step [510/901], Loss: 0.4428\n",
      "Epoch [6/50], Step [520/901], Loss: 0.4791\n",
      "Epoch [6/50], Step [530/901], Loss: 0.4449\n",
      "Epoch [6/50], Step [540/901], Loss: 0.4586\n",
      "Epoch [6/50], Step [550/901], Loss: 0.4655\n",
      "Epoch [6/50], Step [560/901], Loss: 0.4619\n",
      "Epoch [6/50], Step [570/901], Loss: 0.4666\n",
      "Epoch [6/50], Step [580/901], Loss: 0.4403\n",
      "Epoch [6/50], Step [590/901], Loss: 0.4376\n",
      "Epoch [6/50], Step [600/901], Loss: 0.4071\n",
      "Epoch [6/50], Step [610/901], Loss: 0.4264\n",
      "Epoch [6/50], Step [620/901], Loss: 0.4261\n",
      "Epoch [6/50], Step [630/901], Loss: 0.4685\n",
      "Epoch [6/50], Step [640/901], Loss: 0.4464\n",
      "Epoch [6/50], Step [650/901], Loss: 0.4398\n",
      "Epoch [6/50], Step [660/901], Loss: 0.4039\n",
      "Epoch [6/50], Step [670/901], Loss: 0.4141\n",
      "Epoch [6/50], Step [680/901], Loss: 0.4612\n",
      "Epoch [6/50], Step [690/901], Loss: 0.4183\n",
      "Epoch [6/50], Step [700/901], Loss: 0.4679\n",
      "Epoch [6/50], Step [710/901], Loss: 0.4450\n",
      "Epoch [6/50], Step [720/901], Loss: 0.4570\n",
      "Epoch [6/50], Step [730/901], Loss: 0.4859\n",
      "Epoch [6/50], Step [740/901], Loss: 0.4534\n",
      "Epoch [6/50], Step [750/901], Loss: 0.4109\n",
      "Epoch [6/50], Step [760/901], Loss: 0.4201\n",
      "Epoch [6/50], Step [770/901], Loss: 0.4584\n",
      "Epoch [6/50], Step [780/901], Loss: 0.4404\n",
      "Epoch [6/50], Step [790/901], Loss: 0.4713\n",
      "Epoch [6/50], Step [800/901], Loss: 0.4504\n",
      "Epoch [6/50], Step [810/901], Loss: 0.4837\n",
      "Epoch [6/50], Step [820/901], Loss: 0.4287\n",
      "Epoch [6/50], Step [830/901], Loss: 0.4582\n",
      "Epoch [6/50], Step [840/901], Loss: 0.4417\n",
      "Epoch [6/50], Step [850/901], Loss: 0.4460\n",
      "Epoch [6/50], Step [860/901], Loss: 0.4863\n",
      "Epoch [6/50], Step [870/901], Loss: 0.4554\n",
      "Epoch [6/50], Step [880/901], Loss: 0.4704\n",
      "Epoch [6/50], Step [890/901], Loss: 0.5092\n",
      "Epoch [6/50], Step [900/901], Loss: 0.4613\n",
      "\n",
      "train loss: 0.4619, train acc: 87.1836\n",
      "validation loss: 0.7168, validation acc: 95.2286\n",
      "\n",
      "Epoch 7\n",
      "\n",
      "Epoch [7/50], Step [0/901], Loss: 0.4499\n",
      "Epoch [7/50], Step [10/901], Loss: 0.4669\n",
      "Epoch [7/50], Step [20/901], Loss: 0.4896\n",
      "Epoch [7/50], Step [30/901], Loss: 0.4496\n",
      "Epoch [7/50], Step [40/901], Loss: 0.4133\n",
      "Epoch [7/50], Step [50/901], Loss: 0.4446\n",
      "Epoch [7/50], Step [60/901], Loss: 0.4727\n",
      "Epoch [7/50], Step [70/901], Loss: 0.4352\n",
      "Epoch [7/50], Step [80/901], Loss: 0.4288\n",
      "Epoch [7/50], Step [90/901], Loss: 0.4539\n",
      "Epoch [7/50], Step [100/901], Loss: 0.4584\n",
      "Epoch [7/50], Step [110/901], Loss: 0.4718\n",
      "Epoch [7/50], Step [120/901], Loss: 0.4060\n",
      "Epoch [7/50], Step [130/901], Loss: 0.4814\n",
      "Epoch [7/50], Step [140/901], Loss: 0.4630\n",
      "Epoch [7/50], Step [150/901], Loss: 0.4585\n",
      "Epoch [7/50], Step [160/901], Loss: 0.4782\n",
      "Epoch [7/50], Step [170/901], Loss: 0.4441\n",
      "Epoch [7/50], Step [180/901], Loss: 0.4594\n",
      "Epoch [7/50], Step [190/901], Loss: 0.4645\n",
      "Epoch [7/50], Step [200/901], Loss: 0.4643\n",
      "Epoch [7/50], Step [210/901], Loss: 0.4535\n",
      "Epoch [7/50], Step [220/901], Loss: 0.5342\n",
      "Epoch [7/50], Step [230/901], Loss: 0.5103\n",
      "Epoch [7/50], Step [240/901], Loss: 0.4813\n",
      "Epoch [7/50], Step [250/901], Loss: 0.4409\n",
      "Epoch [7/50], Step [260/901], Loss: 0.4572\n",
      "Epoch [7/50], Step [270/901], Loss: 0.4960\n",
      "Epoch [7/50], Step [280/901], Loss: 0.4578\n",
      "Epoch [7/50], Step [290/901], Loss: 0.4695\n",
      "Epoch [7/50], Step [300/901], Loss: 0.4585\n",
      "Epoch [7/50], Step [310/901], Loss: 0.4602\n",
      "Epoch [7/50], Step [320/901], Loss: 0.4524\n",
      "Epoch [7/50], Step [330/901], Loss: 0.4983\n",
      "Epoch [7/50], Step [340/901], Loss: 0.4685\n",
      "Epoch [7/50], Step [350/901], Loss: 0.4581\n",
      "Epoch [7/50], Step [360/901], Loss: 0.4989\n",
      "Epoch [7/50], Step [370/901], Loss: 0.4593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Step [380/901], Loss: 0.4486\n",
      "Epoch [7/50], Step [390/901], Loss: 0.4726\n",
      "Epoch [7/50], Step [400/901], Loss: 0.4615\n",
      "Epoch [7/50], Step [410/901], Loss: 0.4656\n",
      "Epoch [7/50], Step [420/901], Loss: 0.4239\n",
      "Epoch [7/50], Step [430/901], Loss: 0.4402\n",
      "Epoch [7/50], Step [440/901], Loss: 0.4392\n",
      "Epoch [7/50], Step [450/901], Loss: 0.4644\n",
      "Epoch [7/50], Step [460/901], Loss: 0.4594\n",
      "Epoch [7/50], Step [470/901], Loss: 0.4275\n",
      "Epoch [7/50], Step [480/901], Loss: 0.4379\n",
      "Epoch [7/50], Step [490/901], Loss: 0.4471\n",
      "Epoch [7/50], Step [500/901], Loss: 0.4445\n",
      "Epoch [7/50], Step [510/901], Loss: 0.4759\n",
      "Epoch [7/50], Step [520/901], Loss: 0.4027\n",
      "Epoch [7/50], Step [530/901], Loss: 0.5723\n",
      "Epoch [7/50], Step [540/901], Loss: 0.5173\n",
      "Epoch [7/50], Step [550/901], Loss: 0.4918\n",
      "Epoch [7/50], Step [560/901], Loss: 0.4460\n",
      "Epoch [7/50], Step [570/901], Loss: 0.4081\n",
      "Epoch [7/50], Step [580/901], Loss: 0.4148\n",
      "Epoch [7/50], Step [590/901], Loss: 0.4390\n",
      "Epoch [7/50], Step [600/901], Loss: 0.4659\n",
      "Epoch [7/50], Step [610/901], Loss: 0.4597\n",
      "Epoch [7/50], Step [620/901], Loss: 0.4683\n",
      "Epoch [7/50], Step [630/901], Loss: 0.4541\n",
      "Epoch [7/50], Step [640/901], Loss: 0.4716\n",
      "Epoch [7/50], Step [650/901], Loss: 0.4590\n",
      "Epoch [7/50], Step [660/901], Loss: 0.4559\n",
      "Epoch [7/50], Step [670/901], Loss: 0.5176\n",
      "Epoch [7/50], Step [680/901], Loss: 0.4835\n",
      "Epoch [7/50], Step [690/901], Loss: 0.4237\n",
      "Epoch [7/50], Step [700/901], Loss: 0.4435\n",
      "Epoch [7/50], Step [710/901], Loss: 0.4739\n",
      "Epoch [7/50], Step [720/901], Loss: 0.5001\n",
      "Epoch [7/50], Step [730/901], Loss: 0.4432\n",
      "Epoch [7/50], Step [740/901], Loss: 0.4530\n",
      "Epoch [7/50], Step [750/901], Loss: 0.4362\n",
      "Epoch [7/50], Step [760/901], Loss: 0.3893\n",
      "Epoch [7/50], Step [770/901], Loss: 0.4490\n",
      "Epoch [7/50], Step [780/901], Loss: 0.4408\n",
      "Epoch [7/50], Step [790/901], Loss: 0.4758\n",
      "Epoch [7/50], Step [800/901], Loss: 0.4240\n",
      "Epoch [7/50], Step [810/901], Loss: 0.4665\n",
      "Epoch [7/50], Step [820/901], Loss: 0.4561\n",
      "Epoch [7/50], Step [830/901], Loss: 0.4463\n",
      "Epoch [7/50], Step [840/901], Loss: 0.4914\n",
      "Epoch [7/50], Step [850/901], Loss: 0.4447\n",
      "Epoch [7/50], Step [860/901], Loss: 0.4406\n",
      "Epoch [7/50], Step [870/901], Loss: 0.4565\n",
      "Epoch [7/50], Step [880/901], Loss: 0.4579\n",
      "Epoch [7/50], Step [890/901], Loss: 0.4687\n",
      "Epoch [7/50], Step [900/901], Loss: 0.4574\n",
      "\n",
      "train loss: 0.4611, train acc: 87.2087\n",
      "validation loss: 0.7165, validation acc: 96.0093\n",
      "\n",
      "Epoch 8\n",
      "\n",
      "Epoch [8/50], Step [0/901], Loss: 0.4686\n",
      "Epoch [8/50], Step [10/901], Loss: 0.4751\n",
      "Epoch [8/50], Step [20/901], Loss: 0.4676\n",
      "Epoch [8/50], Step [30/901], Loss: 0.4613\n",
      "Epoch [8/50], Step [40/901], Loss: 0.4583\n",
      "Epoch [8/50], Step [50/901], Loss: 0.4693\n",
      "Epoch [8/50], Step [60/901], Loss: 0.5060\n",
      "Epoch [8/50], Step [70/901], Loss: 0.4651\n",
      "Epoch [8/50], Step [80/901], Loss: 0.4738\n",
      "Epoch [8/50], Step [90/901], Loss: 0.4429\n",
      "Epoch [8/50], Step [100/901], Loss: 0.4692\n",
      "Epoch [8/50], Step [110/901], Loss: 0.4460\n",
      "Epoch [8/50], Step [120/901], Loss: 0.4291\n",
      "Epoch [8/50], Step [130/901], Loss: 0.4599\n",
      "Epoch [8/50], Step [140/901], Loss: 0.4634\n",
      "Epoch [8/50], Step [150/901], Loss: 0.4356\n",
      "Epoch [8/50], Step [160/901], Loss: 0.4425\n",
      "Epoch [8/50], Step [170/901], Loss: 0.4908\n",
      "Epoch [8/50], Step [180/901], Loss: 0.4298\n",
      "Epoch [8/50], Step [190/901], Loss: 0.4253\n",
      "Epoch [8/50], Step [200/901], Loss: 0.4630\n",
      "Epoch [8/50], Step [210/901], Loss: 0.4233\n",
      "Epoch [8/50], Step [220/901], Loss: 0.4432\n",
      "Epoch [8/50], Step [230/901], Loss: 0.4446\n",
      "Epoch [8/50], Step [240/901], Loss: 0.4568\n",
      "Epoch [8/50], Step [250/901], Loss: 0.4309\n",
      "Epoch [8/50], Step [260/901], Loss: 0.4080\n",
      "Epoch [8/50], Step [270/901], Loss: 0.4742\n",
      "Epoch [8/50], Step [280/901], Loss: 0.4662\n",
      "Epoch [8/50], Step [290/901], Loss: 0.4173\n",
      "Epoch [8/50], Step [300/901], Loss: 0.4449\n",
      "Epoch [8/50], Step [310/901], Loss: 0.4322\n",
      "Epoch [8/50], Step [320/901], Loss: 0.4571\n",
      "Epoch [8/50], Step [330/901], Loss: 0.4459\n",
      "Epoch [8/50], Step [340/901], Loss: 0.4533\n",
      "Epoch [8/50], Step [350/901], Loss: 0.4816\n",
      "Epoch [8/50], Step [360/901], Loss: 0.4774\n",
      "Epoch [8/50], Step [370/901], Loss: 0.4388\n",
      "Epoch [8/50], Step [380/901], Loss: 0.4607\n",
      "Epoch [8/50], Step [390/901], Loss: 0.4498\n",
      "Epoch [8/50], Step [400/901], Loss: 0.4628\n",
      "Epoch [8/50], Step [410/901], Loss: 0.4777\n",
      "Epoch [8/50], Step [420/901], Loss: 0.4774\n",
      "Epoch [8/50], Step [430/901], Loss: 0.4526\n",
      "Epoch [8/50], Step [440/901], Loss: 0.4589\n",
      "Epoch [8/50], Step [450/901], Loss: 0.4546\n",
      "Epoch [8/50], Step [460/901], Loss: 0.3812\n",
      "Epoch [8/50], Step [470/901], Loss: 0.4570\n",
      "Epoch [8/50], Step [480/901], Loss: 0.4630\n",
      "Epoch [8/50], Step [490/901], Loss: 0.4280\n",
      "Epoch [8/50], Step [500/901], Loss: 0.4811\n",
      "Epoch [8/50], Step [510/901], Loss: 0.4476\n",
      "Epoch [8/50], Step [520/901], Loss: 0.4029\n",
      "Epoch [8/50], Step [530/901], Loss: 0.4409\n",
      "Epoch [8/50], Step [540/901], Loss: 0.4594\n",
      "Epoch [8/50], Step [550/901], Loss: 0.4576\n",
      "Epoch [8/50], Step [560/901], Loss: 0.4493\n",
      "Epoch [8/50], Step [570/901], Loss: 0.4369\n",
      "Epoch [8/50], Step [580/901], Loss: 0.4738\n",
      "Epoch [8/50], Step [590/901], Loss: 0.4463\n",
      "Epoch [8/50], Step [600/901], Loss: 0.4452\n",
      "Epoch [8/50], Step [610/901], Loss: 0.4280\n",
      "Epoch [8/50], Step [620/901], Loss: 0.4578\n",
      "Epoch [8/50], Step [630/901], Loss: 0.4281\n",
      "Epoch [8/50], Step [640/901], Loss: 0.4763\n",
      "Epoch [8/50], Step [650/901], Loss: 0.4282\n",
      "Epoch [8/50], Step [660/901], Loss: 0.4375\n",
      "Epoch [8/50], Step [670/901], Loss: 0.4148\n",
      "Epoch [8/50], Step [680/901], Loss: 0.4402\n",
      "Epoch [8/50], Step [690/901], Loss: 0.4506\n",
      "Epoch [8/50], Step [700/901], Loss: 0.4429\n",
      "Epoch [8/50], Step [710/901], Loss: 0.4620\n",
      "Epoch [8/50], Step [720/901], Loss: 0.4659\n",
      "Epoch [8/50], Step [730/901], Loss: 0.4422\n",
      "Epoch [8/50], Step [740/901], Loss: 0.4668\n",
      "Epoch [8/50], Step [750/901], Loss: 0.4313\n",
      "Epoch [8/50], Step [760/901], Loss: 0.4464\n",
      "Epoch [8/50], Step [770/901], Loss: 0.4567\n",
      "Epoch [8/50], Step [780/901], Loss: 0.5027\n",
      "Epoch [8/50], Step [790/901], Loss: 0.4502\n",
      "Epoch [8/50], Step [800/901], Loss: 0.4677\n",
      "Epoch [8/50], Step [810/901], Loss: 0.4960\n",
      "Epoch [8/50], Step [820/901], Loss: 0.4276\n",
      "Epoch [8/50], Step [830/901], Loss: 0.4481\n",
      "Epoch [8/50], Step [840/901], Loss: 0.4545\n",
      "Epoch [8/50], Step [850/901], Loss: 0.4711\n",
      "Epoch [8/50], Step [860/901], Loss: 0.4574\n",
      "Epoch [8/50], Step [870/901], Loss: 0.4098\n",
      "Epoch [8/50], Step [880/901], Loss: 0.4410\n",
      "Epoch [8/50], Step [890/901], Loss: 0.4868\n",
      "Epoch [8/50], Step [900/901], Loss: 0.4369\n",
      "\n",
      "train loss: 0.4603, train acc: 87.3076\n",
      "validation loss: 0.7151, validation acc: 96.6700\n",
      "\n",
      "Epoch 9\n",
      "\n",
      "Epoch [9/50], Step [0/901], Loss: 0.4517\n",
      "Epoch [9/50], Step [10/901], Loss: 0.4251\n",
      "Epoch [9/50], Step [20/901], Loss: 0.4797\n",
      "Epoch [9/50], Step [30/901], Loss: 0.4582\n",
      "Epoch [9/50], Step [40/901], Loss: 0.4439\n",
      "Epoch [9/50], Step [50/901], Loss: 0.4557\n",
      "Epoch [9/50], Step [60/901], Loss: 0.4369\n",
      "Epoch [9/50], Step [70/901], Loss: 0.4680\n",
      "Epoch [9/50], Step [80/901], Loss: 0.5077\n",
      "Epoch [9/50], Step [90/901], Loss: 0.4409\n",
      "Epoch [9/50], Step [100/901], Loss: 0.4263\n",
      "Epoch [9/50], Step [110/901], Loss: 0.4666\n",
      "Epoch [9/50], Step [120/901], Loss: 0.4665\n",
      "Epoch [9/50], Step [130/901], Loss: 0.4455\n",
      "Epoch [9/50], Step [140/901], Loss: 0.4817\n",
      "Epoch [9/50], Step [150/901], Loss: 0.4836\n",
      "Epoch [9/50], Step [160/901], Loss: 0.4334\n",
      "Epoch [9/50], Step [170/901], Loss: 0.4052\n",
      "Epoch [9/50], Step [180/901], Loss: 0.4499\n",
      "Epoch [9/50], Step [190/901], Loss: 0.4407\n",
      "Epoch [9/50], Step [200/901], Loss: 0.4591\n",
      "Epoch [9/50], Step [210/901], Loss: 0.4255\n",
      "Epoch [9/50], Step [220/901], Loss: 0.4778\n",
      "Epoch [9/50], Step [230/901], Loss: 0.4331\n",
      "Epoch [9/50], Step [240/901], Loss: 0.4287\n",
      "Epoch [9/50], Step [250/901], Loss: 0.4508\n",
      "Epoch [9/50], Step [260/901], Loss: 0.4888\n",
      "Epoch [9/50], Step [270/901], Loss: 0.4524\n",
      "Epoch [9/50], Step [280/901], Loss: 0.4384\n",
      "Epoch [9/50], Step [290/901], Loss: 0.4302\n",
      "Epoch [9/50], Step [300/901], Loss: 0.4420\n",
      "Epoch [9/50], Step [310/901], Loss: 0.4071\n",
      "Epoch [9/50], Step [320/901], Loss: 0.4903\n",
      "Epoch [9/50], Step [330/901], Loss: 0.4311\n",
      "Epoch [9/50], Step [340/901], Loss: 0.4579\n",
      "Epoch [9/50], Step [350/901], Loss: 0.4424\n",
      "Epoch [9/50], Step [360/901], Loss: 0.4596\n",
      "Epoch [9/50], Step [370/901], Loss: 0.4747\n",
      "Epoch [9/50], Step [380/901], Loss: 0.4285\n",
      "Epoch [9/50], Step [390/901], Loss: 0.4565\n",
      "Epoch [9/50], Step [400/901], Loss: 0.4764\n",
      "Epoch [9/50], Step [410/901], Loss: 0.4691\n",
      "Epoch [9/50], Step [420/901], Loss: 0.4965\n",
      "Epoch [9/50], Step [430/901], Loss: 0.4822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Step [440/901], Loss: 0.4868\n",
      "Epoch [9/50], Step [450/901], Loss: 0.4656\n",
      "Epoch [9/50], Step [460/901], Loss: 0.4189\n",
      "Epoch [9/50], Step [470/901], Loss: 0.4309\n",
      "Epoch [9/50], Step [480/901], Loss: 0.4526\n",
      "Epoch [9/50], Step [490/901], Loss: 0.4398\n",
      "Epoch [9/50], Step [500/901], Loss: 0.4353\n",
      "Epoch [9/50], Step [510/901], Loss: 0.4984\n",
      "Epoch [9/50], Step [520/901], Loss: 0.4079\n",
      "Epoch [9/50], Step [530/901], Loss: 0.4367\n",
      "Epoch [9/50], Step [540/901], Loss: 0.4702\n",
      "Epoch [9/50], Step [550/901], Loss: 0.4481\n",
      "Epoch [9/50], Step [560/901], Loss: 0.4814\n",
      "Epoch [9/50], Step [570/901], Loss: 0.4135\n",
      "Epoch [9/50], Step [580/901], Loss: 0.4794\n",
      "Epoch [9/50], Step [590/901], Loss: 0.4574\n",
      "Epoch [9/50], Step [600/901], Loss: 0.4111\n",
      "Epoch [9/50], Step [610/901], Loss: 0.4443\n",
      "Epoch [9/50], Step [620/901], Loss: 0.4618\n",
      "Epoch [9/50], Step [630/901], Loss: 0.4867\n",
      "Epoch [9/50], Step [640/901], Loss: 0.4821\n",
      "Epoch [9/50], Step [650/901], Loss: 0.4373\n",
      "Epoch [9/50], Step [660/901], Loss: 0.4486\n",
      "Epoch [9/50], Step [670/901], Loss: 0.4898\n",
      "Epoch [9/50], Step [680/901], Loss: 0.4273\n",
      "Epoch [9/50], Step [690/901], Loss: 0.4683\n",
      "Epoch [9/50], Step [700/901], Loss: 0.4567\n",
      "Epoch [9/50], Step [710/901], Loss: 0.4641\n",
      "Epoch [9/50], Step [720/901], Loss: 0.4380\n",
      "Epoch [9/50], Step [730/901], Loss: 0.4410\n",
      "Epoch [9/50], Step [740/901], Loss: 0.4940\n",
      "Epoch [9/50], Step [750/901], Loss: 0.5270\n",
      "Epoch [9/50], Step [760/901], Loss: 0.4158\n",
      "Epoch [9/50], Step [770/901], Loss: 0.4840\n",
      "Epoch [9/50], Step [780/901], Loss: 0.4611\n",
      "Epoch [9/50], Step [790/901], Loss: 0.4391\n",
      "Epoch [9/50], Step [800/901], Loss: 0.4340\n",
      "Epoch [9/50], Step [810/901], Loss: 0.4557\n",
      "Epoch [9/50], Step [820/901], Loss: 0.4722\n",
      "Epoch [9/50], Step [830/901], Loss: 0.4501\n",
      "Epoch [9/50], Step [840/901], Loss: 0.4745\n",
      "Epoch [9/50], Step [850/901], Loss: 0.4482\n",
      "Epoch [9/50], Step [860/901], Loss: 0.4263\n",
      "Epoch [9/50], Step [870/901], Loss: 0.4356\n",
      "Epoch [9/50], Step [880/901], Loss: 0.4277\n",
      "Epoch [9/50], Step [890/901], Loss: 0.4599\n",
      "Epoch [9/50], Step [900/901], Loss: 0.4328\n",
      "\n",
      "train loss: 0.4596, train acc: 87.3679\n",
      "validation loss: 0.7139, validation acc: 96.9570\n",
      "\n",
      "Epoch 10\n",
      "\n",
      "Epoch [10/50], Step [0/901], Loss: 0.4378\n",
      "Epoch [10/50], Step [10/901], Loss: 0.3857\n",
      "Epoch [10/50], Step [20/901], Loss: 0.4857\n",
      "Epoch [10/50], Step [30/901], Loss: 0.4242\n",
      "Epoch [10/50], Step [40/901], Loss: 0.4319\n",
      "Epoch [10/50], Step [50/901], Loss: 0.4831\n",
      "Epoch [10/50], Step [60/901], Loss: 0.4244\n",
      "Epoch [10/50], Step [70/901], Loss: 0.4594\n",
      "Epoch [10/50], Step [80/901], Loss: 0.4965\n",
      "Epoch [10/50], Step [90/901], Loss: 0.4418\n",
      "Epoch [10/50], Step [100/901], Loss: 0.4156\n",
      "Epoch [10/50], Step [110/901], Loss: 0.4285\n",
      "Epoch [10/50], Step [120/901], Loss: 0.3968\n",
      "Epoch [10/50], Step [130/901], Loss: 0.4237\n",
      "Epoch [10/50], Step [140/901], Loss: 0.4542\n",
      "Epoch [10/50], Step [150/901], Loss: 0.5238\n",
      "Epoch [10/50], Step [160/901], Loss: 0.4811\n",
      "Epoch [10/50], Step [170/901], Loss: 0.4208\n",
      "Epoch [10/50], Step [180/901], Loss: 0.4456\n",
      "Epoch [10/50], Step [190/901], Loss: 0.4190\n",
      "Epoch [10/50], Step [200/901], Loss: 0.4876\n",
      "Epoch [10/50], Step [210/901], Loss: 0.4301\n",
      "Epoch [10/50], Step [220/901], Loss: 0.4268\n",
      "Epoch [10/50], Step [230/901], Loss: 0.4458\n",
      "Epoch [10/50], Step [240/901], Loss: 0.4363\n",
      "Epoch [10/50], Step [250/901], Loss: 0.4678\n",
      "Epoch [10/50], Step [260/901], Loss: 0.5053\n",
      "Epoch [10/50], Step [270/901], Loss: 0.4560\n",
      "Epoch [10/50], Step [280/901], Loss: 0.4521\n",
      "Epoch [10/50], Step [290/901], Loss: 0.5057\n",
      "Epoch [10/50], Step [300/901], Loss: 0.4919\n",
      "Epoch [10/50], Step [310/901], Loss: 0.4810\n",
      "Epoch [10/50], Step [320/901], Loss: 0.4337\n",
      "Epoch [10/50], Step [330/901], Loss: 0.4461\n",
      "Epoch [10/50], Step [340/901], Loss: 0.4582\n",
      "Epoch [10/50], Step [350/901], Loss: 0.4320\n",
      "Epoch [10/50], Step [360/901], Loss: 0.4483\n",
      "Epoch [10/50], Step [370/901], Loss: 0.4343\n",
      "Epoch [10/50], Step [380/901], Loss: 0.4345\n",
      "Epoch [10/50], Step [390/901], Loss: 0.4587\n",
      "Epoch [10/50], Step [400/901], Loss: 0.4515\n",
      "Epoch [10/50], Step [410/901], Loss: 0.4565\n",
      "Epoch [10/50], Step [420/901], Loss: 0.4385\n",
      "Epoch [10/50], Step [430/901], Loss: 0.4267\n",
      "Epoch [10/50], Step [440/901], Loss: 0.4922\n",
      "Epoch [10/50], Step [450/901], Loss: 0.4456\n",
      "Epoch [10/50], Step [460/901], Loss: 0.4501\n",
      "Epoch [10/50], Step [470/901], Loss: 0.4349\n",
      "Epoch [10/50], Step [480/901], Loss: 0.4243\n",
      "Epoch [10/50], Step [490/901], Loss: 0.4519\n",
      "Epoch [10/50], Step [500/901], Loss: 0.4869\n",
      "Epoch [10/50], Step [510/901], Loss: 0.4559\n",
      "Epoch [10/50], Step [520/901], Loss: 0.4881\n",
      "Epoch [10/50], Step [530/901], Loss: 0.4648\n",
      "Epoch [10/50], Step [540/901], Loss: 0.4248\n",
      "Epoch [10/50], Step [550/901], Loss: 0.4678\n",
      "Epoch [10/50], Step [560/901], Loss: 0.4145\n",
      "Epoch [10/50], Step [570/901], Loss: 0.4824\n",
      "Epoch [10/50], Step [580/901], Loss: 0.4827\n",
      "Epoch [10/50], Step [590/901], Loss: 0.4377\n",
      "Epoch [10/50], Step [600/901], Loss: 0.4620\n",
      "Epoch [10/50], Step [610/901], Loss: 0.4345\n",
      "Epoch [10/50], Step [620/901], Loss: 0.4313\n",
      "Epoch [10/50], Step [630/901], Loss: 0.4456\n",
      "Epoch [10/50], Step [640/901], Loss: 0.4578\n",
      "Epoch [10/50], Step [650/901], Loss: 0.4491\n",
      "Epoch [10/50], Step [660/901], Loss: 0.4468\n",
      "Epoch [10/50], Step [670/901], Loss: 0.4813\n",
      "Epoch [10/50], Step [680/901], Loss: 0.4678\n",
      "Epoch [10/50], Step [690/901], Loss: 0.4639\n",
      "Epoch [10/50], Step [700/901], Loss: 0.4000\n",
      "Epoch [10/50], Step [710/901], Loss: 0.4574\n",
      "Epoch [10/50], Step [720/901], Loss: 0.4622\n",
      "Epoch [10/50], Step [730/901], Loss: 0.4557\n",
      "Epoch [10/50], Step [740/901], Loss: 0.4601\n",
      "Epoch [10/50], Step [750/901], Loss: 0.4233\n",
      "Epoch [10/50], Step [760/901], Loss: 0.4857\n",
      "Epoch [10/50], Step [770/901], Loss: 0.4425\n",
      "Epoch [10/50], Step [780/901], Loss: 0.4702\n",
      "Epoch [10/50], Step [790/901], Loss: 0.4380\n",
      "Epoch [10/50], Step [800/901], Loss: 0.4566\n",
      "Epoch [10/50], Step [810/901], Loss: 0.4003\n",
      "Epoch [10/50], Step [820/901], Loss: 0.4396\n",
      "Epoch [10/50], Step [830/901], Loss: 0.4495\n",
      "Epoch [10/50], Step [840/901], Loss: 0.4759\n",
      "Epoch [10/50], Step [850/901], Loss: 0.4468\n",
      "Epoch [10/50], Step [860/901], Loss: 0.4249\n",
      "Epoch [10/50], Step [870/901], Loss: 0.4573\n",
      "Epoch [10/50], Step [880/901], Loss: 0.4223\n",
      "Epoch [10/50], Step [890/901], Loss: 0.4387\n",
      "Epoch [10/50], Step [900/901], Loss: 0.4584\n",
      "\n",
      "train loss: 0.4590, train acc: 87.3193\n",
      "validation loss: 0.7133, validation acc: 96.7167\n",
      "\n",
      "Epoch 11\n",
      "\n",
      "Epoch [11/50], Step [0/901], Loss: 0.4238\n",
      "Epoch [11/50], Step [10/901], Loss: 0.4065\n",
      "Epoch [11/50], Step [20/901], Loss: 0.4724\n",
      "Epoch [11/50], Step [30/901], Loss: 0.4706\n",
      "Epoch [11/50], Step [40/901], Loss: 0.4439\n",
      "Epoch [11/50], Step [50/901], Loss: 0.4365\n",
      "Epoch [11/50], Step [60/901], Loss: 0.4572\n",
      "Epoch [11/50], Step [70/901], Loss: 0.4915\n",
      "Epoch [11/50], Step [80/901], Loss: 0.4610\n",
      "Epoch [11/50], Step [90/901], Loss: 0.4436\n",
      "Epoch [11/50], Step [100/901], Loss: 0.4524\n",
      "Epoch [11/50], Step [110/901], Loss: 0.4798\n",
      "Epoch [11/50], Step [120/901], Loss: 0.4360\n",
      "Epoch [11/50], Step [130/901], Loss: 0.4636\n",
      "Epoch [11/50], Step [140/901], Loss: 0.4498\n",
      "Epoch [11/50], Step [150/901], Loss: 0.4345\n",
      "Epoch [11/50], Step [160/901], Loss: 0.4690\n",
      "Epoch [11/50], Step [170/901], Loss: 0.4517\n",
      "Epoch [11/50], Step [180/901], Loss: 0.4540\n",
      "Epoch [11/50], Step [190/901], Loss: 0.4845\n",
      "Epoch [11/50], Step [200/901], Loss: 0.4477\n",
      "Epoch [11/50], Step [210/901], Loss: 0.4254\n",
      "Epoch [11/50], Step [220/901], Loss: 0.4329\n",
      "Epoch [11/50], Step [230/901], Loss: 0.4265\n",
      "Epoch [11/50], Step [240/901], Loss: 0.4442\n",
      "Epoch [11/50], Step [250/901], Loss: 0.4645\n",
      "Epoch [11/50], Step [260/901], Loss: 0.5051\n",
      "Epoch [11/50], Step [270/901], Loss: 0.4498\n",
      "Epoch [11/50], Step [280/901], Loss: 0.4577\n",
      "Epoch [11/50], Step [290/901], Loss: 0.4562\n",
      "Epoch [11/50], Step [300/901], Loss: 0.4316\n",
      "Epoch [11/50], Step [310/901], Loss: 0.4793\n",
      "Epoch [11/50], Step [320/901], Loss: 0.5164\n",
      "Epoch [11/50], Step [330/901], Loss: 0.4720\n",
      "Epoch [11/50], Step [340/901], Loss: 0.4274\n",
      "Epoch [11/50], Step [350/901], Loss: 0.4261\n",
      "Epoch [11/50], Step [360/901], Loss: 0.4160\n",
      "Epoch [11/50], Step [370/901], Loss: 0.4478\n",
      "Epoch [11/50], Step [380/901], Loss: 0.4271\n",
      "Epoch [11/50], Step [390/901], Loss: 0.4196\n",
      "Epoch [11/50], Step [400/901], Loss: 0.4816\n",
      "Epoch [11/50], Step [410/901], Loss: 0.4498\n",
      "Epoch [11/50], Step [420/901], Loss: 0.4247\n",
      "Epoch [11/50], Step [430/901], Loss: 0.4749\n",
      "Epoch [11/50], Step [440/901], Loss: 0.4282\n",
      "Epoch [11/50], Step [450/901], Loss: 0.4283\n",
      "Epoch [11/50], Step [460/901], Loss: 0.4884\n",
      "Epoch [11/50], Step [470/901], Loss: 0.4785\n",
      "Epoch [11/50], Step [480/901], Loss: 0.4416\n",
      "Epoch [11/50], Step [490/901], Loss: 0.4685\n",
      "Epoch [11/50], Step [500/901], Loss: 0.5106\n",
      "Epoch [11/50], Step [510/901], Loss: 0.4197\n",
      "Epoch [11/50], Step [520/901], Loss: 0.4546\n",
      "Epoch [11/50], Step [530/901], Loss: 0.4562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Step [540/901], Loss: 0.4755\n",
      "Epoch [11/50], Step [550/901], Loss: 0.4214\n",
      "Epoch [11/50], Step [560/901], Loss: 0.4420\n",
      "Epoch [11/50], Step [570/901], Loss: 0.5325\n",
      "Epoch [11/50], Step [580/901], Loss: 0.4406\n",
      "Epoch [11/50], Step [590/901], Loss: 0.4812\n",
      "Epoch [11/50], Step [600/901], Loss: 0.4973\n",
      "Epoch [11/50], Step [610/901], Loss: 0.4866\n",
      "Epoch [11/50], Step [620/901], Loss: 0.4680\n",
      "Epoch [11/50], Step [630/901], Loss: 0.4267\n",
      "Epoch [11/50], Step [640/901], Loss: 0.4570\n",
      "Epoch [11/50], Step [650/901], Loss: 0.4724\n",
      "Epoch [11/50], Step [660/901], Loss: 0.5304\n",
      "Epoch [11/50], Step [670/901], Loss: 0.5003\n",
      "Epoch [11/50], Step [680/901], Loss: 0.4230\n",
      "Epoch [11/50], Step [690/901], Loss: 0.4231\n",
      "Epoch [11/50], Step [700/901], Loss: 0.4660\n",
      "Epoch [11/50], Step [710/901], Loss: 0.4400\n",
      "Epoch [11/50], Step [720/901], Loss: 0.4422\n",
      "Epoch [11/50], Step [730/901], Loss: 0.4702\n",
      "Epoch [11/50], Step [740/901], Loss: 0.4496\n",
      "Epoch [11/50], Step [750/901], Loss: 0.4274\n",
      "Epoch [11/50], Step [760/901], Loss: 0.4391\n",
      "Epoch [11/50], Step [770/901], Loss: 0.4570\n",
      "Epoch [11/50], Step [780/901], Loss: 0.4278\n",
      "Epoch [11/50], Step [790/901], Loss: 0.4693\n",
      "Epoch [11/50], Step [800/901], Loss: 0.4397\n",
      "Epoch [11/50], Step [810/901], Loss: 0.4502\n",
      "Epoch [11/50], Step [820/901], Loss: 0.4496\n",
      "Epoch [11/50], Step [830/901], Loss: 0.4827\n",
      "Epoch [11/50], Step [840/901], Loss: 0.4489\n",
      "Epoch [11/50], Step [850/901], Loss: 0.4613\n",
      "Epoch [11/50], Step [860/901], Loss: 0.4387\n",
      "Epoch [11/50], Step [870/901], Loss: 0.4665\n",
      "Epoch [11/50], Step [880/901], Loss: 0.5029\n",
      "Epoch [11/50], Step [890/901], Loss: 0.4345\n",
      "Epoch [11/50], Step [900/901], Loss: 0.4679\n",
      "\n",
      "train loss: 0.4584, train acc: 87.5149\n",
      "validation loss: 0.7129, validation acc: 96.6567\n",
      "\n",
      "Epoch 12\n",
      "\n",
      "Epoch [12/50], Step [0/901], Loss: 0.4498\n",
      "Epoch [12/50], Step [10/901], Loss: 0.4635\n",
      "Epoch [12/50], Step [20/901], Loss: 0.4253\n",
      "Epoch [12/50], Step [30/901], Loss: 0.5191\n",
      "Epoch [12/50], Step [40/901], Loss: 0.4134\n",
      "Epoch [12/50], Step [50/901], Loss: 0.4164\n",
      "Epoch [12/50], Step [60/901], Loss: 0.4618\n",
      "Epoch [12/50], Step [70/901], Loss: 0.4304\n",
      "Epoch [12/50], Step [80/901], Loss: 0.4424\n",
      "Epoch [12/50], Step [90/901], Loss: 0.4446\n",
      "Epoch [12/50], Step [100/901], Loss: 0.4376\n",
      "Epoch [12/50], Step [110/901], Loss: 0.4338\n",
      "Epoch [12/50], Step [120/901], Loss: 0.4516\n",
      "Epoch [12/50], Step [130/901], Loss: 0.4713\n",
      "Epoch [12/50], Step [140/901], Loss: 0.4468\n",
      "Epoch [12/50], Step [150/901], Loss: 0.4562\n",
      "Epoch [12/50], Step [160/901], Loss: 0.4461\n",
      "Epoch [12/50], Step [170/901], Loss: 0.4477\n",
      "Epoch [12/50], Step [180/901], Loss: 0.4283\n",
      "Epoch [12/50], Step [190/901], Loss: 0.4426\n",
      "Epoch [12/50], Step [200/901], Loss: 0.4362\n",
      "Epoch [12/50], Step [210/901], Loss: 0.4558\n",
      "Epoch [12/50], Step [220/901], Loss: 0.4633\n",
      "Epoch [12/50], Step [230/901], Loss: 0.4561\n",
      "Epoch [12/50], Step [240/901], Loss: 0.4256\n",
      "Epoch [12/50], Step [250/901], Loss: 0.4348\n",
      "Epoch [12/50], Step [260/901], Loss: 0.4420\n",
      "Epoch [12/50], Step [270/901], Loss: 0.4115\n",
      "Epoch [12/50], Step [280/901], Loss: 0.4684\n",
      "Epoch [12/50], Step [290/901], Loss: 0.4905\n",
      "Epoch [12/50], Step [300/901], Loss: 0.4876\n",
      "Epoch [12/50], Step [310/901], Loss: 0.4295\n",
      "Epoch [12/50], Step [320/901], Loss: 0.3813\n",
      "Epoch [12/50], Step [330/901], Loss: 0.4335\n",
      "Epoch [12/50], Step [340/901], Loss: 0.5009\n",
      "Epoch [12/50], Step [350/901], Loss: 0.3933\n",
      "Epoch [12/50], Step [360/901], Loss: 0.4382\n",
      "Epoch [12/50], Step [370/901], Loss: 0.4535\n",
      "Epoch [12/50], Step [380/901], Loss: 0.4701\n",
      "Epoch [12/50], Step [390/901], Loss: 0.4295\n",
      "Epoch [12/50], Step [400/901], Loss: 0.4610\n",
      "Epoch [12/50], Step [410/901], Loss: 0.4527\n",
      "Epoch [12/50], Step [420/901], Loss: 0.4572\n",
      "Epoch [12/50], Step [430/901], Loss: 0.4337\n",
      "Epoch [12/50], Step [440/901], Loss: 0.4333\n",
      "Epoch [12/50], Step [450/901], Loss: 0.4380\n",
      "Epoch [12/50], Step [460/901], Loss: 0.4313\n",
      "Epoch [12/50], Step [470/901], Loss: 0.4793\n",
      "Epoch [12/50], Step [480/901], Loss: 0.4957\n",
      "Epoch [12/50], Step [490/901], Loss: 0.4545\n",
      "Epoch [12/50], Step [500/901], Loss: 0.4036\n",
      "Epoch [12/50], Step [510/901], Loss: 0.4348\n",
      "Epoch [12/50], Step [520/901], Loss: 0.4493\n",
      "Epoch [12/50], Step [530/901], Loss: 0.4669\n",
      "Epoch [12/50], Step [540/901], Loss: 0.4096\n",
      "Epoch [12/50], Step [550/901], Loss: 0.4741\n",
      "Epoch [12/50], Step [560/901], Loss: 0.4406\n",
      "Epoch [12/50], Step [570/901], Loss: 0.4364\n",
      "Epoch [12/50], Step [580/901], Loss: 0.4312\n",
      "Epoch [12/50], Step [590/901], Loss: 0.4594\n",
      "Epoch [12/50], Step [600/901], Loss: 0.4723\n",
      "Epoch [12/50], Step [610/901], Loss: 0.4214\n",
      "Epoch [12/50], Step [620/901], Loss: 0.4897\n",
      "Epoch [12/50], Step [630/901], Loss: 0.4498\n",
      "Epoch [12/50], Step [640/901], Loss: 0.3977\n",
      "Epoch [12/50], Step [650/901], Loss: 0.4526\n",
      "Epoch [12/50], Step [660/901], Loss: 0.4795\n",
      "Epoch [12/50], Step [670/901], Loss: 0.4420\n",
      "Epoch [12/50], Step [680/901], Loss: 0.3826\n",
      "Epoch [12/50], Step [690/901], Loss: 0.4154\n",
      "Epoch [12/50], Step [700/901], Loss: 0.4608\n",
      "Epoch [12/50], Step [710/901], Loss: 0.3554\n",
      "Epoch [12/50], Step [720/901], Loss: 0.4675\n",
      "Epoch [12/50], Step [730/901], Loss: 0.4245\n",
      "Epoch [12/50], Step [740/901], Loss: 0.4786\n",
      "Epoch [12/50], Step [750/901], Loss: 0.4360\n",
      "Epoch [12/50], Step [760/901], Loss: 0.4280\n",
      "Epoch [12/50], Step [770/901], Loss: 0.4391\n",
      "Epoch [12/50], Step [780/901], Loss: 0.4432\n",
      "Epoch [12/50], Step [790/901], Loss: 0.4160\n",
      "Epoch [12/50], Step [800/901], Loss: 0.4660\n",
      "Epoch [12/50], Step [810/901], Loss: 0.4419\n",
      "Epoch [12/50], Step [820/901], Loss: 0.4660\n",
      "Epoch [12/50], Step [830/901], Loss: 0.4319\n",
      "Epoch [12/50], Step [840/901], Loss: 0.4566\n",
      "Epoch [12/50], Step [850/901], Loss: 0.4059\n",
      "Epoch [12/50], Step [860/901], Loss: 0.4880\n",
      "Epoch [12/50], Step [870/901], Loss: 0.4530\n",
      "Epoch [12/50], Step [880/901], Loss: 0.4354\n",
      "Epoch [12/50], Step [890/901], Loss: 0.4529\n",
      "Epoch [12/50], Step [900/901], Loss: 0.4432\n",
      "\n",
      "train loss: 0.4577, train acc: 87.6081\n",
      "validation loss: 0.7142, validation acc: 95.9693\n",
      "\n",
      "Epoch 13\n",
      "\n",
      "Epoch [13/50], Step [0/901], Loss: 0.4619\n",
      "Epoch [13/50], Step [10/901], Loss: 0.4759\n",
      "Epoch [13/50], Step [20/901], Loss: 0.4406\n",
      "Epoch [13/50], Step [30/901], Loss: 0.4634\n",
      "Epoch [13/50], Step [40/901], Loss: 0.4128\n",
      "Epoch [13/50], Step [50/901], Loss: 0.4501\n",
      "Epoch [13/50], Step [60/901], Loss: 0.4653\n",
      "Epoch [13/50], Step [70/901], Loss: 0.4189\n",
      "Epoch [13/50], Step [80/901], Loss: 0.4594\n",
      "Epoch [13/50], Step [90/901], Loss: 0.4349\n",
      "Epoch [13/50], Step [100/901], Loss: 0.4363\n",
      "Epoch [13/50], Step [110/901], Loss: 0.4600\n",
      "Epoch [13/50], Step [120/901], Loss: 0.4325\n",
      "Epoch [13/50], Step [130/901], Loss: 0.4533\n",
      "Epoch [13/50], Step [140/901], Loss: 0.4204\n",
      "Epoch [13/50], Step [150/901], Loss: 0.4526\n",
      "Epoch [13/50], Step [160/901], Loss: 0.4281\n",
      "Epoch [13/50], Step [170/901], Loss: 0.4406\n",
      "Epoch [13/50], Step [180/901], Loss: 0.4758\n",
      "Epoch [13/50], Step [190/901], Loss: 0.5305\n",
      "Epoch [13/50], Step [200/901], Loss: 0.4321\n",
      "Epoch [13/50], Step [210/901], Loss: 0.4207\n",
      "Epoch [13/50], Step [220/901], Loss: 0.4155\n",
      "Epoch [13/50], Step [230/901], Loss: 0.4365\n",
      "Epoch [13/50], Step [240/901], Loss: 0.4621\n",
      "Epoch [13/50], Step [250/901], Loss: 0.4580\n",
      "Epoch [13/50], Step [260/901], Loss: 0.4728\n",
      "Epoch [13/50], Step [270/901], Loss: 0.4633\n",
      "Epoch [13/50], Step [280/901], Loss: 0.4529\n",
      "Epoch [13/50], Step [290/901], Loss: 0.4384\n",
      "Epoch [13/50], Step [300/901], Loss: 0.4621\n",
      "Epoch [13/50], Step [310/901], Loss: 0.4742\n",
      "Epoch [13/50], Step [320/901], Loss: 0.4490\n",
      "Epoch [13/50], Step [330/901], Loss: 0.4375\n",
      "Epoch [13/50], Step [340/901], Loss: 0.4742\n",
      "Epoch [13/50], Step [350/901], Loss: 0.4699\n",
      "Epoch [13/50], Step [360/901], Loss: 0.4789\n",
      "Epoch [13/50], Step [370/901], Loss: 0.4684\n",
      "Epoch [13/50], Step [380/901], Loss: 0.4361\n",
      "Epoch [13/50], Step [390/901], Loss: 0.4443\n",
      "Epoch [13/50], Step [400/901], Loss: 0.4213\n",
      "Epoch [13/50], Step [410/901], Loss: 0.4829\n",
      "Epoch [13/50], Step [420/901], Loss: 0.4360\n",
      "Epoch [13/50], Step [430/901], Loss: 0.4863\n",
      "Epoch [13/50], Step [440/901], Loss: 0.4404\n",
      "Epoch [13/50], Step [450/901], Loss: 0.4677\n",
      "Epoch [13/50], Step [460/901], Loss: 0.4625\n",
      "Epoch [13/50], Step [470/901], Loss: 0.4397\n",
      "Epoch [13/50], Step [480/901], Loss: 0.4961\n",
      "Epoch [13/50], Step [490/901], Loss: 0.4697\n",
      "Epoch [13/50], Step [500/901], Loss: 0.4563\n",
      "Epoch [13/50], Step [510/901], Loss: 0.4368\n",
      "Epoch [13/50], Step [520/901], Loss: 0.4401\n",
      "Epoch [13/50], Step [530/901], Loss: 0.5246\n",
      "Epoch [13/50], Step [540/901], Loss: 0.4809\n",
      "Epoch [13/50], Step [550/901], Loss: 0.4794\n",
      "Epoch [13/50], Step [560/901], Loss: 0.4343\n",
      "Epoch [13/50], Step [570/901], Loss: 0.4475\n",
      "Epoch [13/50], Step [580/901], Loss: 0.4472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50], Step [590/901], Loss: 0.4503\n",
      "Epoch [13/50], Step [600/901], Loss: 0.4969\n",
      "Epoch [13/50], Step [610/901], Loss: 0.4312\n",
      "Epoch [13/50], Step [620/901], Loss: 0.4697\n",
      "Epoch [13/50], Step [630/901], Loss: 0.4567\n",
      "Epoch [13/50], Step [640/901], Loss: 0.4469\n",
      "Epoch [13/50], Step [650/901], Loss: 0.4382\n",
      "Epoch [13/50], Step [660/901], Loss: 0.4319\n",
      "Epoch [13/50], Step [670/901], Loss: 0.4659\n",
      "Epoch [13/50], Step [680/901], Loss: 0.4512\n",
      "Epoch [13/50], Step [690/901], Loss: 0.4730\n",
      "Epoch [13/50], Step [700/901], Loss: 0.4388\n",
      "Epoch [13/50], Step [710/901], Loss: 0.4146\n",
      "Epoch [13/50], Step [720/901], Loss: 0.4583\n",
      "Epoch [13/50], Step [730/901], Loss: 0.4274\n",
      "Epoch [13/50], Step [740/901], Loss: 0.4128\n",
      "Epoch [13/50], Step [750/901], Loss: 0.5084\n",
      "Epoch [13/50], Step [760/901], Loss: 0.4807\n",
      "Epoch [13/50], Step [770/901], Loss: 0.4663\n",
      "Epoch [13/50], Step [780/901], Loss: 0.4770\n",
      "Epoch [13/50], Step [790/901], Loss: 0.4731\n",
      "Epoch [13/50], Step [800/901], Loss: 0.4642\n",
      "Epoch [13/50], Step [810/901], Loss: 0.4598\n",
      "Epoch [13/50], Step [820/901], Loss: 0.4491\n",
      "Epoch [13/50], Step [830/901], Loss: 0.4936\n",
      "Epoch [13/50], Step [840/901], Loss: 0.4624\n",
      "Epoch [13/50], Step [850/901], Loss: 0.4747\n",
      "Epoch [13/50], Step [860/901], Loss: 0.4501\n",
      "Epoch [13/50], Step [870/901], Loss: 0.4605\n",
      "Epoch [13/50], Step [880/901], Loss: 0.4489\n",
      "Epoch [13/50], Step [890/901], Loss: 0.4594\n",
      "Epoch [13/50], Step [900/901], Loss: 0.4557\n",
      "\n",
      "train loss: 0.4571, train acc: 87.6640\n",
      "validation loss: 0.7137, validation acc: 96.7234\n",
      "\n",
      "Epoch 14\n",
      "\n",
      "Epoch [14/50], Step [0/901], Loss: 0.4676\n",
      "Epoch [14/50], Step [10/901], Loss: 0.4625\n",
      "Epoch [14/50], Step [20/901], Loss: 0.4027\n",
      "Epoch [14/50], Step [30/901], Loss: 0.4296\n",
      "Epoch [14/50], Step [40/901], Loss: 0.4836\n",
      "Epoch [14/50], Step [50/901], Loss: 0.4258\n",
      "Epoch [14/50], Step [60/901], Loss: 0.4493\n",
      "Epoch [14/50], Step [70/901], Loss: 0.4365\n",
      "Epoch [14/50], Step [80/901], Loss: 0.4641\n",
      "Epoch [14/50], Step [90/901], Loss: 0.4557\n",
      "Epoch [14/50], Step [100/901], Loss: 0.4463\n",
      "Epoch [14/50], Step [110/901], Loss: 0.4439\n",
      "Epoch [14/50], Step [120/901], Loss: 0.3996\n",
      "Epoch [14/50], Step [130/901], Loss: 0.4250\n",
      "Epoch [14/50], Step [140/901], Loss: 0.4698\n",
      "Epoch [14/50], Step [150/901], Loss: 0.4359\n",
      "Epoch [14/50], Step [160/901], Loss: 0.4692\n",
      "Epoch [14/50], Step [170/901], Loss: 0.4461\n",
      "Epoch [14/50], Step [180/901], Loss: 0.4122\n",
      "Epoch [14/50], Step [190/901], Loss: 0.4184\n",
      "Epoch [14/50], Step [200/901], Loss: 0.5196\n",
      "Epoch [14/50], Step [210/901], Loss: 0.4619\n",
      "Epoch [14/50], Step [220/901], Loss: 0.4168\n",
      "Epoch [14/50], Step [230/901], Loss: 0.4632\n",
      "Epoch [14/50], Step [240/901], Loss: 0.4706\n",
      "Epoch [14/50], Step [250/901], Loss: 0.4808\n",
      "Epoch [14/50], Step [260/901], Loss: 0.4038\n",
      "Epoch [14/50], Step [270/901], Loss: 0.4863\n",
      "Epoch [14/50], Step [280/901], Loss: 0.4375\n",
      "Epoch [14/50], Step [290/901], Loss: 0.4655\n",
      "Epoch [14/50], Step [300/901], Loss: 0.4793\n",
      "Epoch [14/50], Step [310/901], Loss: 0.4708\n",
      "Epoch [14/50], Step [320/901], Loss: 0.4559\n",
      "Epoch [14/50], Step [330/901], Loss: 0.4477\n",
      "Epoch [14/50], Step [340/901], Loss: 0.5071\n",
      "Epoch [14/50], Step [350/901], Loss: 0.4548\n",
      "Epoch [14/50], Step [360/901], Loss: 0.4558\n",
      "Epoch [14/50], Step [370/901], Loss: 0.4445\n",
      "Epoch [14/50], Step [380/901], Loss: 0.4509\n",
      "Epoch [14/50], Step [390/901], Loss: 0.4343\n",
      "Epoch [14/50], Step [400/901], Loss: 0.4182\n",
      "Epoch [14/50], Step [410/901], Loss: 0.4370\n",
      "Epoch [14/50], Step [420/901], Loss: 0.4600\n",
      "Epoch [14/50], Step [430/901], Loss: 0.4557\n",
      "Epoch [14/50], Step [440/901], Loss: 0.4940\n",
      "Epoch [14/50], Step [450/901], Loss: 0.4471\n",
      "Epoch [14/50], Step [460/901], Loss: 0.4969\n",
      "Epoch [14/50], Step [470/901], Loss: 0.4647\n",
      "Epoch [14/50], Step [480/901], Loss: 0.4406\n",
      "Epoch [14/50], Step [490/901], Loss: 0.4785\n",
      "Epoch [14/50], Step [500/901], Loss: 0.4405\n",
      "Epoch [14/50], Step [510/901], Loss: 0.4650\n",
      "Epoch [14/50], Step [520/901], Loss: 0.4164\n",
      "Epoch [14/50], Step [530/901], Loss: 0.4499\n",
      "Epoch [14/50], Step [540/901], Loss: 0.4468\n",
      "Epoch [14/50], Step [550/901], Loss: 0.4293\n",
      "Epoch [14/50], Step [560/901], Loss: 0.5108\n",
      "Epoch [14/50], Step [570/901], Loss: 0.4225\n",
      "Epoch [14/50], Step [580/901], Loss: 0.4619\n",
      "Epoch [14/50], Step [590/901], Loss: 0.4059\n",
      "Epoch [14/50], Step [600/901], Loss: 0.4257\n",
      "Epoch [14/50], Step [610/901], Loss: 0.4760\n",
      "Epoch [14/50], Step [620/901], Loss: 0.4653\n",
      "Epoch [14/50], Step [630/901], Loss: 0.4472\n",
      "Epoch [14/50], Step [640/901], Loss: 0.4442\n",
      "Epoch [14/50], Step [650/901], Loss: 0.4634\n",
      "Epoch [14/50], Step [660/901], Loss: 0.4607\n",
      "Epoch [14/50], Step [670/901], Loss: 0.4502\n",
      "Epoch [14/50], Step [680/901], Loss: 0.4761\n",
      "Epoch [14/50], Step [690/901], Loss: 0.4436\n",
      "Epoch [14/50], Step [700/901], Loss: 0.4422\n",
      "Epoch [14/50], Step [710/901], Loss: 0.4549\n",
      "Epoch [14/50], Step [720/901], Loss: 0.4643\n",
      "Epoch [14/50], Step [730/901], Loss: 0.4564\n",
      "Epoch [14/50], Step [740/901], Loss: 0.4286\n",
      "Epoch [14/50], Step [750/901], Loss: 0.4155\n",
      "Epoch [14/50], Step [760/901], Loss: 0.4536\n",
      "Epoch [14/50], Step [770/901], Loss: 0.4770\n",
      "Epoch [14/50], Step [780/901], Loss: 0.4535\n",
      "Epoch [14/50], Step [790/901], Loss: 0.4405\n",
      "Epoch [14/50], Step [800/901], Loss: 0.4643\n",
      "Epoch [14/50], Step [810/901], Loss: 0.4553\n",
      "Epoch [14/50], Step [820/901], Loss: 0.4268\n",
      "Epoch [14/50], Step [830/901], Loss: 0.4193\n",
      "Epoch [14/50], Step [840/901], Loss: 0.4135\n",
      "Epoch [14/50], Step [850/901], Loss: 0.4021\n",
      "Epoch [14/50], Step [860/901], Loss: 0.4475\n",
      "Epoch [14/50], Step [870/901], Loss: 0.4882\n",
      "Epoch [14/50], Step [880/901], Loss: 0.4389\n",
      "Epoch [14/50], Step [890/901], Loss: 0.4225\n",
      "Epoch [14/50], Step [900/901], Loss: 0.4418\n",
      "\n",
      "train loss: 0.4566, train acc: 87.6553\n",
      "validation loss: 0.7130, validation acc: 97.0904\n",
      "\n",
      "Epoch 15\n",
      "\n",
      "Epoch [15/50], Step [0/901], Loss: 0.4483\n",
      "Epoch [15/50], Step [10/901], Loss: 0.4368\n",
      "Epoch [15/50], Step [20/901], Loss: 0.4705\n",
      "Epoch [15/50], Step [30/901], Loss: 0.4756\n",
      "Epoch [15/50], Step [40/901], Loss: 0.4667\n",
      "Epoch [15/50], Step [50/901], Loss: 0.4652\n",
      "Epoch [15/50], Step [60/901], Loss: 0.4848\n",
      "Epoch [15/50], Step [70/901], Loss: 0.4327\n",
      "Epoch [15/50], Step [80/901], Loss: 0.4374\n",
      "Epoch [15/50], Step [90/901], Loss: 0.4987\n",
      "Epoch [15/50], Step [100/901], Loss: 0.4580\n",
      "Epoch [15/50], Step [110/901], Loss: 0.4627\n",
      "Epoch [15/50], Step [120/901], Loss: 0.4632\n",
      "Epoch [15/50], Step [130/901], Loss: 0.4324\n",
      "Epoch [15/50], Step [140/901], Loss: 0.4487\n",
      "Epoch [15/50], Step [150/901], Loss: 0.4266\n",
      "Epoch [15/50], Step [160/901], Loss: 0.4251\n",
      "Epoch [15/50], Step [170/901], Loss: 0.4060\n",
      "Epoch [15/50], Step [180/901], Loss: 0.4310\n",
      "Epoch [15/50], Step [190/901], Loss: 0.4317\n",
      "Epoch [15/50], Step [200/901], Loss: 0.4875\n",
      "Epoch [15/50], Step [210/901], Loss: 0.4318\n",
      "Epoch [15/50], Step [220/901], Loss: 0.4188\n",
      "Epoch [15/50], Step [230/901], Loss: 0.4692\n",
      "Epoch [15/50], Step [240/901], Loss: 0.3797\n",
      "Epoch [15/50], Step [250/901], Loss: 0.4733\n",
      "Epoch [15/50], Step [260/901], Loss: 0.4271\n",
      "Epoch [15/50], Step [270/901], Loss: 0.4518\n",
      "Epoch [15/50], Step [280/901], Loss: 0.4205\n",
      "Epoch [15/50], Step [290/901], Loss: 0.4540\n",
      "Epoch [15/50], Step [300/901], Loss: 0.4483\n",
      "Epoch [15/50], Step [310/901], Loss: 0.4209\n",
      "Epoch [15/50], Step [320/901], Loss: 0.4550\n",
      "Epoch [15/50], Step [330/901], Loss: 0.4795\n",
      "Epoch [15/50], Step [340/901], Loss: 0.4515\n",
      "Epoch [15/50], Step [350/901], Loss: 0.4339\n",
      "Epoch [15/50], Step [360/901], Loss: 0.4920\n",
      "Epoch [15/50], Step [370/901], Loss: 0.4900\n",
      "Epoch [15/50], Step [380/901], Loss: 0.4894\n",
      "Epoch [15/50], Step [390/901], Loss: 0.4295\n",
      "Epoch [15/50], Step [400/901], Loss: 0.4950\n",
      "Epoch [15/50], Step [410/901], Loss: 0.4270\n",
      "Epoch [15/50], Step [420/901], Loss: 0.4851\n",
      "Epoch [15/50], Step [430/901], Loss: 0.4284\n",
      "Epoch [15/50], Step [440/901], Loss: 0.4241\n",
      "Epoch [15/50], Step [450/901], Loss: 0.4602\n",
      "Epoch [15/50], Step [460/901], Loss: 0.4403\n",
      "Epoch [15/50], Step [470/901], Loss: 0.4272\n",
      "Epoch [15/50], Step [480/901], Loss: 0.4914\n",
      "Epoch [15/50], Step [490/901], Loss: 0.4500\n",
      "Epoch [15/50], Step [500/901], Loss: 0.4476\n",
      "Epoch [15/50], Step [510/901], Loss: 0.4511\n",
      "Epoch [15/50], Step [520/901], Loss: 0.4833\n",
      "Epoch [15/50], Step [530/901], Loss: 0.4487\n",
      "Epoch [15/50], Step [540/901], Loss: 0.4397\n",
      "Epoch [15/50], Step [550/901], Loss: 0.4246\n",
      "Epoch [15/50], Step [560/901], Loss: 0.4482\n",
      "Epoch [15/50], Step [570/901], Loss: 0.4392\n",
      "Epoch [15/50], Step [580/901], Loss: 0.4576\n",
      "Epoch [15/50], Step [590/901], Loss: 0.4329\n",
      "Epoch [15/50], Step [600/901], Loss: 0.4408\n",
      "Epoch [15/50], Step [610/901], Loss: 0.4175\n",
      "Epoch [15/50], Step [620/901], Loss: 0.4933\n",
      "Epoch [15/50], Step [630/901], Loss: 0.4184\n",
      "Epoch [15/50], Step [640/901], Loss: 0.4434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Step [650/901], Loss: 0.4575\n",
      "Epoch [15/50], Step [660/901], Loss: 0.4416\n",
      "Epoch [15/50], Step [670/901], Loss: 0.4479\n",
      "Epoch [15/50], Step [680/901], Loss: 0.4316\n",
      "Epoch [15/50], Step [690/901], Loss: 0.4079\n",
      "Epoch [15/50], Step [700/901], Loss: 0.4137\n",
      "Epoch [15/50], Step [710/901], Loss: 0.4364\n",
      "Epoch [15/50], Step [720/901], Loss: 0.4270\n",
      "Epoch [15/50], Step [730/901], Loss: 0.4449\n",
      "Epoch [15/50], Step [740/901], Loss: 0.4495\n",
      "Epoch [15/50], Step [750/901], Loss: 0.4858\n",
      "Epoch [15/50], Step [760/901], Loss: 0.4780\n",
      "Epoch [15/50], Step [770/901], Loss: 0.5020\n",
      "Epoch [15/50], Step [780/901], Loss: 0.4430\n",
      "Epoch [15/50], Step [790/901], Loss: 0.4593\n",
      "Epoch [15/50], Step [800/901], Loss: 0.4348\n",
      "Epoch [15/50], Step [810/901], Loss: 0.4771\n",
      "Epoch [15/50], Step [820/901], Loss: 0.4602\n",
      "Epoch [15/50], Step [830/901], Loss: 0.4687\n",
      "Epoch [15/50], Step [840/901], Loss: 0.4311\n",
      "Epoch [15/50], Step [850/901], Loss: 0.4169\n",
      "Epoch [15/50], Step [860/901], Loss: 0.4335\n",
      "Epoch [15/50], Step [870/901], Loss: 0.4640\n",
      "Epoch [15/50], Step [880/901], Loss: 0.4354\n",
      "Epoch [15/50], Step [890/901], Loss: 0.4474\n",
      "Epoch [15/50], Step [900/901], Loss: 0.5129\n",
      "\n",
      "train loss: 0.4560, train acc: 87.9194\n",
      "validation loss: 0.7119, validation acc: 97.3707\n",
      "\n",
      "Epoch 16\n",
      "\n",
      "Epoch [16/50], Step [0/901], Loss: 0.4361\n",
      "Epoch [16/50], Step [10/901], Loss: 0.4741\n",
      "Epoch [16/50], Step [20/901], Loss: 0.4533\n",
      "Epoch [16/50], Step [30/901], Loss: 0.4232\n",
      "Epoch [16/50], Step [40/901], Loss: 0.4396\n",
      "Epoch [16/50], Step [50/901], Loss: 0.4900\n",
      "Epoch [16/50], Step [60/901], Loss: 0.4450\n",
      "Epoch [16/50], Step [70/901], Loss: 0.4826\n",
      "Epoch [16/50], Step [80/901], Loss: 0.4132\n",
      "Epoch [16/50], Step [90/901], Loss: 0.4590\n",
      "Epoch [16/50], Step [100/901], Loss: 0.4609\n",
      "Epoch [16/50], Step [110/901], Loss: 0.4106\n",
      "Epoch [16/50], Step [120/901], Loss: 0.4932\n",
      "Epoch [16/50], Step [130/901], Loss: 0.4781\n",
      "Epoch [16/50], Step [140/901], Loss: 0.4403\n",
      "Epoch [16/50], Step [150/901], Loss: 0.4301\n",
      "Epoch [16/50], Step [160/901], Loss: 0.4460\n",
      "Epoch [16/50], Step [170/901], Loss: 0.4479\n",
      "Epoch [16/50], Step [180/901], Loss: 0.3946\n",
      "Epoch [16/50], Step [190/901], Loss: 0.4514\n",
      "Epoch [16/50], Step [200/901], Loss: 0.4419\n",
      "Epoch [16/50], Step [210/901], Loss: 0.4368\n",
      "Epoch [16/50], Step [220/901], Loss: 0.4279\n",
      "Epoch [16/50], Step [230/901], Loss: 0.4373\n",
      "Epoch [16/50], Step [240/901], Loss: 0.4464\n",
      "Epoch [16/50], Step [250/901], Loss: 0.4731\n",
      "Epoch [16/50], Step [260/901], Loss: 0.4656\n",
      "Epoch [16/50], Step [270/901], Loss: 0.4308\n",
      "Epoch [16/50], Step [280/901], Loss: 0.5026\n",
      "Epoch [16/50], Step [290/901], Loss: 0.4638\n",
      "Epoch [16/50], Step [300/901], Loss: 0.4642\n",
      "Epoch [16/50], Step [310/901], Loss: 0.4825\n",
      "Epoch [16/50], Step [320/901], Loss: 0.4696\n",
      "Epoch [16/50], Step [330/901], Loss: 0.4620\n",
      "Epoch [16/50], Step [340/901], Loss: 0.4413\n",
      "Epoch [16/50], Step [350/901], Loss: 0.4512\n",
      "Epoch [16/50], Step [360/901], Loss: 0.4853\n",
      "Epoch [16/50], Step [370/901], Loss: 0.4646\n",
      "Epoch [16/50], Step [380/901], Loss: 0.4372\n",
      "Epoch [16/50], Step [390/901], Loss: 0.4466\n",
      "Epoch [16/50], Step [400/901], Loss: 0.4234\n",
      "Epoch [16/50], Step [410/901], Loss: 0.4229\n",
      "Epoch [16/50], Step [420/901], Loss: 0.4721\n",
      "Epoch [16/50], Step [430/901], Loss: 0.4545\n",
      "Epoch [16/50], Step [440/901], Loss: 0.4248\n",
      "Epoch [16/50], Step [450/901], Loss: 0.5332\n",
      "Epoch [16/50], Step [460/901], Loss: 0.4344\n",
      "Epoch [16/50], Step [470/901], Loss: 0.4573\n",
      "Epoch [16/50], Step [480/901], Loss: 0.4792\n",
      "Epoch [16/50], Step [490/901], Loss: 0.4711\n",
      "Epoch [16/50], Step [500/901], Loss: 0.4476\n",
      "Epoch [16/50], Step [510/901], Loss: 0.4150\n",
      "Epoch [16/50], Step [520/901], Loss: 0.4107\n",
      "Epoch [16/50], Step [530/901], Loss: 0.4199\n",
      "Epoch [16/50], Step [540/901], Loss: 0.4611\n",
      "Epoch [16/50], Step [550/901], Loss: 0.4352\n",
      "Epoch [16/50], Step [560/901], Loss: 0.4361\n",
      "Epoch [16/50], Step [570/901], Loss: 0.4809\n",
      "Epoch [16/50], Step [580/901], Loss: 0.4470\n",
      "Epoch [16/50], Step [590/901], Loss: 0.4337\n",
      "Epoch [16/50], Step [600/901], Loss: 0.4079\n",
      "Epoch [16/50], Step [610/901], Loss: 0.4368\n",
      "Epoch [16/50], Step [620/901], Loss: 0.4629\n",
      "Epoch [16/50], Step [630/901], Loss: 0.4487\n",
      "Epoch [16/50], Step [640/901], Loss: 0.4606\n",
      "Epoch [16/50], Step [650/901], Loss: 0.4703\n",
      "Epoch [16/50], Step [660/901], Loss: 0.4173\n",
      "Epoch [16/50], Step [670/901], Loss: 0.4645\n",
      "Epoch [16/50], Step [680/901], Loss: 0.4357\n",
      "Epoch [16/50], Step [690/901], Loss: 0.4146\n",
      "Epoch [16/50], Step [700/901], Loss: 0.4586\n",
      "Epoch [16/50], Step [710/901], Loss: 0.4387\n",
      "Epoch [16/50], Step [720/901], Loss: 0.4410\n",
      "Epoch [16/50], Step [730/901], Loss: 0.4440\n",
      "Epoch [16/50], Step [740/901], Loss: 0.4459\n",
      "Epoch [16/50], Step [750/901], Loss: 0.4792\n",
      "Epoch [16/50], Step [760/901], Loss: 0.4734\n",
      "Epoch [16/50], Step [770/901], Loss: 0.4470\n",
      "Epoch [16/50], Step [780/901], Loss: 0.4780\n",
      "Epoch [16/50], Step [790/901], Loss: 0.4410\n",
      "Epoch [16/50], Step [800/901], Loss: 0.4100\n",
      "Epoch [16/50], Step [810/901], Loss: 0.4383\n",
      "Epoch [16/50], Step [820/901], Loss: 0.4907\n",
      "Epoch [16/50], Step [830/901], Loss: 0.4226\n",
      "Epoch [16/50], Step [840/901], Loss: 0.4194\n",
      "Epoch [16/50], Step [850/901], Loss: 0.4860\n",
      "Epoch [16/50], Step [860/901], Loss: 0.4422\n",
      "Epoch [16/50], Step [870/901], Loss: 0.4053\n",
      "Epoch [16/50], Step [880/901], Loss: 0.4567\n",
      "Epoch [16/50], Step [890/901], Loss: 0.4612\n",
      "Epoch [16/50], Step [900/901], Loss: 0.4314\n",
      "\n",
      "train loss: 0.4555, train acc: 87.9207\n",
      "validation loss: 0.7112, validation acc: 97.1705\n",
      "\n",
      "Epoch 17\n",
      "\n",
      "Epoch [17/50], Step [0/901], Loss: 0.3768\n",
      "Epoch [17/50], Step [10/901], Loss: 0.4513\n",
      "Epoch [17/50], Step [20/901], Loss: 0.3856\n",
      "Epoch [17/50], Step [30/901], Loss: 0.4525\n",
      "Epoch [17/50], Step [40/901], Loss: 0.4817\n",
      "Epoch [17/50], Step [50/901], Loss: 0.4795\n",
      "Epoch [17/50], Step [60/901], Loss: 0.4316\n",
      "Epoch [17/50], Step [70/901], Loss: 0.4598\n",
      "Epoch [17/50], Step [80/901], Loss: 0.4945\n",
      "Epoch [17/50], Step [90/901], Loss: 0.4483\n",
      "Epoch [17/50], Step [100/901], Loss: 0.4622\n",
      "Epoch [17/50], Step [110/901], Loss: 0.4803\n",
      "Epoch [17/50], Step [120/901], Loss: 0.4788\n",
      "Epoch [17/50], Step [130/901], Loss: 0.4055\n",
      "Epoch [17/50], Step [140/901], Loss: 0.4483\n",
      "Epoch [17/50], Step [150/901], Loss: 0.4266\n",
      "Epoch [17/50], Step [160/901], Loss: 0.4582\n",
      "Epoch [17/50], Step [170/901], Loss: 0.4516\n",
      "Epoch [17/50], Step [180/901], Loss: 0.4677\n",
      "Epoch [17/50], Step [190/901], Loss: 0.4539\n",
      "Epoch [17/50], Step [200/901], Loss: 0.4818\n",
      "Epoch [17/50], Step [210/901], Loss: 0.4437\n",
      "Epoch [17/50], Step [220/901], Loss: 0.4613\n",
      "Epoch [17/50], Step [230/901], Loss: 0.4526\n",
      "Epoch [17/50], Step [240/901], Loss: 0.4689\n",
      "Epoch [17/50], Step [250/901], Loss: 0.4275\n",
      "Epoch [17/50], Step [260/901], Loss: 0.4540\n",
      "Epoch [17/50], Step [270/901], Loss: 0.5037\n",
      "Epoch [17/50], Step [280/901], Loss: 0.4866\n",
      "Epoch [17/50], Step [290/901], Loss: 0.4171\n",
      "Epoch [17/50], Step [300/901], Loss: 0.4497\n",
      "Epoch [17/50], Step [310/901], Loss: 0.4385\n",
      "Epoch [17/50], Step [320/901], Loss: 0.4754\n",
      "Epoch [17/50], Step [330/901], Loss: 0.4545\n",
      "Epoch [17/50], Step [340/901], Loss: 0.4238\n",
      "Epoch [17/50], Step [350/901], Loss: 0.4402\n",
      "Epoch [17/50], Step [360/901], Loss: 0.4880\n",
      "Epoch [17/50], Step [370/901], Loss: 0.4615\n",
      "Epoch [17/50], Step [380/901], Loss: 0.4556\n",
      "Epoch [17/50], Step [390/901], Loss: 0.4676\n",
      "Epoch [17/50], Step [400/901], Loss: 0.3718\n",
      "Epoch [17/50], Step [410/901], Loss: 0.4550\n",
      "Epoch [17/50], Step [420/901], Loss: 0.4393\n",
      "Epoch [17/50], Step [430/901], Loss: 0.4187\n",
      "Epoch [17/50], Step [440/901], Loss: 0.4529\n",
      "Epoch [17/50], Step [450/901], Loss: 0.4317\n",
      "Epoch [17/50], Step [460/901], Loss: 0.4413\n",
      "Epoch [17/50], Step [470/901], Loss: 0.4243\n",
      "Epoch [17/50], Step [480/901], Loss: 0.4492\n",
      "Epoch [17/50], Step [490/901], Loss: 0.4425\n",
      "Epoch [17/50], Step [500/901], Loss: 0.4533\n",
      "Epoch [17/50], Step [510/901], Loss: 0.4209\n",
      "Epoch [17/50], Step [520/901], Loss: 0.4701\n",
      "Epoch [17/50], Step [530/901], Loss: 0.4896\n",
      "Epoch [17/50], Step [540/901], Loss: 0.4579\n",
      "Epoch [17/50], Step [550/901], Loss: 0.4498\n",
      "Epoch [17/50], Step [560/901], Loss: 0.4154\n",
      "Epoch [17/50], Step [570/901], Loss: 0.4654\n",
      "Epoch [17/50], Step [580/901], Loss: 0.4452\n",
      "Epoch [17/50], Step [590/901], Loss: 0.4201\n",
      "Epoch [17/50], Step [600/901], Loss: 0.4137\n",
      "Epoch [17/50], Step [610/901], Loss: 0.4619\n",
      "Epoch [17/50], Step [620/901], Loss: 0.4642\n",
      "Epoch [17/50], Step [630/901], Loss: 0.4162\n",
      "Epoch [17/50], Step [640/901], Loss: 0.4538\n",
      "Epoch [17/50], Step [650/901], Loss: 0.4578\n",
      "Epoch [17/50], Step [660/901], Loss: 0.3951\n",
      "Epoch [17/50], Step [670/901], Loss: 0.4560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/50], Step [680/901], Loss: 0.4501\n",
      "Epoch [17/50], Step [690/901], Loss: 0.4598\n",
      "Epoch [17/50], Step [700/901], Loss: 0.4532\n",
      "Epoch [17/50], Step [710/901], Loss: 0.4283\n",
      "Epoch [17/50], Step [720/901], Loss: 0.4488\n",
      "Epoch [17/50], Step [730/901], Loss: 0.4327\n",
      "Epoch [17/50], Step [740/901], Loss: 0.4634\n",
      "Epoch [17/50], Step [750/901], Loss: 0.4291\n",
      "Epoch [17/50], Step [760/901], Loss: 0.4610\n",
      "Epoch [17/50], Step [770/901], Loss: 0.4573\n",
      "Epoch [17/50], Step [780/901], Loss: 0.4784\n",
      "Epoch [17/50], Step [790/901], Loss: 0.4182\n",
      "Epoch [17/50], Step [800/901], Loss: 0.4809\n",
      "Epoch [17/50], Step [810/901], Loss: 0.4900\n",
      "Epoch [17/50], Step [820/901], Loss: 0.4337\n",
      "Epoch [17/50], Step [830/901], Loss: 0.4679\n",
      "Epoch [17/50], Step [840/901], Loss: 0.4755\n",
      "Epoch [17/50], Step [850/901], Loss: 0.4429\n",
      "Epoch [17/50], Step [860/901], Loss: 0.4340\n",
      "Epoch [17/50], Step [870/901], Loss: 0.4800\n",
      "Epoch [17/50], Step [880/901], Loss: 0.3995\n",
      "Epoch [17/50], Step [890/901], Loss: 0.4424\n",
      "Epoch [17/50], Step [900/901], Loss: 0.4347\n",
      "\n",
      "train loss: 0.4550, train acc: 87.9870\n",
      "validation loss: 0.7111, validation acc: 96.7301\n",
      "\n",
      "Epoch 18\n",
      "\n",
      "Epoch [18/50], Step [0/901], Loss: 0.4481\n",
      "Epoch [18/50], Step [10/901], Loss: 0.4366\n",
      "Epoch [18/50], Step [20/901], Loss: 0.4202\n",
      "Epoch [18/50], Step [30/901], Loss: 0.4266\n",
      "Epoch [18/50], Step [40/901], Loss: 0.4269\n",
      "Epoch [18/50], Step [50/901], Loss: 0.4478\n",
      "Epoch [18/50], Step [60/901], Loss: 0.4609\n",
      "Epoch [18/50], Step [70/901], Loss: 0.4199\n",
      "Epoch [18/50], Step [80/901], Loss: 0.4634\n",
      "Epoch [18/50], Step [90/901], Loss: 0.4545\n",
      "Epoch [18/50], Step [100/901], Loss: 0.4346\n",
      "Epoch [18/50], Step [110/901], Loss: 0.4733\n",
      "Epoch [18/50], Step [120/901], Loss: 0.4294\n",
      "Epoch [18/50], Step [130/901], Loss: 0.4835\n",
      "Epoch [18/50], Step [140/901], Loss: 0.4590\n",
      "Epoch [18/50], Step [150/901], Loss: 0.4502\n",
      "Epoch [18/50], Step [160/901], Loss: 0.4585\n",
      "Epoch [18/50], Step [170/901], Loss: 0.4314\n",
      "Epoch [18/50], Step [180/901], Loss: 0.4597\n",
      "Epoch [18/50], Step [190/901], Loss: 0.4369\n",
      "Epoch [18/50], Step [200/901], Loss: 0.4136\n",
      "Epoch [18/50], Step [210/901], Loss: 0.4384\n",
      "Epoch [18/50], Step [220/901], Loss: 0.4626\n",
      "Epoch [18/50], Step [230/901], Loss: 0.4358\n",
      "Epoch [18/50], Step [240/901], Loss: 0.4544\n",
      "Epoch [18/50], Step [250/901], Loss: 0.4311\n",
      "Epoch [18/50], Step [260/901], Loss: 0.4436\n",
      "Epoch [18/50], Step [270/901], Loss: 0.4389\n",
      "Epoch [18/50], Step [280/901], Loss: 0.3958\n",
      "Epoch [18/50], Step [290/901], Loss: 0.4370\n",
      "Epoch [18/50], Step [300/901], Loss: 0.4041\n",
      "Epoch [18/50], Step [310/901], Loss: 0.4528\n",
      "Epoch [18/50], Step [320/901], Loss: 0.4815\n",
      "Epoch [18/50], Step [330/901], Loss: 0.4434\n",
      "Epoch [18/50], Step [340/901], Loss: 0.4714\n",
      "Epoch [18/50], Step [350/901], Loss: 0.4841\n",
      "Epoch [18/50], Step [360/901], Loss: 0.4616\n",
      "Epoch [18/50], Step [370/901], Loss: 0.4501\n",
      "Epoch [18/50], Step [380/901], Loss: 0.4275\n",
      "Epoch [18/50], Step [390/901], Loss: 0.4518\n",
      "Epoch [18/50], Step [400/901], Loss: 0.4571\n",
      "Epoch [18/50], Step [410/901], Loss: 0.4054\n",
      "Epoch [18/50], Step [420/901], Loss: 0.4422\n",
      "Epoch [18/50], Step [430/901], Loss: 0.4176\n",
      "Epoch [18/50], Step [440/901], Loss: 0.4060\n",
      "Epoch [18/50], Step [450/901], Loss: 0.4222\n",
      "Epoch [18/50], Step [460/901], Loss: 0.4509\n",
      "Epoch [18/50], Step [470/901], Loss: 0.4113\n",
      "Epoch [18/50], Step [480/901], Loss: 0.4489\n",
      "Epoch [18/50], Step [490/901], Loss: 0.4805\n",
      "Epoch [18/50], Step [500/901], Loss: 0.4328\n",
      "Epoch [18/50], Step [510/901], Loss: 0.4396\n",
      "Epoch [18/50], Step [520/901], Loss: 0.4340\n",
      "Epoch [18/50], Step [530/901], Loss: 0.4380\n",
      "Epoch [18/50], Step [540/901], Loss: 0.4216\n",
      "Epoch [18/50], Step [550/901], Loss: 0.4151\n",
      "Epoch [18/50], Step [560/901], Loss: 0.4036\n",
      "Epoch [18/50], Step [570/901], Loss: 0.4437\n",
      "Epoch [18/50], Step [580/901], Loss: 0.4609\n",
      "Epoch [18/50], Step [590/901], Loss: 0.4157\n",
      "Epoch [18/50], Step [600/901], Loss: 0.4344\n",
      "Epoch [18/50], Step [610/901], Loss: 0.3999\n",
      "Epoch [18/50], Step [620/901], Loss: 0.4534\n",
      "Epoch [18/50], Step [630/901], Loss: 0.4268\n",
      "Epoch [18/50], Step [640/901], Loss: 0.4189\n",
      "Epoch [18/50], Step [650/901], Loss: 0.4542\n",
      "Epoch [18/50], Step [660/901], Loss: 0.4016\n",
      "Epoch [18/50], Step [670/901], Loss: 0.3947\n",
      "Epoch [18/50], Step [680/901], Loss: 0.4417\n",
      "Epoch [18/50], Step [690/901], Loss: 0.4502\n",
      "Epoch [18/50], Step [700/901], Loss: 0.4617\n",
      "Epoch [18/50], Step [710/901], Loss: 0.4536\n",
      "Epoch [18/50], Step [720/901], Loss: 0.4485\n",
      "Epoch [18/50], Step [730/901], Loss: 0.4402\n",
      "Epoch [18/50], Step [740/901], Loss: 0.4493\n",
      "Epoch [18/50], Step [750/901], Loss: 0.4638\n",
      "Epoch [18/50], Step [760/901], Loss: 0.4122\n",
      "Epoch [18/50], Step [770/901], Loss: 0.4444\n",
      "Epoch [18/50], Step [780/901], Loss: 0.4654\n",
      "Epoch [18/50], Step [790/901], Loss: 0.4317\n",
      "Epoch [18/50], Step [800/901], Loss: 0.4361\n",
      "Epoch [18/50], Step [810/901], Loss: 0.4403\n",
      "Epoch [18/50], Step [820/901], Loss: 0.4013\n",
      "Epoch [18/50], Step [830/901], Loss: 0.4512\n",
      "Epoch [18/50], Step [840/901], Loss: 0.4379\n",
      "Epoch [18/50], Step [850/901], Loss: 0.4808\n",
      "Epoch [18/50], Step [860/901], Loss: 0.8106\n",
      "Epoch [18/50], Step [870/901], Loss: 0.4186\n",
      "Epoch [18/50], Step [880/901], Loss: 0.4054\n",
      "Epoch [18/50], Step [890/901], Loss: 0.4723\n",
      "Epoch [18/50], Step [900/901], Loss: 0.4452\n",
      "\n",
      "train loss: 0.4545, train acc: 87.9888\n",
      "validation loss: 0.7105, validation acc: 97.0771\n",
      "\n",
      "Epoch 19\n",
      "\n",
      "Epoch [19/50], Step [0/901], Loss: 0.4448\n",
      "Epoch [19/50], Step [10/901], Loss: 0.4338\n",
      "Epoch [19/50], Step [20/901], Loss: 0.4522\n",
      "Epoch [19/50], Step [30/901], Loss: 0.4789\n",
      "Epoch [19/50], Step [40/901], Loss: 0.4286\n",
      "Epoch [19/50], Step [50/901], Loss: 0.4290\n",
      "Epoch [19/50], Step [60/901], Loss: 0.4134\n",
      "Epoch [19/50], Step [70/901], Loss: 0.4283\n",
      "Epoch [19/50], Step [80/901], Loss: 0.4374\n",
      "Epoch [19/50], Step [90/901], Loss: 0.4808\n",
      "Epoch [19/50], Step [100/901], Loss: 0.4755\n",
      "Epoch [19/50], Step [110/901], Loss: 0.4338\n",
      "Epoch [19/50], Step [120/901], Loss: 0.4578\n",
      "Epoch [19/50], Step [130/901], Loss: 0.4170\n",
      "Epoch [19/50], Step [140/901], Loss: 0.4933\n",
      "Epoch [19/50], Step [150/901], Loss: 0.5189\n",
      "Epoch [19/50], Step [160/901], Loss: 0.4170\n",
      "Epoch [19/50], Step [170/901], Loss: 0.4589\n",
      "Epoch [19/50], Step [180/901], Loss: 0.4914\n",
      "Epoch [19/50], Step [190/901], Loss: 0.4850\n",
      "Epoch [19/50], Step [200/901], Loss: 0.4287\n",
      "Epoch [19/50], Step [210/901], Loss: 0.4401\n",
      "Epoch [19/50], Step [220/901], Loss: 0.4185\n",
      "Epoch [19/50], Step [230/901], Loss: 0.4102\n",
      "Epoch [19/50], Step [240/901], Loss: 0.4626\n",
      "Epoch [19/50], Step [250/901], Loss: 0.4646\n",
      "Epoch [19/50], Step [260/901], Loss: 0.4749\n",
      "Epoch [19/50], Step [270/901], Loss: 0.4649\n",
      "Epoch [19/50], Step [280/901], Loss: 0.4358\n",
      "Epoch [19/50], Step [290/901], Loss: 0.4150\n",
      "Epoch [19/50], Step [300/901], Loss: 0.4570\n",
      "Epoch [19/50], Step [310/901], Loss: 0.4309\n",
      "Epoch [19/50], Step [320/901], Loss: 0.4584\n",
      "Epoch [19/50], Step [330/901], Loss: 0.4769\n",
      "Epoch [19/50], Step [340/901], Loss: 0.4693\n",
      "Epoch [19/50], Step [350/901], Loss: 0.4117\n",
      "Epoch [19/50], Step [360/901], Loss: 0.4500\n",
      "Epoch [19/50], Step [370/901], Loss: 0.4603\n",
      "Epoch [19/50], Step [380/901], Loss: 0.4740\n",
      "Epoch [19/50], Step [390/901], Loss: 0.4558\n",
      "Epoch [19/50], Step [400/901], Loss: 0.4452\n",
      "Epoch [19/50], Step [410/901], Loss: 0.4235\n",
      "Epoch [19/50], Step [420/901], Loss: 0.4095\n",
      "Epoch [19/50], Step [430/901], Loss: 0.4455\n",
      "Epoch [19/50], Step [440/901], Loss: 0.4552\n",
      "Epoch [19/50], Step [450/901], Loss: 0.4371\n",
      "Epoch [19/50], Step [460/901], Loss: 0.4729\n",
      "Epoch [19/50], Step [470/901], Loss: 0.4774\n",
      "Epoch [19/50], Step [480/901], Loss: 0.4445\n",
      "Epoch [19/50], Step [490/901], Loss: 0.4614\n",
      "Epoch [19/50], Step [500/901], Loss: 0.4726\n",
      "Epoch [19/50], Step [510/901], Loss: 0.4501\n",
      "Epoch [19/50], Step [520/901], Loss: 0.4500\n",
      "Epoch [19/50], Step [530/901], Loss: 0.4506\n",
      "Epoch [19/50], Step [540/901], Loss: 0.4615\n",
      "Epoch [19/50], Step [550/901], Loss: 0.4381\n",
      "Epoch [19/50], Step [560/901], Loss: 0.4411\n",
      "Epoch [19/50], Step [570/901], Loss: 0.4732\n",
      "Epoch [19/50], Step [580/901], Loss: 0.4605\n",
      "Epoch [19/50], Step [590/901], Loss: 0.4973\n",
      "Epoch [19/50], Step [600/901], Loss: 0.4401\n",
      "Epoch [19/50], Step [610/901], Loss: 0.4433\n",
      "Epoch [19/50], Step [620/901], Loss: 0.4688\n",
      "Epoch [19/50], Step [630/901], Loss: 0.4656\n",
      "Epoch [19/50], Step [640/901], Loss: 0.4224\n",
      "Epoch [19/50], Step [650/901], Loss: 0.4286\n",
      "Epoch [19/50], Step [660/901], Loss: 0.4661\n",
      "Epoch [19/50], Step [670/901], Loss: 0.4556\n",
      "Epoch [19/50], Step [680/901], Loss: 0.4375\n",
      "Epoch [19/50], Step [690/901], Loss: 0.4276\n",
      "Epoch [19/50], Step [700/901], Loss: 0.4563\n",
      "Epoch [19/50], Step [710/901], Loss: 0.4417\n",
      "Epoch [19/50], Step [720/901], Loss: 0.4353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/50], Step [730/901], Loss: 0.4800\n",
      "Epoch [19/50], Step [740/901], Loss: 0.4171\n",
      "Epoch [19/50], Step [750/901], Loss: 0.4751\n",
      "Epoch [19/50], Step [760/901], Loss: 0.4200\n",
      "Epoch [19/50], Step [770/901], Loss: 0.4280\n",
      "Epoch [19/50], Step [780/901], Loss: 0.4365\n",
      "Epoch [19/50], Step [790/901], Loss: 0.4475\n",
      "Epoch [19/50], Step [800/901], Loss: 0.4555\n",
      "Epoch [19/50], Step [810/901], Loss: 0.4315\n",
      "Epoch [19/50], Step [820/901], Loss: 0.4353\n",
      "Epoch [19/50], Step [830/901], Loss: 0.5086\n",
      "Epoch [19/50], Step [840/901], Loss: 0.4204\n",
      "Epoch [19/50], Step [850/901], Loss: 0.4603\n",
      "Epoch [19/50], Step [860/901], Loss: 0.4142\n",
      "Epoch [19/50], Step [870/901], Loss: 0.4365\n",
      "Epoch [19/50], Step [880/901], Loss: 0.4476\n",
      "Epoch [19/50], Step [890/901], Loss: 0.4177\n",
      "Epoch [19/50], Step [900/901], Loss: 0.4497\n",
      "\n",
      "train loss: 0.4540, train acc: 87.9927\n",
      "validation loss: 0.7100, validation acc: 97.2105\n",
      "\n",
      "Epoch 20\n",
      "\n",
      "Epoch [20/50], Step [0/901], Loss: 0.4715\n",
      "Epoch [20/50], Step [10/901], Loss: 0.4209\n",
      "Epoch [20/50], Step [20/901], Loss: 0.4479\n",
      "Epoch [20/50], Step [30/901], Loss: 0.4315\n",
      "Epoch [20/50], Step [40/901], Loss: 0.4462\n",
      "Epoch [20/50], Step [50/901], Loss: 0.4411\n",
      "Epoch [20/50], Step [60/901], Loss: 0.4475\n",
      "Epoch [20/50], Step [70/901], Loss: 0.4871\n",
      "Epoch [20/50], Step [80/901], Loss: 0.4506\n",
      "Epoch [20/50], Step [90/901], Loss: 0.4672\n",
      "Epoch [20/50], Step [100/901], Loss: 0.4307\n",
      "Epoch [20/50], Step [110/901], Loss: 0.4303\n",
      "Epoch [20/50], Step [120/901], Loss: 0.4226\n",
      "Epoch [20/50], Step [130/901], Loss: 0.4423\n",
      "Epoch [20/50], Step [140/901], Loss: 0.4342\n",
      "Epoch [20/50], Step [150/901], Loss: 0.4516\n",
      "Epoch [20/50], Step [160/901], Loss: 0.4288\n",
      "Epoch [20/50], Step [170/901], Loss: 0.4536\n",
      "Epoch [20/50], Step [180/901], Loss: 0.4309\n",
      "Epoch [20/50], Step [190/901], Loss: 0.4490\n",
      "Epoch [20/50], Step [200/901], Loss: 0.4265\n",
      "Epoch [20/50], Step [210/901], Loss: 0.4891\n",
      "Epoch [20/50], Step [220/901], Loss: 0.4370\n",
      "Epoch [20/50], Step [230/901], Loss: 0.4463\n",
      "Epoch [20/50], Step [240/901], Loss: 0.4654\n",
      "Epoch [20/50], Step [250/901], Loss: 0.4390\n",
      "Epoch [20/50], Step [260/901], Loss: 0.4343\n",
      "Epoch [20/50], Step [270/901], Loss: 0.4520\n",
      "Epoch [20/50], Step [280/901], Loss: 0.4263\n",
      "Epoch [20/50], Step [290/901], Loss: 0.4644\n",
      "Epoch [20/50], Step [300/901], Loss: 0.4537\n",
      "Epoch [20/50], Step [310/901], Loss: 0.4592\n",
      "Epoch [20/50], Step [320/901], Loss: 0.4376\n",
      "Epoch [20/50], Step [330/901], Loss: 0.4540\n",
      "Epoch [20/50], Step [340/901], Loss: 0.4512\n",
      "Epoch [20/50], Step [350/901], Loss: 0.4547\n",
      "Epoch [20/50], Step [360/901], Loss: 0.4678\n",
      "Epoch [20/50], Step [370/901], Loss: 0.4417\n",
      "Epoch [20/50], Step [380/901], Loss: 0.4248\n",
      "Epoch [20/50], Step [390/901], Loss: 0.4620\n",
      "Epoch [20/50], Step [400/901], Loss: 0.4537\n",
      "Epoch [20/50], Step [410/901], Loss: 0.4942\n",
      "Epoch [20/50], Step [420/901], Loss: 0.4419\n",
      "Epoch [20/50], Step [430/901], Loss: 0.4212\n",
      "Epoch [20/50], Step [440/901], Loss: 0.4189\n",
      "Epoch [20/50], Step [450/901], Loss: 0.4731\n",
      "Epoch [20/50], Step [460/901], Loss: 0.4535\n",
      "Epoch [20/50], Step [470/901], Loss: 0.4067\n",
      "Epoch [20/50], Step [480/901], Loss: 0.4242\n",
      "Epoch [20/50], Step [490/901], Loss: 0.4637\n",
      "Epoch [20/50], Step [500/901], Loss: 0.4502\n",
      "Epoch [20/50], Step [510/901], Loss: 0.4825\n",
      "Epoch [20/50], Step [520/901], Loss: 0.4099\n",
      "Epoch [20/50], Step [530/901], Loss: 0.4508\n",
      "Epoch [20/50], Step [540/901], Loss: 0.4868\n",
      "Epoch [20/50], Step [550/901], Loss: 0.4133\n",
      "Epoch [20/50], Step [560/901], Loss: 0.5024\n",
      "Epoch [20/50], Step [570/901], Loss: 0.4447\n",
      "Epoch [20/50], Step [580/901], Loss: 0.4319\n",
      "Epoch [20/50], Step [590/901], Loss: 0.4153\n",
      "Epoch [20/50], Step [600/901], Loss: 0.4464\n",
      "Epoch [20/50], Step [610/901], Loss: 0.4484\n",
      "Epoch [20/50], Step [620/901], Loss: 0.4455\n",
      "Epoch [20/50], Step [630/901], Loss: 0.4333\n",
      "Epoch [20/50], Step [640/901], Loss: 0.4524\n",
      "Epoch [20/50], Step [650/901], Loss: 0.5079\n",
      "Epoch [20/50], Step [660/901], Loss: 0.4074\n",
      "Epoch [20/50], Step [670/901], Loss: 0.4150\n",
      "Epoch [20/50], Step [680/901], Loss: 0.4691\n",
      "Epoch [20/50], Step [690/901], Loss: 0.4176\n",
      "Epoch [20/50], Step [700/901], Loss: 0.4566\n",
      "Epoch [20/50], Step [710/901], Loss: 0.4499\n",
      "Epoch [20/50], Step [720/901], Loss: 0.4341\n",
      "Epoch [20/50], Step [730/901], Loss: 0.4469\n",
      "Epoch [20/50], Step [740/901], Loss: 0.3996\n",
      "Epoch [20/50], Step [750/901], Loss: 0.4147\n",
      "Epoch [20/50], Step [760/901], Loss: 0.4426\n",
      "Epoch [20/50], Step [770/901], Loss: 0.4679\n",
      "Epoch [20/50], Step [780/901], Loss: 0.4300\n",
      "Epoch [20/50], Step [790/901], Loss: 0.4737\n",
      "Epoch [20/50], Step [800/901], Loss: 0.4424\n",
      "Epoch [20/50], Step [810/901], Loss: 0.4913\n",
      "Epoch [20/50], Step [820/901], Loss: 0.4591\n",
      "Epoch [20/50], Step [830/901], Loss: 0.4195\n",
      "Epoch [20/50], Step [840/901], Loss: 0.4857\n",
      "Epoch [20/50], Step [850/901], Loss: 0.4388\n",
      "Epoch [20/50], Step [860/901], Loss: 0.4046\n",
      "Epoch [20/50], Step [870/901], Loss: 0.4494\n",
      "Epoch [20/50], Step [880/901], Loss: 0.4698\n",
      "Epoch [20/50], Step [890/901], Loss: 0.4834\n",
      "Epoch [20/50], Step [900/901], Loss: 0.4353\n",
      "\n",
      "train loss: 0.4535, train acc: 88.0517\n",
      "validation loss: 0.7102, validation acc: 97.3440\n",
      "\n",
      "Epoch 21\n",
      "\n",
      "Epoch [21/50], Step [0/901], Loss: 0.4525\n",
      "Epoch [21/50], Step [10/901], Loss: 0.4389\n",
      "Epoch [21/50], Step [20/901], Loss: 0.3789\n",
      "Epoch [21/50], Step [30/901], Loss: 0.4637\n",
      "Epoch [21/50], Step [40/901], Loss: 0.4524\n",
      "Epoch [21/50], Step [50/901], Loss: 0.4369\n",
      "Epoch [21/50], Step [60/901], Loss: 0.4190\n",
      "Epoch [21/50], Step [70/901], Loss: 0.4386\n",
      "Epoch [21/50], Step [80/901], Loss: 0.4904\n",
      "Epoch [21/50], Step [90/901], Loss: 0.4693\n",
      "Epoch [21/50], Step [100/901], Loss: 0.4419\n",
      "Epoch [21/50], Step [110/901], Loss: 0.4680\n",
      "Epoch [21/50], Step [120/901], Loss: 0.4482\n",
      "Epoch [21/50], Step [130/901], Loss: 0.4606\n",
      "Epoch [21/50], Step [140/901], Loss: 0.4204\n",
      "Epoch [21/50], Step [150/901], Loss: 0.4599\n",
      "Epoch [21/50], Step [160/901], Loss: 0.4065\n",
      "Epoch [21/50], Step [170/901], Loss: 0.4032\n",
      "Epoch [21/50], Step [180/901], Loss: 0.4810\n",
      "Epoch [21/50], Step [190/901], Loss: 0.4455\n",
      "Epoch [21/50], Step [200/901], Loss: 0.4519\n",
      "Epoch [21/50], Step [210/901], Loss: 0.4491\n",
      "Epoch [21/50], Step [220/901], Loss: 0.4666\n",
      "Epoch [21/50], Step [230/901], Loss: 0.4077\n",
      "Epoch [21/50], Step [240/901], Loss: 0.4334\n",
      "Epoch [21/50], Step [250/901], Loss: 0.4225\n",
      "Epoch [21/50], Step [260/901], Loss: 0.4564\n",
      "Epoch [21/50], Step [270/901], Loss: 0.3986\n",
      "Epoch [21/50], Step [280/901], Loss: 0.4681\n",
      "Epoch [21/50], Step [290/901], Loss: 0.4757\n",
      "Epoch [21/50], Step [300/901], Loss: 0.4509\n",
      "Epoch [21/50], Step [310/901], Loss: 0.4337\n",
      "Epoch [21/50], Step [320/901], Loss: 0.4314\n",
      "Epoch [21/50], Step [330/901], Loss: 0.4241\n",
      "Epoch [21/50], Step [340/901], Loss: 0.4242\n",
      "Epoch [21/50], Step [350/901], Loss: 0.4406\n",
      "Epoch [21/50], Step [360/901], Loss: 0.4922\n",
      "Epoch [21/50], Step [370/901], Loss: 0.4547\n",
      "Epoch [21/50], Step [380/901], Loss: 0.4451\n",
      "Epoch [21/50], Step [390/901], Loss: 0.4335\n",
      "Epoch [21/50], Step [400/901], Loss: 0.4608\n",
      "Epoch [21/50], Step [410/901], Loss: 0.4275\n",
      "Epoch [21/50], Step [420/901], Loss: 0.4192\n",
      "Epoch [21/50], Step [430/901], Loss: 0.4556\n",
      "Epoch [21/50], Step [440/901], Loss: 0.4520\n",
      "Epoch [21/50], Step [450/901], Loss: 0.4668\n",
      "Epoch [21/50], Step [460/901], Loss: 0.4652\n",
      "Epoch [21/50], Step [470/901], Loss: 0.4676\n",
      "Epoch [21/50], Step [480/901], Loss: 0.4538\n",
      "Epoch [21/50], Step [490/901], Loss: 0.4144\n",
      "Epoch [21/50], Step [500/901], Loss: 0.4512\n",
      "Epoch [21/50], Step [510/901], Loss: 0.4499\n",
      "Epoch [21/50], Step [520/901], Loss: 0.4720\n",
      "Epoch [21/50], Step [530/901], Loss: 0.4101\n",
      "Epoch [21/50], Step [540/901], Loss: 0.4773\n",
      "Epoch [21/50], Step [550/901], Loss: 0.4776\n",
      "Epoch [21/50], Step [560/901], Loss: 0.4718\n",
      "Epoch [21/50], Step [570/901], Loss: 0.3808\n",
      "Epoch [21/50], Step [580/901], Loss: 0.4370\n",
      "Epoch [21/50], Step [590/901], Loss: 0.4448\n",
      "Epoch [21/50], Step [600/901], Loss: 0.4913\n",
      "Epoch [21/50], Step [610/901], Loss: 0.4897\n",
      "Epoch [21/50], Step [620/901], Loss: 0.4479\n",
      "Epoch [21/50], Step [630/901], Loss: 0.4503\n",
      "Epoch [21/50], Step [640/901], Loss: 0.4221\n",
      "Epoch [21/50], Step [650/901], Loss: 0.4624\n",
      "Epoch [21/50], Step [660/901], Loss: 0.4785\n",
      "Epoch [21/50], Step [670/901], Loss: 0.4606\n",
      "Epoch [21/50], Step [680/901], Loss: 0.4564\n",
      "Epoch [21/50], Step [690/901], Loss: 0.4401\n",
      "Epoch [21/50], Step [700/901], Loss: 0.3912\n",
      "Epoch [21/50], Step [710/901], Loss: 0.4617\n",
      "Epoch [21/50], Step [720/901], Loss: 0.4396\n",
      "Epoch [21/50], Step [730/901], Loss: 0.4448\n",
      "Epoch [21/50], Step [740/901], Loss: 0.4416\n",
      "Epoch [21/50], Step [750/901], Loss: 0.4580\n",
      "Epoch [21/50], Step [760/901], Loss: 0.4320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/50], Step [770/901], Loss: 0.4396\n",
      "Epoch [21/50], Step [780/901], Loss: 0.4383\n",
      "Epoch [21/50], Step [790/901], Loss: 0.4635\n",
      "Epoch [21/50], Step [800/901], Loss: 0.4511\n",
      "Epoch [21/50], Step [810/901], Loss: 0.4314\n",
      "Epoch [21/50], Step [820/901], Loss: 0.4324\n",
      "Epoch [21/50], Step [830/901], Loss: 0.4615\n",
      "Epoch [21/50], Step [840/901], Loss: 0.4709\n",
      "Epoch [21/50], Step [850/901], Loss: 0.4489\n",
      "Epoch [21/50], Step [860/901], Loss: 0.4274\n",
      "Epoch [21/50], Step [870/901], Loss: 0.4560\n",
      "Epoch [21/50], Step [880/901], Loss: 0.4511\n",
      "Epoch [21/50], Step [890/901], Loss: 0.4810\n",
      "Epoch [21/50], Step [900/901], Loss: 0.4552\n",
      "\n",
      "train loss: 0.4531, train acc: 88.1011\n",
      "validation loss: 0.7096, validation acc: 97.4842\n",
      "\n",
      "Epoch 22\n",
      "\n",
      "Epoch [22/50], Step [0/901], Loss: 0.4085\n",
      "Epoch [22/50], Step [10/901], Loss: 0.4584\n",
      "Epoch [22/50], Step [20/901], Loss: 0.4180\n",
      "Epoch [22/50], Step [30/901], Loss: 0.4287\n",
      "Epoch [22/50], Step [40/901], Loss: 0.4549\n",
      "Epoch [22/50], Step [50/901], Loss: 0.4822\n",
      "Epoch [22/50], Step [60/901], Loss: 0.4354\n",
      "Epoch [22/50], Step [70/901], Loss: 0.4718\n",
      "Epoch [22/50], Step [80/901], Loss: 0.4466\n",
      "Epoch [22/50], Step [90/901], Loss: 0.4339\n",
      "Epoch [22/50], Step [100/901], Loss: 0.4330\n",
      "Epoch [22/50], Step [110/901], Loss: 0.4409\n",
      "Epoch [22/50], Step [120/901], Loss: 0.4587\n",
      "Epoch [22/50], Step [130/901], Loss: 0.4294\n",
      "Epoch [22/50], Step [140/901], Loss: 0.4188\n",
      "Epoch [22/50], Step [150/901], Loss: 0.4362\n",
      "Epoch [22/50], Step [160/901], Loss: 0.3803\n",
      "Epoch [22/50], Step [170/901], Loss: 0.4369\n",
      "Epoch [22/50], Step [180/901], Loss: 0.4444\n",
      "Epoch [22/50], Step [190/901], Loss: 0.4230\n",
      "Epoch [22/50], Step [200/901], Loss: 0.4920\n",
      "Epoch [22/50], Step [210/901], Loss: 0.4633\n",
      "Epoch [22/50], Step [220/901], Loss: 0.4580\n",
      "Epoch [22/50], Step [230/901], Loss: 0.4186\n",
      "Epoch [22/50], Step [240/901], Loss: 0.4487\n",
      "Epoch [22/50], Step [250/901], Loss: 0.4349\n",
      "Epoch [22/50], Step [260/901], Loss: 0.4392\n",
      "Epoch [22/50], Step [270/901], Loss: 0.4321\n",
      "Epoch [22/50], Step [280/901], Loss: 0.4653\n",
      "Epoch [22/50], Step [290/901], Loss: 0.4348\n",
      "Epoch [22/50], Step [300/901], Loss: 0.4320\n",
      "Epoch [22/50], Step [310/901], Loss: 0.4098\n",
      "Epoch [22/50], Step [320/901], Loss: 0.4688\n",
      "Epoch [22/50], Step [330/901], Loss: 0.5129\n",
      "Epoch [22/50], Step [340/901], Loss: 0.4801\n",
      "Epoch [22/50], Step [350/901], Loss: 0.4618\n",
      "Epoch [22/50], Step [360/901], Loss: 0.4568\n",
      "Epoch [22/50], Step [370/901], Loss: 0.4604\n",
      "Epoch [22/50], Step [380/901], Loss: 0.4544\n",
      "Epoch [22/50], Step [390/901], Loss: 0.4518\n",
      "Epoch [22/50], Step [400/901], Loss: 0.4676\n",
      "Epoch [22/50], Step [410/901], Loss: 0.4040\n",
      "Epoch [22/50], Step [420/901], Loss: 0.4663\n",
      "Epoch [22/50], Step [430/901], Loss: 0.4825\n",
      "Epoch [22/50], Step [440/901], Loss: 0.4174\n",
      "Epoch [22/50], Step [450/901], Loss: 0.4342\n",
      "Epoch [22/50], Step [460/901], Loss: 0.4631\n",
      "Epoch [22/50], Step [470/901], Loss: 0.4442\n",
      "Epoch [22/50], Step [480/901], Loss: 0.4376\n",
      "Epoch [22/50], Step [490/901], Loss: 0.4257\n",
      "Epoch [22/50], Step [500/901], Loss: 0.4321\n",
      "Epoch [22/50], Step [510/901], Loss: 0.4769\n",
      "Epoch [22/50], Step [520/901], Loss: 0.4347\n",
      "Epoch [22/50], Step [530/901], Loss: 0.4152\n",
      "Epoch [22/50], Step [540/901], Loss: 0.4841\n",
      "Epoch [22/50], Step [550/901], Loss: 0.4534\n",
      "Epoch [22/50], Step [560/901], Loss: 0.4297\n",
      "Epoch [22/50], Step [570/901], Loss: 0.4118\n",
      "Epoch [22/50], Step [580/901], Loss: 0.4270\n",
      "Epoch [22/50], Step [590/901], Loss: 0.4614\n",
      "Epoch [22/50], Step [600/901], Loss: 0.4367\n",
      "Epoch [22/50], Step [610/901], Loss: 0.5020\n",
      "Epoch [22/50], Step [620/901], Loss: 0.4537\n",
      "Epoch [22/50], Step [630/901], Loss: 0.4995\n",
      "Epoch [22/50], Step [640/901], Loss: 0.4860\n",
      "Epoch [22/50], Step [650/901], Loss: 0.4380\n",
      "Epoch [22/50], Step [660/901], Loss: 0.4705\n",
      "Epoch [22/50], Step [670/901], Loss: 0.4211\n",
      "Epoch [22/50], Step [680/901], Loss: 0.4202\n",
      "Epoch [22/50], Step [690/901], Loss: 0.4880\n",
      "Epoch [22/50], Step [700/901], Loss: 0.4407\n",
      "Epoch [22/50], Step [710/901], Loss: 0.4431\n",
      "Epoch [22/50], Step [720/901], Loss: 0.4496\n",
      "Epoch [22/50], Step [730/901], Loss: 0.4364\n",
      "Epoch [22/50], Step [740/901], Loss: 0.4393\n",
      "Epoch [22/50], Step [750/901], Loss: 0.4060\n",
      "Epoch [22/50], Step [760/901], Loss: 0.4274\n",
      "Epoch [22/50], Step [770/901], Loss: 0.4319\n",
      "Epoch [22/50], Step [780/901], Loss: 0.4242\n",
      "Epoch [22/50], Step [790/901], Loss: 0.4178\n",
      "Epoch [22/50], Step [800/901], Loss: 0.3920\n",
      "Epoch [22/50], Step [810/901], Loss: 0.4326\n",
      "Epoch [22/50], Step [820/901], Loss: 0.4000\n",
      "Epoch [22/50], Step [830/901], Loss: 0.4283\n",
      "Epoch [22/50], Step [840/901], Loss: 0.4380\n",
      "Epoch [22/50], Step [850/901], Loss: 0.4053\n",
      "Epoch [22/50], Step [860/901], Loss: 0.4085\n",
      "Epoch [22/50], Step [870/901], Loss: 0.4451\n",
      "Epoch [22/50], Step [880/901], Loss: 0.4796\n",
      "Epoch [22/50], Step [890/901], Loss: 0.4115\n",
      "Epoch [22/50], Step [900/901], Loss: 0.4568\n",
      "\n",
      "train loss: 0.4527, train acc: 87.9602\n",
      "validation loss: 0.7096, validation acc: 96.6633\n",
      "\n",
      "Epoch 23\n",
      "\n",
      "Epoch [23/50], Step [0/901], Loss: 0.4599\n",
      "Epoch [23/50], Step [10/901], Loss: 0.4574\n",
      "Epoch [23/50], Step [20/901], Loss: 0.4149\n",
      "Epoch [23/50], Step [30/901], Loss: 0.4348\n",
      "Epoch [23/50], Step [40/901], Loss: 0.4340\n",
      "Epoch [23/50], Step [50/901], Loss: 0.4054\n",
      "Epoch [23/50], Step [60/901], Loss: 0.4163\n",
      "Epoch [23/50], Step [70/901], Loss: 0.4210\n",
      "Epoch [23/50], Step [80/901], Loss: 0.4405\n",
      "Epoch [23/50], Step [90/901], Loss: 0.4765\n",
      "Epoch [23/50], Step [100/901], Loss: 0.4774\n",
      "Epoch [23/50], Step [110/901], Loss: 0.4467\n",
      "Epoch [23/50], Step [120/901], Loss: 0.4824\n",
      "Epoch [23/50], Step [130/901], Loss: 0.4270\n",
      "Epoch [23/50], Step [140/901], Loss: 0.4301\n",
      "Epoch [23/50], Step [150/901], Loss: 0.4791\n",
      "Epoch [23/50], Step [160/901], Loss: 0.4556\n",
      "Epoch [23/50], Step [170/901], Loss: 0.4364\n",
      "Epoch [23/50], Step [180/901], Loss: 0.4151\n",
      "Epoch [23/50], Step [190/901], Loss: 0.4450\n",
      "Epoch [23/50], Step [200/901], Loss: 0.4019\n",
      "Epoch [23/50], Step [210/901], Loss: 0.4380\n",
      "Epoch [23/50], Step [220/901], Loss: 0.4135\n",
      "Epoch [23/50], Step [230/901], Loss: 0.4044\n",
      "Epoch [23/50], Step [240/901], Loss: 0.4260\n",
      "Epoch [23/50], Step [250/901], Loss: 0.4479\n",
      "Epoch [23/50], Step [260/901], Loss: 0.4407\n",
      "Epoch [23/50], Step [270/901], Loss: 0.4216\n",
      "Epoch [23/50], Step [280/901], Loss: 0.4699\n",
      "Epoch [23/50], Step [290/901], Loss: 0.4633\n",
      "Epoch [23/50], Step [300/901], Loss: 0.4502\n",
      "Epoch [23/50], Step [310/901], Loss: 0.4369\n",
      "Epoch [23/50], Step [320/901], Loss: 0.4457\n",
      "Epoch [23/50], Step [330/901], Loss: 0.4501\n",
      "Epoch [23/50], Step [340/901], Loss: 0.4119\n",
      "Epoch [23/50], Step [350/901], Loss: 0.4390\n",
      "Epoch [23/50], Step [360/901], Loss: 0.4651\n",
      "Epoch [23/50], Step [370/901], Loss: 0.4565\n",
      "Epoch [23/50], Step [380/901], Loss: 0.4390\n",
      "Epoch [23/50], Step [390/901], Loss: 0.4701\n",
      "Epoch [23/50], Step [400/901], Loss: 0.4046\n",
      "Epoch [23/50], Step [410/901], Loss: 0.4495\n",
      "Epoch [23/50], Step [420/901], Loss: 0.4260\n",
      "Epoch [23/50], Step [430/901], Loss: 0.4620\n",
      "Epoch [23/50], Step [440/901], Loss: 0.4169\n",
      "Epoch [23/50], Step [450/901], Loss: 0.4411\n",
      "Epoch [23/50], Step [460/901], Loss: 0.4747\n",
      "Epoch [23/50], Step [470/901], Loss: 0.4149\n",
      "Epoch [23/50], Step [480/901], Loss: 0.4571\n",
      "Epoch [23/50], Step [490/901], Loss: 0.4645\n",
      "Epoch [23/50], Step [500/901], Loss: 0.4324\n",
      "Epoch [23/50], Step [510/901], Loss: 0.4505\n",
      "Epoch [23/50], Step [520/901], Loss: 0.4280\n",
      "Epoch [23/50], Step [530/901], Loss: 0.4664\n",
      "Epoch [23/50], Step [540/901], Loss: 0.4407\n",
      "Epoch [23/50], Step [550/901], Loss: 0.4220\n",
      "Epoch [23/50], Step [560/901], Loss: 0.4403\n",
      "Epoch [23/50], Step [570/901], Loss: 0.4256\n",
      "Epoch [23/50], Step [580/901], Loss: 0.4666\n",
      "Epoch [23/50], Step [590/901], Loss: 0.4339\n",
      "Epoch [23/50], Step [600/901], Loss: 0.4281\n",
      "Epoch [23/50], Step [610/901], Loss: 0.4453\n",
      "Epoch [23/50], Step [620/901], Loss: 0.4189\n",
      "Epoch [23/50], Step [630/901], Loss: 0.4708\n",
      "Epoch [23/50], Step [640/901], Loss: 0.4934\n",
      "Epoch [23/50], Step [650/901], Loss: 0.4824\n",
      "Epoch [23/50], Step [660/901], Loss: 0.4386\n",
      "Epoch [23/50], Step [670/901], Loss: 0.4170\n",
      "Epoch [23/50], Step [680/901], Loss: 0.4208\n",
      "Epoch [23/50], Step [690/901], Loss: 0.4590\n",
      "Epoch [23/50], Step [700/901], Loss: 0.4726\n",
      "Epoch [23/50], Step [710/901], Loss: 0.4461\n",
      "Epoch [23/50], Step [720/901], Loss: 0.4429\n",
      "Epoch [23/50], Step [730/901], Loss: 0.4612\n",
      "Epoch [23/50], Step [740/901], Loss: 0.4569\n",
      "Epoch [23/50], Step [750/901], Loss: 0.4329\n",
      "Epoch [23/50], Step [760/901], Loss: 0.4181\n",
      "Epoch [23/50], Step [770/901], Loss: 0.4968\n",
      "Epoch [23/50], Step [780/901], Loss: 0.4439\n",
      "Epoch [23/50], Step [790/901], Loss: 0.4388\n",
      "Epoch [23/50], Step [800/901], Loss: 0.4298\n",
      "Epoch [23/50], Step [810/901], Loss: 0.4422\n",
      "Epoch [23/50], Step [820/901], Loss: 0.4344\n",
      "Epoch [23/50], Step [830/901], Loss: 0.4803\n",
      "Epoch [23/50], Step [840/901], Loss: 0.4578\n",
      "Epoch [23/50], Step [850/901], Loss: 0.4557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/50], Step [860/901], Loss: 0.4428\n",
      "Epoch [23/50], Step [870/901], Loss: 0.4311\n",
      "Epoch [23/50], Step [880/901], Loss: 0.4289\n",
      "Epoch [23/50], Step [890/901], Loss: 0.4407\n",
      "Epoch [23/50], Step [900/901], Loss: 0.4830\n",
      "\n",
      "train loss: 0.4523, train acc: 88.0881\n",
      "validation loss: 0.7098, validation acc: 96.9770\n",
      "\n",
      "Epoch 24\n",
      "\n",
      "Epoch [24/50], Step [0/901], Loss: 0.4355\n",
      "Epoch [24/50], Step [10/901], Loss: 0.4437\n",
      "Epoch [24/50], Step [20/901], Loss: 0.4407\n",
      "Epoch [24/50], Step [30/901], Loss: 0.3993\n",
      "Epoch [24/50], Step [40/901], Loss: 0.4386\n",
      "Epoch [24/50], Step [50/901], Loss: 0.3831\n",
      "Epoch [24/50], Step [60/901], Loss: 0.4719\n",
      "Epoch [24/50], Step [70/901], Loss: 0.4396\n",
      "Epoch [24/50], Step [80/901], Loss: 0.4032\n",
      "Epoch [24/50], Step [90/901], Loss: 0.4166\n",
      "Epoch [24/50], Step [100/901], Loss: 0.4878\n",
      "Epoch [24/50], Step [110/901], Loss: 0.4425\n",
      "Epoch [24/50], Step [120/901], Loss: 0.4409\n",
      "Epoch [24/50], Step [130/901], Loss: 0.4688\n",
      "Epoch [24/50], Step [140/901], Loss: 0.4448\n",
      "Epoch [24/50], Step [150/901], Loss: 0.4043\n",
      "Epoch [24/50], Step [160/901], Loss: 0.4890\n",
      "Epoch [24/50], Step [170/901], Loss: 0.4765\n",
      "Epoch [24/50], Step [180/901], Loss: 0.4140\n",
      "Epoch [24/50], Step [190/901], Loss: 0.4386\n",
      "Epoch [24/50], Step [200/901], Loss: 0.4568\n",
      "Epoch [24/50], Step [210/901], Loss: 0.4181\n",
      "Epoch [24/50], Step [220/901], Loss: 0.4693\n",
      "Epoch [24/50], Step [230/901], Loss: 0.4198\n",
      "Epoch [24/50], Step [240/901], Loss: 0.4229\n",
      "Epoch [24/50], Step [250/901], Loss: 0.4683\n",
      "Epoch [24/50], Step [260/901], Loss: 0.4408\n",
      "Epoch [24/50], Step [270/901], Loss: 0.4255\n",
      "Epoch [24/50], Step [280/901], Loss: 0.4826\n",
      "Epoch [24/50], Step [290/901], Loss: 0.4305\n",
      "Epoch [24/50], Step [300/901], Loss: 0.4470\n",
      "Epoch [24/50], Step [310/901], Loss: 0.4201\n",
      "Epoch [24/50], Step [320/901], Loss: 0.4696\n",
      "Epoch [24/50], Step [330/901], Loss: 0.4465\n",
      "Epoch [24/50], Step [340/901], Loss: 0.4462\n",
      "Epoch [24/50], Step [350/901], Loss: 0.4837\n",
      "Epoch [24/50], Step [360/901], Loss: 0.4478\n",
      "Epoch [24/50], Step [370/901], Loss: 0.4140\n",
      "Epoch [24/50], Step [380/901], Loss: 0.4589\n",
      "Epoch [24/50], Step [390/901], Loss: 0.4190\n",
      "Epoch [24/50], Step [400/901], Loss: 0.4441\n",
      "Epoch [24/50], Step [410/901], Loss: 0.4524\n",
      "Epoch [24/50], Step [420/901], Loss: 0.5035\n",
      "Epoch [24/50], Step [430/901], Loss: 0.4463\n",
      "Epoch [24/50], Step [440/901], Loss: 0.4421\n",
      "Epoch [24/50], Step [450/901], Loss: 0.4315\n",
      "Epoch [24/50], Step [460/901], Loss: 0.4476\n",
      "Epoch [24/50], Step [470/901], Loss: 0.4252\n",
      "Epoch [24/50], Step [480/901], Loss: 0.4140\n",
      "Epoch [24/50], Step [490/901], Loss: 0.4110\n",
      "Epoch [24/50], Step [500/901], Loss: 0.4477\n",
      "Epoch [24/50], Step [510/901], Loss: 0.4282\n",
      "Epoch [24/50], Step [520/901], Loss: 0.4855\n",
      "Epoch [24/50], Step [530/901], Loss: 0.4108\n",
      "Epoch [24/50], Step [540/901], Loss: 0.4479\n",
      "Epoch [24/50], Step [550/901], Loss: 0.4406\n",
      "Epoch [24/50], Step [560/901], Loss: 0.4460\n",
      "Epoch [24/50], Step [570/901], Loss: 0.4654\n",
      "Epoch [24/50], Step [580/901], Loss: 0.4430\n",
      "Epoch [24/50], Step [590/901], Loss: 0.4587\n",
      "Epoch [24/50], Step [600/901], Loss: 0.4701\n",
      "Epoch [24/50], Step [610/901], Loss: 0.4412\n",
      "Epoch [24/50], Step [620/901], Loss: 0.4336\n",
      "Epoch [24/50], Step [630/901], Loss: 0.4473\n",
      "Epoch [24/50], Step [640/901], Loss: 0.4646\n",
      "Epoch [24/50], Step [650/901], Loss: 0.4439\n",
      "Epoch [24/50], Step [660/901], Loss: 0.4628\n",
      "Epoch [24/50], Step [670/901], Loss: 0.4528\n",
      "Epoch [24/50], Step [680/901], Loss: 0.4365\n",
      "Epoch [24/50], Step [690/901], Loss: 0.4478\n",
      "Epoch [24/50], Step [700/901], Loss: 0.4378\n",
      "Epoch [24/50], Step [710/901], Loss: 0.4281\n",
      "Epoch [24/50], Step [720/901], Loss: 0.4538\n",
      "Epoch [24/50], Step [730/901], Loss: 0.4319\n",
      "Epoch [24/50], Step [740/901], Loss: 0.4672\n",
      "Epoch [24/50], Step [750/901], Loss: 0.4048\n",
      "Epoch [24/50], Step [760/901], Loss: 0.4281\n",
      "Epoch [24/50], Step [770/901], Loss: 0.4609\n",
      "Epoch [24/50], Step [780/901], Loss: 0.4193\n",
      "Epoch [24/50], Step [790/901], Loss: 0.4548\n",
      "Epoch [24/50], Step [800/901], Loss: 0.3976\n",
      "Epoch [24/50], Step [810/901], Loss: 0.4637\n",
      "Epoch [24/50], Step [820/901], Loss: 0.4713\n",
      "Epoch [24/50], Step [830/901], Loss: 0.4430\n",
      "Epoch [24/50], Step [840/901], Loss: 0.4252\n",
      "Epoch [24/50], Step [850/901], Loss: 0.4565\n",
      "Epoch [24/50], Step [860/901], Loss: 0.4638\n",
      "Epoch [24/50], Step [870/901], Loss: 0.4395\n",
      "Epoch [24/50], Step [880/901], Loss: 0.4085\n",
      "Epoch [24/50], Step [890/901], Loss: 0.4153\n",
      "Epoch [24/50], Step [900/901], Loss: 0.4426\n",
      "\n",
      "train loss: 0.4520, train acc: 88.1327\n",
      "validation loss: 0.7093, validation acc: 97.4641\n",
      "\n",
      "Epoch 25\n",
      "\n",
      "Epoch [25/50], Step [0/901], Loss: 0.4450\n",
      "Epoch [25/50], Step [10/901], Loss: 0.4465\n",
      "Epoch [25/50], Step [20/901], Loss: 0.4285\n",
      "Epoch [25/50], Step [30/901], Loss: 0.4534\n",
      "Epoch [25/50], Step [40/901], Loss: 0.4611\n",
      "Epoch [25/50], Step [50/901], Loss: 0.4016\n",
      "Epoch [25/50], Step [60/901], Loss: 0.4433\n",
      "Epoch [25/50], Step [70/901], Loss: 0.4817\n",
      "Epoch [25/50], Step [80/901], Loss: 0.4784\n",
      "Epoch [25/50], Step [90/901], Loss: 0.4565\n",
      "Epoch [25/50], Step [100/901], Loss: 0.4377\n",
      "Epoch [25/50], Step [110/901], Loss: 0.4722\n",
      "Epoch [25/50], Step [120/901], Loss: 0.4576\n",
      "Epoch [25/50], Step [130/901], Loss: 0.4270\n",
      "Epoch [25/50], Step [140/901], Loss: 0.4303\n",
      "Epoch [25/50], Step [150/901], Loss: 0.4285\n",
      "Epoch [25/50], Step [160/901], Loss: 0.4830\n",
      "Epoch [25/50], Step [170/901], Loss: 0.4226\n",
      "Epoch [25/50], Step [180/901], Loss: 0.4536\n",
      "Epoch [25/50], Step [190/901], Loss: 0.4807\n",
      "Epoch [25/50], Step [200/901], Loss: 0.4313\n",
      "Epoch [25/50], Step [210/901], Loss: 0.4751\n",
      "Epoch [25/50], Step [220/901], Loss: 0.4338\n",
      "Epoch [25/50], Step [230/901], Loss: 0.4448\n",
      "Epoch [25/50], Step [240/901], Loss: 0.4475\n",
      "Epoch [25/50], Step [250/901], Loss: 0.4317\n",
      "Epoch [25/50], Step [260/901], Loss: 0.4752\n",
      "Epoch [25/50], Step [270/901], Loss: 0.4454\n",
      "Epoch [25/50], Step [280/901], Loss: 0.4274\n",
      "Epoch [25/50], Step [290/901], Loss: 0.4609\n",
      "Epoch [25/50], Step [300/901], Loss: 0.4316\n",
      "Epoch [25/50], Step [310/901], Loss: 0.4607\n",
      "Epoch [25/50], Step [320/901], Loss: 0.4642\n",
      "Epoch [25/50], Step [330/901], Loss: 0.4272\n",
      "Epoch [25/50], Step [340/901], Loss: 0.4221\n",
      "Epoch [25/50], Step [350/901], Loss: 0.4187\n",
      "Epoch [25/50], Step [360/901], Loss: 0.4373\n",
      "Epoch [25/50], Step [370/901], Loss: 0.4593\n",
      "Epoch [25/50], Step [380/901], Loss: 0.4606\n",
      "Epoch [25/50], Step [390/901], Loss: 0.4420\n",
      "Epoch [25/50], Step [400/901], Loss: 0.4659\n",
      "Epoch [25/50], Step [410/901], Loss: 0.4398\n",
      "Epoch [25/50], Step [420/901], Loss: 0.4264\n",
      "Epoch [25/50], Step [430/901], Loss: 0.4004\n",
      "Epoch [25/50], Step [440/901], Loss: 0.4251\n",
      "Epoch [25/50], Step [450/901], Loss: 0.4684\n",
      "Epoch [25/50], Step [460/901], Loss: 0.4593\n",
      "Epoch [25/50], Step [470/901], Loss: 0.4482\n",
      "Epoch [25/50], Step [480/901], Loss: 0.4325\n",
      "Epoch [25/50], Step [490/901], Loss: 0.4346\n",
      "Epoch [25/50], Step [500/901], Loss: 0.4668\n",
      "Epoch [25/50], Step [510/901], Loss: 0.4077\n",
      "Epoch [25/50], Step [520/901], Loss: 0.4469\n",
      "Epoch [25/50], Step [530/901], Loss: 0.4282\n",
      "Epoch [25/50], Step [540/901], Loss: 0.4781\n",
      "Epoch [25/50], Step [550/901], Loss: 0.4758\n",
      "Epoch [25/50], Step [560/901], Loss: 0.4024\n",
      "Epoch [25/50], Step [570/901], Loss: 0.4432\n",
      "Epoch [25/50], Step [580/901], Loss: 0.4363\n",
      "Epoch [25/50], Step [590/901], Loss: 0.4437\n",
      "Epoch [25/50], Step [600/901], Loss: 0.4225\n",
      "Epoch [25/50], Step [610/901], Loss: 0.4759\n",
      "Epoch [25/50], Step [620/901], Loss: 0.4386\n",
      "Epoch [25/50], Step [630/901], Loss: 0.4359\n",
      "Epoch [25/50], Step [640/901], Loss: 0.4595\n",
      "Epoch [25/50], Step [650/901], Loss: 0.4481\n",
      "Epoch [25/50], Step [660/901], Loss: 0.4317\n",
      "Epoch [25/50], Step [670/901], Loss: 0.4434\n",
      "Epoch [25/50], Step [680/901], Loss: 0.4049\n",
      "Epoch [25/50], Step [690/901], Loss: 0.4049\n",
      "Epoch [25/50], Step [700/901], Loss: 0.4727\n",
      "Epoch [25/50], Step [710/901], Loss: 0.4832\n",
      "Epoch [25/50], Step [720/901], Loss: 0.4305\n",
      "Epoch [25/50], Step [730/901], Loss: 0.4235\n",
      "Epoch [25/50], Step [740/901], Loss: 0.4909\n",
      "Epoch [25/50], Step [750/901], Loss: 0.4401\n",
      "Epoch [25/50], Step [760/901], Loss: 0.4431\n",
      "Epoch [25/50], Step [770/901], Loss: 0.4450\n",
      "Epoch [25/50], Step [780/901], Loss: 0.4339\n",
      "Epoch [25/50], Step [790/901], Loss: 0.4061\n",
      "Epoch [25/50], Step [800/901], Loss: 0.4648\n",
      "Epoch [25/50], Step [810/901], Loss: 0.4943\n",
      "Epoch [25/50], Step [820/901], Loss: 0.4519\n",
      "Epoch [25/50], Step [830/901], Loss: 0.4247\n",
      "Epoch [25/50], Step [840/901], Loss: 0.4962\n",
      "Epoch [25/50], Step [850/901], Loss: 0.4538\n",
      "Epoch [25/50], Step [860/901], Loss: 0.4309\n",
      "Epoch [25/50], Step [870/901], Loss: 0.4458\n",
      "Epoch [25/50], Step [880/901], Loss: 0.4170\n",
      "Epoch [25/50], Step [890/901], Loss: 0.4493\n",
      "Epoch [25/50], Step [900/901], Loss: 0.4591\n",
      "\n",
      "train loss: 0.4516, train acc: 88.1327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.7088, validation acc: 97.6510\n",
      "\n",
      "Epoch 26\n",
      "\n",
      "Epoch [26/50], Step [0/901], Loss: 0.4525\n",
      "Epoch [26/50], Step [10/901], Loss: 0.4366\n",
      "Epoch [26/50], Step [20/901], Loss: 0.4139\n",
      "Epoch [26/50], Step [30/901], Loss: 0.4842\n",
      "Epoch [26/50], Step [40/901], Loss: 0.4232\n",
      "Epoch [26/50], Step [50/901], Loss: 0.4560\n",
      "Epoch [26/50], Step [60/901], Loss: 0.4168\n",
      "Epoch [26/50], Step [70/901], Loss: 0.4594\n",
      "Epoch [26/50], Step [80/901], Loss: 0.4260\n",
      "Epoch [26/50], Step [90/901], Loss: 0.3993\n",
      "Epoch [26/50], Step [100/901], Loss: 0.4376\n",
      "Epoch [26/50], Step [110/901], Loss: 0.3814\n",
      "Epoch [26/50], Step [120/901], Loss: 0.4475\n",
      "Epoch [26/50], Step [130/901], Loss: 0.4123\n",
      "Epoch [26/50], Step [140/901], Loss: 0.4362\n",
      "Epoch [26/50], Step [150/901], Loss: 0.4907\n",
      "Epoch [26/50], Step [160/901], Loss: 0.4345\n",
      "Epoch [26/50], Step [170/901], Loss: 0.4611\n",
      "Epoch [26/50], Step [180/901], Loss: 0.4806\n",
      "Epoch [26/50], Step [190/901], Loss: 0.4385\n",
      "Epoch [26/50], Step [200/901], Loss: 0.4671\n",
      "Epoch [26/50], Step [210/901], Loss: 0.4197\n",
      "Epoch [26/50], Step [220/901], Loss: 0.4346\n",
      "Epoch [26/50], Step [230/901], Loss: 0.4490\n",
      "Epoch [26/50], Step [240/901], Loss: 0.4602\n",
      "Epoch [26/50], Step [250/901], Loss: 0.4711\n",
      "Epoch [26/50], Step [260/901], Loss: 0.4605\n",
      "Epoch [26/50], Step [270/901], Loss: 0.4162\n",
      "Epoch [26/50], Step [280/901], Loss: 0.4617\n",
      "Epoch [26/50], Step [290/901], Loss: 0.4464\n",
      "Epoch [26/50], Step [300/901], Loss: 0.4623\n",
      "Epoch [26/50], Step [310/901], Loss: 0.4223\n",
      "Epoch [26/50], Step [320/901], Loss: 0.4430\n",
      "Epoch [26/50], Step [330/901], Loss: 0.4502\n",
      "Epoch [26/50], Step [340/901], Loss: 0.4432\n",
      "Epoch [26/50], Step [350/901], Loss: 0.4393\n",
      "Epoch [26/50], Step [360/901], Loss: 0.4616\n",
      "Epoch [26/50], Step [370/901], Loss: 0.4281\n",
      "Epoch [26/50], Step [380/901], Loss: 0.4280\n",
      "Epoch [26/50], Step [390/901], Loss: 0.4247\n",
      "Epoch [26/50], Step [400/901], Loss: 0.4238\n",
      "Epoch [26/50], Step [410/901], Loss: 0.4314\n",
      "Epoch [26/50], Step [420/901], Loss: 0.4273\n",
      "Epoch [26/50], Step [430/901], Loss: 0.4196\n",
      "Epoch [26/50], Step [440/901], Loss: 0.4720\n",
      "Epoch [26/50], Step [450/901], Loss: 0.4515\n",
      "Epoch [26/50], Step [460/901], Loss: 0.4490\n",
      "Epoch [26/50], Step [470/901], Loss: 0.4273\n",
      "Epoch [26/50], Step [480/901], Loss: 0.4363\n",
      "Epoch [26/50], Step [490/901], Loss: 0.4102\n",
      "Epoch [26/50], Step [500/901], Loss: 0.4414\n",
      "Epoch [26/50], Step [510/901], Loss: 0.4185\n",
      "Epoch [26/50], Step [520/901], Loss: 0.4586\n",
      "Epoch [26/50], Step [530/901], Loss: 0.4327\n",
      "Epoch [26/50], Step [540/901], Loss: 0.4387\n",
      "Epoch [26/50], Step [550/901], Loss: 0.4019\n",
      "Epoch [26/50], Step [560/901], Loss: 0.4130\n",
      "Epoch [26/50], Step [570/901], Loss: 0.4467\n",
      "Epoch [26/50], Step [580/901], Loss: 0.4644\n",
      "Epoch [26/50], Step [590/901], Loss: 0.4905\n",
      "Epoch [26/50], Step [600/901], Loss: 0.4210\n",
      "Epoch [26/50], Step [610/901], Loss: 0.4393\n",
      "Epoch [26/50], Step [620/901], Loss: 0.4641\n",
      "Epoch [26/50], Step [630/901], Loss: 0.4571\n",
      "Epoch [26/50], Step [640/901], Loss: 0.4533\n",
      "Epoch [26/50], Step [650/901], Loss: 0.4351\n",
      "Epoch [26/50], Step [660/901], Loss: 0.4359\n",
      "Epoch [26/50], Step [670/901], Loss: 0.4376\n",
      "Epoch [26/50], Step [680/901], Loss: 0.4128\n",
      "Epoch [26/50], Step [690/901], Loss: 0.4636\n",
      "Epoch [26/50], Step [700/901], Loss: 0.4504\n",
      "Epoch [26/50], Step [710/901], Loss: 0.4435\n",
      "Epoch [26/50], Step [720/901], Loss: 0.4471\n",
      "Epoch [26/50], Step [730/901], Loss: 0.4168\n",
      "Epoch [26/50], Step [740/901], Loss: 0.4543\n",
      "Epoch [26/50], Step [750/901], Loss: 0.4453\n",
      "Epoch [26/50], Step [760/901], Loss: 0.4274\n",
      "Epoch [26/50], Step [770/901], Loss: 0.4499\n",
      "Epoch [26/50], Step [780/901], Loss: 0.4248\n",
      "Epoch [26/50], Step [790/901], Loss: 0.4374\n",
      "Epoch [26/50], Step [800/901], Loss: 0.4560\n",
      "Epoch [26/50], Step [810/901], Loss: 0.4464\n",
      "Epoch [26/50], Step [820/901], Loss: 0.4550\n",
      "Epoch [26/50], Step [830/901], Loss: 0.3953\n",
      "Epoch [26/50], Step [840/901], Loss: 0.4082\n",
      "Epoch [26/50], Step [850/901], Loss: 0.4552\n",
      "Epoch [26/50], Step [860/901], Loss: 0.4761\n",
      "Epoch [26/50], Step [870/901], Loss: 0.4524\n",
      "Epoch [26/50], Step [880/901], Loss: 0.4638\n",
      "Epoch [26/50], Step [890/901], Loss: 0.4173\n",
      "Epoch [26/50], Step [900/901], Loss: 0.4348\n",
      "\n",
      "train loss: 0.4513, train acc: 88.0928\n",
      "validation loss: 0.7085, validation acc: 97.2372\n",
      "\n",
      "Epoch 27\n",
      "\n",
      "Epoch [27/50], Step [0/901], Loss: 0.4500\n",
      "Epoch [27/50], Step [10/901], Loss: 0.4119\n",
      "Epoch [27/50], Step [20/901], Loss: 0.3897\n",
      "Epoch [27/50], Step [30/901], Loss: 0.4528\n",
      "Epoch [27/50], Step [40/901], Loss: 0.4118\n",
      "Epoch [27/50], Step [50/901], Loss: 0.4187\n",
      "Epoch [27/50], Step [60/901], Loss: 0.4627\n",
      "Epoch [27/50], Step [70/901], Loss: 0.4624\n",
      "Epoch [27/50], Step [80/901], Loss: 0.4683\n",
      "Epoch [27/50], Step [90/901], Loss: 0.4565\n",
      "Epoch [27/50], Step [100/901], Loss: 0.4375\n",
      "Epoch [27/50], Step [110/901], Loss: 0.4597\n",
      "Epoch [27/50], Step [120/901], Loss: 0.4768\n",
      "Epoch [27/50], Step [130/901], Loss: 0.4180\n",
      "Epoch [27/50], Step [140/901], Loss: 0.4426\n",
      "Epoch [27/50], Step [150/901], Loss: 0.4379\n",
      "Epoch [27/50], Step [160/901], Loss: 0.4870\n",
      "Epoch [27/50], Step [170/901], Loss: 0.4566\n",
      "Epoch [27/50], Step [180/901], Loss: 0.4246\n",
      "Epoch [27/50], Step [190/901], Loss: 0.4535\n",
      "Epoch [27/50], Step [200/901], Loss: 0.4308\n",
      "Epoch [27/50], Step [210/901], Loss: 0.4315\n",
      "Epoch [27/50], Step [220/901], Loss: 0.4333\n",
      "Epoch [27/50], Step [230/901], Loss: 0.4751\n",
      "Epoch [27/50], Step [240/901], Loss: 0.4561\n",
      "Epoch [27/50], Step [250/901], Loss: 0.4286\n",
      "Epoch [27/50], Step [260/901], Loss: 0.4317\n",
      "Epoch [27/50], Step [270/901], Loss: 0.4322\n",
      "Epoch [27/50], Step [280/901], Loss: 0.4695\n",
      "Epoch [27/50], Step [290/901], Loss: 0.4091\n",
      "Epoch [27/50], Step [300/901], Loss: 0.4551\n",
      "Epoch [27/50], Step [310/901], Loss: 0.4234\n",
      "Epoch [27/50], Step [320/901], Loss: 0.4842\n",
      "Epoch [27/50], Step [330/901], Loss: 0.4076\n",
      "Epoch [27/50], Step [340/901], Loss: 0.4296\n",
      "Epoch [27/50], Step [350/901], Loss: 0.4349\n",
      "Epoch [27/50], Step [360/901], Loss: 0.4425\n",
      "Epoch [27/50], Step [370/901], Loss: 0.4095\n",
      "Epoch [27/50], Step [380/901], Loss: 0.4365\n",
      "Epoch [27/50], Step [390/901], Loss: 0.4329\n",
      "Epoch [27/50], Step [400/901], Loss: 0.4515\n",
      "Epoch [27/50], Step [410/901], Loss: 0.4450\n",
      "Epoch [27/50], Step [420/901], Loss: 0.4495\n",
      "Epoch [27/50], Step [430/901], Loss: 0.4318\n",
      "Epoch [27/50], Step [440/901], Loss: 0.4233\n",
      "Epoch [27/50], Step [450/901], Loss: 0.4289\n",
      "Epoch [27/50], Step [460/901], Loss: 0.4283\n",
      "Epoch [27/50], Step [470/901], Loss: 0.4413\n",
      "Epoch [27/50], Step [480/901], Loss: 0.3830\n",
      "Epoch [27/50], Step [490/901], Loss: 0.4582\n",
      "Epoch [27/50], Step [500/901], Loss: 0.4342\n",
      "Epoch [27/50], Step [510/901], Loss: 0.4338\n",
      "Epoch [27/50], Step [520/901], Loss: 0.4125\n",
      "Epoch [27/50], Step [530/901], Loss: 0.4186\n",
      "Epoch [27/50], Step [540/901], Loss: 0.4078\n",
      "Epoch [27/50], Step [550/901], Loss: 0.4564\n",
      "Epoch [27/50], Step [560/901], Loss: 0.4290\n",
      "Epoch [27/50], Step [570/901], Loss: 0.4025\n",
      "Epoch [27/50], Step [580/901], Loss: 0.4363\n",
      "Epoch [27/50], Step [590/901], Loss: 0.4474\n",
      "Epoch [27/50], Step [600/901], Loss: 0.4634\n",
      "Epoch [27/50], Step [610/901], Loss: 0.4244\n",
      "Epoch [27/50], Step [620/901], Loss: 0.4601\n",
      "Epoch [27/50], Step [630/901], Loss: 0.4154\n",
      "Epoch [27/50], Step [640/901], Loss: 0.5050\n",
      "Epoch [27/50], Step [650/901], Loss: 0.4584\n",
      "Epoch [27/50], Step [660/901], Loss: 0.4225\n",
      "Epoch [27/50], Step [670/901], Loss: 0.5016\n",
      "Epoch [27/50], Step [680/901], Loss: 0.4259\n",
      "Epoch [27/50], Step [690/901], Loss: 0.4229\n",
      "Epoch [27/50], Step [700/901], Loss: 0.4178\n",
      "Epoch [27/50], Step [710/901], Loss: 0.4878\n",
      "Epoch [27/50], Step [720/901], Loss: 0.4740\n",
      "Epoch [27/50], Step [730/901], Loss: 0.4427\n",
      "Epoch [27/50], Step [740/901], Loss: 0.4150\n",
      "Epoch [27/50], Step [750/901], Loss: 0.4510\n",
      "Epoch [27/50], Step [760/901], Loss: 0.4335\n",
      "Epoch [27/50], Step [770/901], Loss: 0.4295\n",
      "Epoch [27/50], Step [780/901], Loss: 0.4512\n",
      "Epoch [27/50], Step [790/901], Loss: 0.4475\n",
      "Epoch [27/50], Step [800/901], Loss: 0.4372\n",
      "Epoch [27/50], Step [810/901], Loss: 0.5182\n",
      "Epoch [27/50], Step [820/901], Loss: 0.4420\n",
      "Epoch [27/50], Step [830/901], Loss: 0.4461\n",
      "Epoch [27/50], Step [840/901], Loss: 0.4213\n",
      "Epoch [27/50], Step [850/901], Loss: 0.4400\n",
      "Epoch [27/50], Step [860/901], Loss: 0.4236\n",
      "Epoch [27/50], Step [870/901], Loss: 0.4351\n",
      "Epoch [27/50], Step [880/901], Loss: 0.4272\n",
      "Epoch [27/50], Step [890/901], Loss: 0.4363\n",
      "Epoch [27/50], Step [900/901], Loss: 0.4515\n",
      "\n",
      "train loss: 0.4509, train acc: 88.2212\n",
      "validation loss: 0.7079, validation acc: 97.7911\n",
      "\n",
      "Epoch 28\n",
      "\n",
      "Epoch [28/50], Step [0/901], Loss: 0.4182\n",
      "Epoch [28/50], Step [10/901], Loss: 0.4190\n",
      "Epoch [28/50], Step [20/901], Loss: 0.4678\n",
      "Epoch [28/50], Step [30/901], Loss: 0.4103\n",
      "Epoch [28/50], Step [40/901], Loss: 0.4423\n",
      "Epoch [28/50], Step [50/901], Loss: 0.4295\n",
      "Epoch [28/50], Step [60/901], Loss: 0.4750\n",
      "Epoch [28/50], Step [70/901], Loss: 0.4667\n",
      "Epoch [28/50], Step [80/901], Loss: 0.4514\n",
      "Epoch [28/50], Step [90/901], Loss: 0.4470\n",
      "Epoch [28/50], Step [100/901], Loss: 0.4438\n",
      "Epoch [28/50], Step [110/901], Loss: 0.4543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/50], Step [120/901], Loss: 0.4461\n",
      "Epoch [28/50], Step [130/901], Loss: 0.4403\n",
      "Epoch [28/50], Step [140/901], Loss: 0.4435\n",
      "Epoch [28/50], Step [150/901], Loss: 0.4572\n",
      "Epoch [28/50], Step [160/901], Loss: 0.4902\n",
      "Epoch [28/50], Step [170/901], Loss: 0.4660\n",
      "Epoch [28/50], Step [180/901], Loss: 0.4538\n",
      "Epoch [28/50], Step [190/901], Loss: 0.4419\n",
      "Epoch [28/50], Step [200/901], Loss: 0.4158\n",
      "Epoch [28/50], Step [210/901], Loss: 0.4031\n",
      "Epoch [28/50], Step [220/901], Loss: 0.4623\n",
      "Epoch [28/50], Step [230/901], Loss: 0.4664\n",
      "Epoch [28/50], Step [240/901], Loss: 0.4400\n",
      "Epoch [28/50], Step [250/901], Loss: 0.4533\n",
      "Epoch [28/50], Step [260/901], Loss: 0.4673\n",
      "Epoch [28/50], Step [270/901], Loss: 0.5166\n",
      "Epoch [28/50], Step [280/901], Loss: 0.4257\n",
      "Epoch [28/50], Step [290/901], Loss: 0.4354\n",
      "Epoch [28/50], Step [300/901], Loss: 0.4998\n",
      "Epoch [28/50], Step [310/901], Loss: 0.4478\n",
      "Epoch [28/50], Step [320/901], Loss: 0.4329\n",
      "Epoch [28/50], Step [330/901], Loss: 0.4410\n",
      "Epoch [28/50], Step [340/901], Loss: 0.4274\n",
      "Epoch [28/50], Step [350/901], Loss: 0.4711\n",
      "Epoch [28/50], Step [360/901], Loss: 0.4213\n",
      "Epoch [28/50], Step [370/901], Loss: 0.4419\n",
      "Epoch [28/50], Step [380/901], Loss: 0.4532\n",
      "Epoch [28/50], Step [390/901], Loss: 0.4211\n",
      "Epoch [28/50], Step [400/901], Loss: 0.4556\n",
      "Epoch [28/50], Step [410/901], Loss: 0.4581\n",
      "Epoch [28/50], Step [420/901], Loss: 0.4075\n",
      "Epoch [28/50], Step [430/901], Loss: 0.4053\n",
      "Epoch [28/50], Step [440/901], Loss: 0.4401\n",
      "Epoch [28/50], Step [450/901], Loss: 0.4402\n",
      "Epoch [28/50], Step [460/901], Loss: 0.4391\n",
      "Epoch [28/50], Step [470/901], Loss: 0.4537\n",
      "Epoch [28/50], Step [480/901], Loss: 0.4457\n",
      "Epoch [28/50], Step [490/901], Loss: 0.4302\n",
      "Epoch [28/50], Step [500/901], Loss: 0.4230\n",
      "Epoch [28/50], Step [510/901], Loss: 0.4444\n",
      "Epoch [28/50], Step [520/901], Loss: 0.4456\n",
      "Epoch [28/50], Step [530/901], Loss: 0.4221\n",
      "Epoch [28/50], Step [540/901], Loss: 0.4479\n",
      "Epoch [28/50], Step [550/901], Loss: 0.4158\n",
      "Epoch [28/50], Step [560/901], Loss: 0.4532\n",
      "Epoch [28/50], Step [570/901], Loss: 0.4674\n",
      "Epoch [28/50], Step [580/901], Loss: 0.4573\n",
      "Epoch [28/50], Step [590/901], Loss: 0.4537\n",
      "Epoch [28/50], Step [600/901], Loss: 0.4344\n",
      "Epoch [28/50], Step [610/901], Loss: 0.4217\n",
      "Epoch [28/50], Step [620/901], Loss: 0.4120\n",
      "Epoch [28/50], Step [630/901], Loss: 0.4643\n",
      "Epoch [28/50], Step [640/901], Loss: 0.4476\n",
      "Epoch [28/50], Step [650/901], Loss: 0.4114\n",
      "Epoch [28/50], Step [660/901], Loss: 0.3895\n",
      "Epoch [28/50], Step [670/901], Loss: 0.4538\n",
      "Epoch [28/50], Step [680/901], Loss: 0.4328\n",
      "Epoch [28/50], Step [690/901], Loss: 0.4523\n",
      "Epoch [28/50], Step [700/901], Loss: 0.4489\n",
      "Epoch [28/50], Step [710/901], Loss: 0.4195\n",
      "Epoch [28/50], Step [720/901], Loss: 0.4715\n",
      "Epoch [28/50], Step [730/901], Loss: 0.4294\n",
      "Epoch [28/50], Step [740/901], Loss: 0.4284\n",
      "Epoch [28/50], Step [750/901], Loss: 0.4362\n",
      "Epoch [28/50], Step [760/901], Loss: 0.4115\n",
      "Epoch [28/50], Step [770/901], Loss: 0.4208\n",
      "Epoch [28/50], Step [780/901], Loss: 0.4297\n",
      "Epoch [28/50], Step [790/901], Loss: 0.4086\n",
      "Epoch [28/50], Step [800/901], Loss: 0.4545\n",
      "Epoch [28/50], Step [810/901], Loss: 0.4780\n",
      "Epoch [28/50], Step [820/901], Loss: 0.4671\n",
      "Epoch [28/50], Step [830/901], Loss: 0.4468\n",
      "Epoch [28/50], Step [840/901], Loss: 0.4120\n",
      "Epoch [28/50], Step [850/901], Loss: 0.4626\n",
      "Epoch [28/50], Step [860/901], Loss: 0.4544\n",
      "Epoch [28/50], Step [870/901], Loss: 0.4649\n",
      "Epoch [28/50], Step [880/901], Loss: 0.4401\n",
      "Epoch [28/50], Step [890/901], Loss: 0.4901\n",
      "Epoch [28/50], Step [900/901], Loss: 0.4655\n",
      "\n",
      "train loss: 0.4506, train acc: 88.1852\n",
      "validation loss: 0.7073, validation acc: 97.8312\n",
      "\n",
      "Epoch 29\n",
      "\n",
      "Epoch [29/50], Step [0/901], Loss: 0.4401\n",
      "Epoch [29/50], Step [10/901], Loss: 0.4469\n",
      "Epoch [29/50], Step [20/901], Loss: 0.4392\n",
      "Epoch [29/50], Step [30/901], Loss: 0.4358\n",
      "Epoch [29/50], Step [40/901], Loss: 0.4382\n",
      "Epoch [29/50], Step [50/901], Loss: 0.3979\n",
      "Epoch [29/50], Step [60/901], Loss: 0.4550\n",
      "Epoch [29/50], Step [70/901], Loss: 0.4193\n",
      "Epoch [29/50], Step [80/901], Loss: 0.4542\n",
      "Epoch [29/50], Step [90/901], Loss: 0.4577\n",
      "Epoch [29/50], Step [100/901], Loss: 0.4314\n",
      "Epoch [29/50], Step [110/901], Loss: 0.4503\n",
      "Epoch [29/50], Step [120/901], Loss: 0.4392\n",
      "Epoch [29/50], Step [130/901], Loss: 0.4137\n",
      "Epoch [29/50], Step [140/901], Loss: 0.4097\n",
      "Epoch [29/50], Step [150/901], Loss: 0.4098\n",
      "Epoch [29/50], Step [160/901], Loss: 0.4208\n",
      "Epoch [29/50], Step [170/901], Loss: 0.4359\n",
      "Epoch [29/50], Step [180/901], Loss: 0.4425\n",
      "Epoch [29/50], Step [190/901], Loss: 0.4613\n",
      "Epoch [29/50], Step [200/901], Loss: 0.4235\n",
      "Epoch [29/50], Step [210/901], Loss: 0.4538\n",
      "Epoch [29/50], Step [220/901], Loss: 0.4302\n",
      "Epoch [29/50], Step [230/901], Loss: 0.4239\n",
      "Epoch [29/50], Step [240/901], Loss: 0.4185\n",
      "Epoch [29/50], Step [250/901], Loss: 0.4250\n",
      "Epoch [29/50], Step [260/901], Loss: 0.4273\n",
      "Epoch [29/50], Step [270/901], Loss: 0.4272\n",
      "Epoch [29/50], Step [280/901], Loss: 0.4859\n",
      "Epoch [29/50], Step [290/901], Loss: 0.4758\n",
      "Epoch [29/50], Step [300/901], Loss: 0.4320\n",
      "Epoch [29/50], Step [310/901], Loss: 0.4317\n",
      "Epoch [29/50], Step [320/901], Loss: 0.4603\n",
      "Epoch [29/50], Step [330/901], Loss: 0.4795\n",
      "Epoch [29/50], Step [340/901], Loss: 0.4450\n",
      "Epoch [29/50], Step [350/901], Loss: 0.4338\n",
      "Epoch [29/50], Step [360/901], Loss: 0.4457\n",
      "Epoch [29/50], Step [370/901], Loss: 0.4211\n",
      "Epoch [29/50], Step [380/901], Loss: 0.4301\n",
      "Epoch [29/50], Step [390/901], Loss: 0.4480\n",
      "Epoch [29/50], Step [400/901], Loss: 0.4522\n",
      "Epoch [29/50], Step [410/901], Loss: 0.4305\n",
      "Epoch [29/50], Step [420/901], Loss: 0.4000\n",
      "Epoch [29/50], Step [430/901], Loss: 0.4297\n",
      "Epoch [29/50], Step [440/901], Loss: 0.4546\n",
      "Epoch [29/50], Step [450/901], Loss: 0.4307\n",
      "Epoch [29/50], Step [460/901], Loss: 0.4589\n",
      "Epoch [29/50], Step [470/901], Loss: 0.4360\n",
      "Epoch [29/50], Step [480/901], Loss: 0.4492\n",
      "Epoch [29/50], Step [490/901], Loss: 0.4484\n",
      "Epoch [29/50], Step [500/901], Loss: 0.4210\n",
      "Epoch [29/50], Step [510/901], Loss: 0.4350\n",
      "Epoch [29/50], Step [520/901], Loss: 0.4654\n",
      "Epoch [29/50], Step [530/901], Loss: 0.4737\n",
      "Epoch [29/50], Step [540/901], Loss: 0.4037\n",
      "Epoch [29/50], Step [550/901], Loss: 0.4870\n",
      "Epoch [29/50], Step [560/901], Loss: 0.4494\n",
      "Epoch [29/50], Step [570/901], Loss: 0.4538\n",
      "Epoch [29/50], Step [580/901], Loss: 0.4003\n",
      "Epoch [29/50], Step [590/901], Loss: 0.4165\n",
      "Epoch [29/50], Step [600/901], Loss: 0.4297\n",
      "Epoch [29/50], Step [610/901], Loss: 0.4856\n",
      "Epoch [29/50], Step [620/901], Loss: 0.4706\n",
      "Epoch [29/50], Step [630/901], Loss: 0.4767\n",
      "Epoch [29/50], Step [640/901], Loss: 0.4263\n",
      "Epoch [29/50], Step [650/901], Loss: 0.4005\n",
      "Epoch [29/50], Step [660/901], Loss: 0.4146\n",
      "Epoch [29/50], Step [670/901], Loss: 0.4742\n",
      "Epoch [29/50], Step [680/901], Loss: 0.4266\n",
      "Epoch [29/50], Step [690/901], Loss: 0.4115\n",
      "Epoch [29/50], Step [700/901], Loss: 0.4697\n",
      "Epoch [29/50], Step [710/901], Loss: 0.4816\n",
      "Epoch [29/50], Step [720/901], Loss: 0.4407\n",
      "Epoch [29/50], Step [730/901], Loss: 0.4728\n",
      "Epoch [29/50], Step [740/901], Loss: 0.4736\n",
      "Epoch [29/50], Step [750/901], Loss: 0.4585\n",
      "Epoch [29/50], Step [760/901], Loss: 0.4396\n",
      "Epoch [29/50], Step [770/901], Loss: 0.4298\n",
      "Epoch [29/50], Step [780/901], Loss: 0.3845\n",
      "Epoch [29/50], Step [790/901], Loss: 0.4291\n",
      "Epoch [29/50], Step [800/901], Loss: 0.4201\n",
      "Epoch [29/50], Step [810/901], Loss: 0.4149\n",
      "Epoch [29/50], Step [820/901], Loss: 0.3968\n",
      "Epoch [29/50], Step [830/901], Loss: 0.4275\n",
      "Epoch [29/50], Step [840/901], Loss: 0.4311\n",
      "Epoch [29/50], Step [850/901], Loss: 0.4670\n",
      "Epoch [29/50], Step [860/901], Loss: 0.4238\n",
      "Epoch [29/50], Step [870/901], Loss: 0.4732\n",
      "Epoch [29/50], Step [880/901], Loss: 0.3801\n",
      "Epoch [29/50], Step [890/901], Loss: 0.4224\n",
      "Epoch [29/50], Step [900/901], Loss: 0.4085\n",
      "\n",
      "train loss: 0.4503, train acc: 88.2329\n",
      "validation loss: 0.7069, validation acc: 97.5709\n",
      "\n",
      "Epoch 30\n",
      "\n",
      "Epoch [30/50], Step [0/901], Loss: 0.4189\n",
      "Epoch [30/50], Step [10/901], Loss: 0.4739\n",
      "Epoch [30/50], Step [20/901], Loss: 0.4025\n",
      "Epoch [30/50], Step [30/901], Loss: 0.4555\n",
      "Epoch [30/50], Step [40/901], Loss: 0.4486\n",
      "Epoch [30/50], Step [50/901], Loss: 0.4235\n",
      "Epoch [30/50], Step [60/901], Loss: 0.4636\n",
      "Epoch [30/50], Step [70/901], Loss: 0.4148\n",
      "Epoch [30/50], Step [80/901], Loss: 0.4691\n",
      "Epoch [30/50], Step [90/901], Loss: 0.4930\n",
      "Epoch [30/50], Step [100/901], Loss: 0.4716\n",
      "Epoch [30/50], Step [110/901], Loss: 0.4317\n",
      "Epoch [30/50], Step [120/901], Loss: 0.4221\n",
      "Epoch [30/50], Step [130/901], Loss: 0.4033\n",
      "Epoch [30/50], Step [140/901], Loss: 0.4193\n",
      "Epoch [30/50], Step [150/901], Loss: 0.4533\n",
      "Epoch [30/50], Step [160/901], Loss: 0.4764\n",
      "Epoch [30/50], Step [170/901], Loss: 0.3926\n",
      "Epoch [30/50], Step [180/901], Loss: 0.4461\n",
      "Epoch [30/50], Step [190/901], Loss: 0.4377\n",
      "Epoch [30/50], Step [200/901], Loss: 0.4380\n",
      "Epoch [30/50], Step [210/901], Loss: 0.4829\n",
      "Epoch [30/50], Step [220/901], Loss: 0.4274\n",
      "Epoch [30/50], Step [230/901], Loss: 0.4428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50], Step [240/901], Loss: 0.4526\n",
      "Epoch [30/50], Step [250/901], Loss: 0.4919\n",
      "Epoch [30/50], Step [260/901], Loss: 0.4383\n",
      "Epoch [30/50], Step [270/901], Loss: 0.4843\n",
      "Epoch [30/50], Step [280/901], Loss: 0.4273\n",
      "Epoch [30/50], Step [290/901], Loss: 0.4268\n",
      "Epoch [30/50], Step [300/901], Loss: 0.4407\n",
      "Epoch [30/50], Step [310/901], Loss: 0.4471\n",
      "Epoch [30/50], Step [320/901], Loss: 0.4424\n",
      "Epoch [30/50], Step [330/901], Loss: 0.4434\n",
      "Epoch [30/50], Step [340/901], Loss: 0.4682\n",
      "Epoch [30/50], Step [350/901], Loss: 0.4473\n",
      "Epoch [30/50], Step [360/901], Loss: 0.4412\n",
      "Epoch [30/50], Step [370/901], Loss: 0.4558\n",
      "Epoch [30/50], Step [380/901], Loss: 0.4013\n",
      "Epoch [30/50], Step [390/901], Loss: 0.4341\n",
      "Epoch [30/50], Step [400/901], Loss: 0.4057\n",
      "Epoch [30/50], Step [410/901], Loss: 0.4599\n",
      "Epoch [30/50], Step [420/901], Loss: 0.4224\n",
      "Epoch [30/50], Step [430/901], Loss: 0.4409\n",
      "Epoch [30/50], Step [440/901], Loss: 0.4652\n",
      "Epoch [30/50], Step [450/901], Loss: 0.4333\n",
      "Epoch [30/50], Step [460/901], Loss: 0.4211\n",
      "Epoch [30/50], Step [470/901], Loss: 0.4366\n",
      "Epoch [30/50], Step [480/901], Loss: 0.4584\n",
      "Epoch [30/50], Step [490/901], Loss: 0.4150\n",
      "Epoch [30/50], Step [500/901], Loss: 0.4069\n",
      "Epoch [30/50], Step [510/901], Loss: 0.4547\n",
      "Epoch [30/50], Step [520/901], Loss: 0.4301\n",
      "Epoch [30/50], Step [530/901], Loss: 0.4231\n",
      "Epoch [30/50], Step [540/901], Loss: 0.4528\n",
      "Epoch [30/50], Step [550/901], Loss: 0.4261\n",
      "Epoch [30/50], Step [560/901], Loss: 0.4292\n",
      "Epoch [30/50], Step [570/901], Loss: 0.4342\n",
      "Epoch [30/50], Step [580/901], Loss: 0.4519\n",
      "Epoch [30/50], Step [590/901], Loss: 0.4264\n",
      "Epoch [30/50], Step [600/901], Loss: 0.4460\n",
      "Epoch [30/50], Step [610/901], Loss: 0.4380\n",
      "Epoch [30/50], Step [620/901], Loss: 0.4220\n",
      "Epoch [30/50], Step [630/901], Loss: 0.4539\n",
      "Epoch [30/50], Step [640/901], Loss: 0.4247\n",
      "Epoch [30/50], Step [650/901], Loss: 0.4681\n",
      "Epoch [30/50], Step [660/901], Loss: 0.4005\n",
      "Epoch [30/50], Step [670/901], Loss: 0.4027\n",
      "Epoch [30/50], Step [680/901], Loss: 0.4510\n",
      "Epoch [30/50], Step [690/901], Loss: 0.4672\n",
      "Epoch [30/50], Step [700/901], Loss: 0.4426\n",
      "Epoch [30/50], Step [710/901], Loss: 0.4358\n",
      "Epoch [30/50], Step [720/901], Loss: 0.4387\n",
      "Epoch [30/50], Step [730/901], Loss: 0.4170\n",
      "Epoch [30/50], Step [740/901], Loss: 0.4549\n",
      "Epoch [30/50], Step [750/901], Loss: 0.4536\n",
      "Epoch [30/50], Step [760/901], Loss: 0.4017\n",
      "Epoch [30/50], Step [770/901], Loss: 0.4251\n",
      "Epoch [30/50], Step [780/901], Loss: 0.4464\n",
      "Epoch [30/50], Step [790/901], Loss: 0.4373\n",
      "Epoch [30/50], Step [800/901], Loss: 0.4257\n",
      "Epoch [30/50], Step [810/901], Loss: 0.4628\n",
      "Epoch [30/50], Step [820/901], Loss: 0.4075\n",
      "Epoch [30/50], Step [830/901], Loss: 0.4290\n",
      "Epoch [30/50], Step [840/901], Loss: 0.4184\n",
      "Epoch [30/50], Step [850/901], Loss: 0.4332\n",
      "Epoch [30/50], Step [860/901], Loss: 0.4578\n",
      "Epoch [30/50], Step [870/901], Loss: 0.4038\n",
      "Epoch [30/50], Step [880/901], Loss: 0.4773\n",
      "Epoch [30/50], Step [890/901], Loss: 0.4731\n",
      "Epoch [30/50], Step [900/901], Loss: 0.4478\n",
      "\n",
      "train loss: 0.4500, train acc: 88.1934\n",
      "validation loss: 0.7065, validation acc: 97.6243\n",
      "\n",
      "Epoch 31\n",
      "\n",
      "Epoch [31/50], Step [0/901], Loss: 0.4698\n",
      "Epoch [31/50], Step [10/901], Loss: 0.4876\n",
      "Epoch [31/50], Step [20/901], Loss: 0.4694\n",
      "Epoch [31/50], Step [30/901], Loss: 0.4378\n",
      "Epoch [31/50], Step [40/901], Loss: 0.4552\n",
      "Epoch [31/50], Step [50/901], Loss: 0.4614\n",
      "Epoch [31/50], Step [60/901], Loss: 0.4970\n",
      "Epoch [31/50], Step [70/901], Loss: 0.4349\n",
      "Epoch [31/50], Step [80/901], Loss: 0.4414\n",
      "Epoch [31/50], Step [90/901], Loss: 0.4742\n",
      "Epoch [31/50], Step [100/901], Loss: 0.4208\n",
      "Epoch [31/50], Step [110/901], Loss: 0.4339\n",
      "Epoch [31/50], Step [120/901], Loss: 0.4078\n",
      "Epoch [31/50], Step [130/901], Loss: 0.4048\n",
      "Epoch [31/50], Step [140/901], Loss: 0.4873\n",
      "Epoch [31/50], Step [150/901], Loss: 0.4387\n",
      "Epoch [31/50], Step [160/901], Loss: 0.4261\n",
      "Epoch [31/50], Step [170/901], Loss: 0.4495\n",
      "Epoch [31/50], Step [180/901], Loss: 0.4241\n",
      "Epoch [31/50], Step [190/901], Loss: 0.4353\n",
      "Epoch [31/50], Step [200/901], Loss: 0.4899\n",
      "Epoch [31/50], Step [210/901], Loss: 0.4602\n",
      "Epoch [31/50], Step [220/901], Loss: 0.4043\n",
      "Epoch [31/50], Step [230/901], Loss: 0.4644\n",
      "Epoch [31/50], Step [240/901], Loss: 0.4280\n",
      "Epoch [31/50], Step [250/901], Loss: 0.4706\n",
      "Epoch [31/50], Step [260/901], Loss: 0.4539\n",
      "Epoch [31/50], Step [270/901], Loss: 0.4413\n",
      "Epoch [31/50], Step [280/901], Loss: 0.4280\n",
      "Epoch [31/50], Step [290/901], Loss: 0.4977\n",
      "Epoch [31/50], Step [300/901], Loss: 0.4028\n",
      "Epoch [31/50], Step [310/901], Loss: 0.4485\n",
      "Epoch [31/50], Step [320/901], Loss: 0.4317\n",
      "Epoch [31/50], Step [330/901], Loss: 0.4726\n",
      "Epoch [31/50], Step [340/901], Loss: 0.4425\n",
      "Epoch [31/50], Step [350/901], Loss: 0.4501\n",
      "Epoch [31/50], Step [360/901], Loss: 0.4668\n",
      "Epoch [31/50], Step [370/901], Loss: 0.4556\n",
      "Epoch [31/50], Step [380/901], Loss: 0.4605\n",
      "Epoch [31/50], Step [390/901], Loss: 0.4502\n",
      "Epoch [31/50], Step [400/901], Loss: 0.4570\n",
      "Epoch [31/50], Step [410/901], Loss: 0.4444\n",
      "Epoch [31/50], Step [420/901], Loss: 0.4144\n",
      "Epoch [31/50], Step [430/901], Loss: 0.4623\n",
      "Epoch [31/50], Step [440/901], Loss: 0.4138\n",
      "Epoch [31/50], Step [450/901], Loss: 0.4071\n",
      "Epoch [31/50], Step [460/901], Loss: 0.4029\n",
      "Epoch [31/50], Step [470/901], Loss: 0.4268\n",
      "Epoch [31/50], Step [480/901], Loss: 0.4301\n",
      "Epoch [31/50], Step [490/901], Loss: 0.4623\n",
      "Epoch [31/50], Step [500/901], Loss: 0.4448\n",
      "Epoch [31/50], Step [510/901], Loss: 0.4266\n",
      "Epoch [31/50], Step [520/901], Loss: 0.4523\n",
      "Epoch [31/50], Step [530/901], Loss: 0.3973\n",
      "Epoch [31/50], Step [540/901], Loss: 0.3898\n",
      "Epoch [31/50], Step [550/901], Loss: 0.4727\n",
      "Epoch [31/50], Step [560/901], Loss: 0.4339\n",
      "Epoch [31/50], Step [570/901], Loss: 0.4646\n",
      "Epoch [31/50], Step [580/901], Loss: 0.4005\n",
      "Epoch [31/50], Step [590/901], Loss: 0.4248\n",
      "Epoch [31/50], Step [600/901], Loss: 0.4483\n",
      "Epoch [31/50], Step [610/901], Loss: 0.4308\n",
      "Epoch [31/50], Step [620/901], Loss: 0.4621\n",
      "Epoch [31/50], Step [630/901], Loss: 0.4276\n",
      "Epoch [31/50], Step [640/901], Loss: 0.4307\n",
      "Epoch [31/50], Step [650/901], Loss: 0.4328\n",
      "Epoch [31/50], Step [660/901], Loss: 0.4616\n",
      "Epoch [31/50], Step [670/901], Loss: 0.4810\n",
      "Epoch [31/50], Step [680/901], Loss: 0.4802\n",
      "Epoch [31/50], Step [690/901], Loss: 0.4437\n",
      "Epoch [31/50], Step [700/901], Loss: 0.4233\n",
      "Epoch [31/50], Step [710/901], Loss: 0.4880\n",
      "Epoch [31/50], Step [720/901], Loss: 0.4203\n",
      "Epoch [31/50], Step [730/901], Loss: 0.4726\n",
      "Epoch [31/50], Step [740/901], Loss: 0.4524\n",
      "Epoch [31/50], Step [750/901], Loss: 0.4304\n",
      "Epoch [31/50], Step [760/901], Loss: 0.4078\n",
      "Epoch [31/50], Step [770/901], Loss: 0.4524\n",
      "Epoch [31/50], Step [780/901], Loss: 0.4129\n",
      "Epoch [31/50], Step [790/901], Loss: 0.4417\n",
      "Epoch [31/50], Step [800/901], Loss: 0.4552\n",
      "Epoch [31/50], Step [810/901], Loss: 0.4753\n",
      "Epoch [31/50], Step [820/901], Loss: 0.4348\n",
      "Epoch [31/50], Step [830/901], Loss: 0.4589\n",
      "Epoch [31/50], Step [840/901], Loss: 0.4288\n",
      "Epoch [31/50], Step [850/901], Loss: 0.4638\n",
      "Epoch [31/50], Step [860/901], Loss: 0.4104\n",
      "Epoch [31/50], Step [870/901], Loss: 0.4845\n",
      "Epoch [31/50], Step [880/901], Loss: 0.4092\n",
      "Epoch [31/50], Step [890/901], Loss: 0.4310\n",
      "Epoch [31/50], Step [900/901], Loss: 0.4211\n",
      "\n",
      "train loss: 0.4497, train acc: 88.3573\n",
      "validation loss: 0.7061, validation acc: 97.9112\n",
      "\n",
      "Epoch 32\n",
      "\n",
      "Epoch [32/50], Step [0/901], Loss: 0.4198\n",
      "Epoch [32/50], Step [10/901], Loss: 0.4155\n",
      "Epoch [32/50], Step [20/901], Loss: 0.4055\n",
      "Epoch [32/50], Step [30/901], Loss: 0.3982\n",
      "Epoch [32/50], Step [40/901], Loss: 0.4310\n",
      "Epoch [32/50], Step [50/901], Loss: 0.4384\n",
      "Epoch [32/50], Step [60/901], Loss: 0.4395\n",
      "Epoch [32/50], Step [70/901], Loss: 0.4150\n",
      "Epoch [32/50], Step [80/901], Loss: 0.4160\n",
      "Epoch [32/50], Step [90/901], Loss: 0.4638\n",
      "Epoch [32/50], Step [100/901], Loss: 0.4558\n",
      "Epoch [32/50], Step [110/901], Loss: 0.4242\n",
      "Epoch [32/50], Step [120/901], Loss: 0.4512\n",
      "Epoch [32/50], Step [130/901], Loss: 0.4460\n",
      "Epoch [32/50], Step [140/901], Loss: 0.4283\n",
      "Epoch [32/50], Step [150/901], Loss: 0.4672\n",
      "Epoch [32/50], Step [160/901], Loss: 0.5128\n",
      "Epoch [32/50], Step [170/901], Loss: 0.4194\n",
      "Epoch [32/50], Step [180/901], Loss: 0.4065\n",
      "Epoch [32/50], Step [190/901], Loss: 0.4566\n",
      "Epoch [32/50], Step [200/901], Loss: 0.4244\n",
      "Epoch [32/50], Step [210/901], Loss: 0.4256\n",
      "Epoch [32/50], Step [220/901], Loss: 0.4114\n",
      "Epoch [32/50], Step [230/901], Loss: 0.5088\n",
      "Epoch [32/50], Step [240/901], Loss: 0.4713\n",
      "Epoch [32/50], Step [250/901], Loss: 0.4692\n",
      "Epoch [32/50], Step [260/901], Loss: 0.4578\n",
      "Epoch [32/50], Step [270/901], Loss: 0.4306\n",
      "Epoch [32/50], Step [280/901], Loss: 0.4456\n",
      "Epoch [32/50], Step [290/901], Loss: 0.4417\n",
      "Epoch [32/50], Step [300/901], Loss: 0.4432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/50], Step [310/901], Loss: 0.4384\n",
      "Epoch [32/50], Step [320/901], Loss: 0.4746\n",
      "Epoch [32/50], Step [330/901], Loss: 0.4581\n",
      "Epoch [32/50], Step [340/901], Loss: 0.4395\n",
      "Epoch [32/50], Step [350/901], Loss: 0.4462\n",
      "Epoch [32/50], Step [360/901], Loss: 0.4158\n",
      "Epoch [32/50], Step [370/901], Loss: 0.4146\n",
      "Epoch [32/50], Step [380/901], Loss: 0.4178\n",
      "Epoch [32/50], Step [390/901], Loss: 0.4544\n",
      "Epoch [32/50], Step [400/901], Loss: 0.4437\n",
      "Epoch [32/50], Step [410/901], Loss: 0.4793\n",
      "Epoch [32/50], Step [420/901], Loss: 0.4588\n",
      "Epoch [32/50], Step [430/901], Loss: 0.4490\n",
      "Epoch [32/50], Step [440/901], Loss: 0.5046\n",
      "Epoch [32/50], Step [450/901], Loss: 0.4440\n",
      "Epoch [32/50], Step [460/901], Loss: 0.4235\n",
      "Epoch [32/50], Step [470/901], Loss: 0.4768\n",
      "Epoch [32/50], Step [480/901], Loss: 0.4501\n",
      "Epoch [32/50], Step [490/901], Loss: 0.4105\n",
      "Epoch [32/50], Step [500/901], Loss: 0.4738\n",
      "Epoch [32/50], Step [510/901], Loss: 0.4408\n",
      "Epoch [32/50], Step [520/901], Loss: 0.4348\n",
      "Epoch [32/50], Step [530/901], Loss: 0.4745\n",
      "Epoch [32/50], Step [540/901], Loss: 0.4317\n",
      "Epoch [32/50], Step [550/901], Loss: 0.4512\n",
      "Epoch [32/50], Step [560/901], Loss: 0.4301\n",
      "Epoch [32/50], Step [570/901], Loss: 0.4224\n",
      "Epoch [32/50], Step [580/901], Loss: 0.4073\n",
      "Epoch [32/50], Step [590/901], Loss: 0.4552\n",
      "Epoch [32/50], Step [600/901], Loss: 0.4361\n",
      "Epoch [32/50], Step [610/901], Loss: 0.4595\n",
      "Epoch [32/50], Step [620/901], Loss: 0.4577\n",
      "Epoch [32/50], Step [630/901], Loss: 0.4747\n",
      "Epoch [32/50], Step [640/901], Loss: 0.4377\n",
      "Epoch [32/50], Step [650/901], Loss: 0.4648\n",
      "Epoch [32/50], Step [660/901], Loss: 0.4012\n",
      "Epoch [32/50], Step [670/901], Loss: 0.4535\n",
      "Epoch [32/50], Step [680/901], Loss: 0.4153\n",
      "Epoch [32/50], Step [690/901], Loss: 0.4730\n",
      "Epoch [32/50], Step [700/901], Loss: 0.4291\n",
      "Epoch [32/50], Step [710/901], Loss: 0.4782\n",
      "Epoch [32/50], Step [720/901], Loss: 0.4193\n",
      "Epoch [32/50], Step [730/901], Loss: 0.4737\n",
      "Epoch [32/50], Step [740/901], Loss: 0.4493\n",
      "Epoch [32/50], Step [750/901], Loss: 0.4256\n",
      "Epoch [32/50], Step [760/901], Loss: 0.4392\n",
      "Epoch [32/50], Step [770/901], Loss: 0.4314\n",
      "Epoch [32/50], Step [780/901], Loss: 0.4291\n",
      "Epoch [32/50], Step [790/901], Loss: 0.3793\n",
      "Epoch [32/50], Step [800/901], Loss: 0.4417\n",
      "Epoch [32/50], Step [810/901], Loss: 0.4424\n",
      "Epoch [32/50], Step [820/901], Loss: 0.4767\n",
      "Epoch [32/50], Step [830/901], Loss: 0.4567\n",
      "Epoch [32/50], Step [840/901], Loss: 0.4457\n",
      "Epoch [32/50], Step [850/901], Loss: 0.4412\n",
      "Epoch [32/50], Step [860/901], Loss: 0.4448\n",
      "Epoch [32/50], Step [870/901], Loss: 0.4140\n",
      "Epoch [32/50], Step [880/901], Loss: 0.4000\n",
      "Epoch [32/50], Step [890/901], Loss: 0.4254\n",
      "Epoch [32/50], Step [900/901], Loss: 0.4144\n",
      "\n",
      "train loss: 0.4494, train acc: 88.3994\n",
      "validation loss: 0.7058, validation acc: 97.8579\n",
      "\n",
      "Epoch 33\n",
      "\n",
      "Epoch [33/50], Step [0/901], Loss: 0.4122\n",
      "Epoch [33/50], Step [10/901], Loss: 0.4138\n",
      "Epoch [33/50], Step [20/901], Loss: 0.3830\n",
      "Epoch [33/50], Step [30/901], Loss: 0.4481\n",
      "Epoch [33/50], Step [40/901], Loss: 0.4275\n",
      "Epoch [33/50], Step [50/901], Loss: 0.4156\n",
      "Epoch [33/50], Step [60/901], Loss: 0.4433\n",
      "Epoch [33/50], Step [70/901], Loss: 0.4199\n",
      "Epoch [33/50], Step [80/901], Loss: 0.4352\n",
      "Epoch [33/50], Step [90/901], Loss: 0.4325\n",
      "Epoch [33/50], Step [100/901], Loss: 0.4513\n",
      "Epoch [33/50], Step [110/901], Loss: 0.4473\n",
      "Epoch [33/50], Step [120/901], Loss: 0.4190\n",
      "Epoch [33/50], Step [130/901], Loss: 0.4561\n",
      "Epoch [33/50], Step [140/901], Loss: 0.4033\n",
      "Epoch [33/50], Step [150/901], Loss: 0.4461\n",
      "Epoch [33/50], Step [160/901], Loss: 0.4252\n",
      "Epoch [33/50], Step [170/901], Loss: 0.4535\n",
      "Epoch [33/50], Step [180/901], Loss: 0.4384\n",
      "Epoch [33/50], Step [190/901], Loss: 0.4585\n",
      "Epoch [33/50], Step [200/901], Loss: 0.4367\n",
      "Epoch [33/50], Step [210/901], Loss: 0.4282\n",
      "Epoch [33/50], Step [220/901], Loss: 0.4534\n",
      "Epoch [33/50], Step [230/901], Loss: 0.4641\n",
      "Epoch [33/50], Step [240/901], Loss: 0.4337\n",
      "Epoch [33/50], Step [250/901], Loss: 0.4281\n",
      "Epoch [33/50], Step [260/901], Loss: 0.4526\n",
      "Epoch [33/50], Step [270/901], Loss: 0.4184\n",
      "Epoch [33/50], Step [280/901], Loss: 0.4288\n",
      "Epoch [33/50], Step [290/901], Loss: 0.4340\n",
      "Epoch [33/50], Step [300/901], Loss: 0.4664\n",
      "Epoch [33/50], Step [310/901], Loss: 0.4458\n",
      "Epoch [33/50], Step [320/901], Loss: 0.4003\n",
      "Epoch [33/50], Step [330/901], Loss: 0.4744\n",
      "Epoch [33/50], Step [340/901], Loss: 0.4315\n",
      "Epoch [33/50], Step [350/901], Loss: 0.4360\n",
      "Epoch [33/50], Step [360/901], Loss: 0.4381\n",
      "Epoch [33/50], Step [370/901], Loss: 0.4198\n",
      "Epoch [33/50], Step [380/901], Loss: 0.4260\n",
      "Epoch [33/50], Step [390/901], Loss: 0.4109\n",
      "Epoch [33/50], Step [400/901], Loss: 0.4457\n",
      "Epoch [33/50], Step [410/901], Loss: 0.4565\n",
      "Epoch [33/50], Step [420/901], Loss: 0.4127\n",
      "Epoch [33/50], Step [430/901], Loss: 0.4516\n",
      "Epoch [33/50], Step [440/901], Loss: 0.4785\n",
      "Epoch [33/50], Step [450/901], Loss: 0.4449\n",
      "Epoch [33/50], Step [460/901], Loss: 0.4377\n",
      "Epoch [33/50], Step [470/901], Loss: 0.4425\n",
      "Epoch [33/50], Step [480/901], Loss: 0.4387\n",
      "Epoch [33/50], Step [490/901], Loss: 0.4265\n",
      "Epoch [33/50], Step [500/901], Loss: 0.4468\n",
      "Epoch [33/50], Step [510/901], Loss: 0.3869\n",
      "Epoch [33/50], Step [520/901], Loss: 0.4612\n",
      "Epoch [33/50], Step [530/901], Loss: 0.3822\n",
      "Epoch [33/50], Step [540/901], Loss: 0.4204\n",
      "Epoch [33/50], Step [550/901], Loss: 0.4518\n",
      "Epoch [33/50], Step [560/901], Loss: 0.4909\n",
      "Epoch [33/50], Step [570/901], Loss: 0.4566\n",
      "Epoch [33/50], Step [580/901], Loss: 0.4734\n",
      "Epoch [33/50], Step [590/901], Loss: 0.4895\n",
      "Epoch [33/50], Step [600/901], Loss: 0.4126\n",
      "Epoch [33/50], Step [610/901], Loss: 0.4464\n",
      "Epoch [33/50], Step [620/901], Loss: 0.4349\n",
      "Epoch [33/50], Step [630/901], Loss: 0.4694\n",
      "Epoch [33/50], Step [640/901], Loss: 0.4636\n",
      "Epoch [33/50], Step [650/901], Loss: 0.4473\n",
      "Epoch [33/50], Step [660/901], Loss: 0.4464\n",
      "Epoch [33/50], Step [670/901], Loss: 0.4440\n",
      "Epoch [33/50], Step [680/901], Loss: 0.4345\n",
      "Epoch [33/50], Step [690/901], Loss: 0.4316\n",
      "Epoch [33/50], Step [700/901], Loss: 0.5001\n",
      "Epoch [33/50], Step [710/901], Loss: 0.4430\n",
      "Epoch [33/50], Step [720/901], Loss: 0.4466\n",
      "Epoch [33/50], Step [730/901], Loss: 0.4572\n",
      "Epoch [33/50], Step [740/901], Loss: 0.4704\n",
      "Epoch [33/50], Step [750/901], Loss: 0.4213\n",
      "Epoch [33/50], Step [760/901], Loss: 0.4101\n",
      "Epoch [33/50], Step [770/901], Loss: 0.4316\n",
      "Epoch [33/50], Step [780/901], Loss: 0.4074\n",
      "Epoch [33/50], Step [790/901], Loss: 0.4273\n",
      "Epoch [33/50], Step [800/901], Loss: 0.4478\n",
      "Epoch [33/50], Step [810/901], Loss: 0.4150\n",
      "Epoch [33/50], Step [820/901], Loss: 0.4254\n",
      "Epoch [33/50], Step [830/901], Loss: 0.4582\n",
      "Epoch [33/50], Step [840/901], Loss: 0.4258\n",
      "Epoch [33/50], Step [850/901], Loss: 0.4554\n",
      "Epoch [33/50], Step [860/901], Loss: 0.4430\n",
      "Epoch [33/50], Step [870/901], Loss: 0.4555\n",
      "Epoch [33/50], Step [880/901], Loss: 0.4596\n",
      "Epoch [33/50], Step [890/901], Loss: 0.4163\n",
      "Epoch [33/50], Step [900/901], Loss: 0.4347\n",
      "\n",
      "train loss: 0.4491, train acc: 88.4809\n",
      "validation loss: 0.7057, validation acc: 97.0838\n",
      "\n",
      "Epoch 34\n",
      "\n",
      "Epoch [34/50], Step [0/901], Loss: 0.4298\n",
      "Epoch [34/50], Step [10/901], Loss: 0.4105\n",
      "Epoch [34/50], Step [20/901], Loss: 0.4025\n",
      "Epoch [34/50], Step [30/901], Loss: 0.3959\n",
      "Epoch [34/50], Step [40/901], Loss: 0.4333\n",
      "Epoch [34/50], Step [50/901], Loss: 0.4776\n",
      "Epoch [34/50], Step [60/901], Loss: 0.4067\n",
      "Epoch [34/50], Step [70/901], Loss: 0.4307\n",
      "Epoch [34/50], Step [80/901], Loss: 0.4307\n",
      "Epoch [34/50], Step [90/901], Loss: 0.4051\n",
      "Epoch [34/50], Step [100/901], Loss: 0.4524\n",
      "Epoch [34/50], Step [110/901], Loss: 0.4301\n",
      "Epoch [34/50], Step [120/901], Loss: 0.4147\n",
      "Epoch [34/50], Step [130/901], Loss: 0.4227\n",
      "Epoch [34/50], Step [140/901], Loss: 0.4765\n",
      "Epoch [34/50], Step [150/901], Loss: 0.4428\n",
      "Epoch [34/50], Step [160/901], Loss: 0.4569\n",
      "Epoch [34/50], Step [170/901], Loss: 0.4218\n",
      "Epoch [34/50], Step [180/901], Loss: 0.4365\n",
      "Epoch [34/50], Step [190/901], Loss: 0.4334\n",
      "Epoch [34/50], Step [200/901], Loss: 0.4226\n",
      "Epoch [34/50], Step [210/901], Loss: 0.5078\n",
      "Epoch [34/50], Step [220/901], Loss: 0.4273\n",
      "Epoch [34/50], Step [230/901], Loss: 0.4479\n",
      "Epoch [34/50], Step [240/901], Loss: 0.4228\n",
      "Epoch [34/50], Step [250/901], Loss: 0.4333\n",
      "Epoch [34/50], Step [260/901], Loss: 0.4410\n",
      "Epoch [34/50], Step [270/901], Loss: 0.4813\n",
      "Epoch [34/50], Step [280/901], Loss: 0.4159\n",
      "Epoch [34/50], Step [290/901], Loss: 0.4495\n",
      "Epoch [34/50], Step [300/901], Loss: 0.4435\n",
      "Epoch [34/50], Step [310/901], Loss: 0.4164\n",
      "Epoch [34/50], Step [320/901], Loss: 0.4317\n",
      "Epoch [34/50], Step [330/901], Loss: 0.4150\n",
      "Epoch [34/50], Step [340/901], Loss: 0.4503\n",
      "Epoch [34/50], Step [350/901], Loss: 0.4494\n",
      "Epoch [34/50], Step [360/901], Loss: 0.4247\n",
      "Epoch [34/50], Step [370/901], Loss: 0.4247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/50], Step [380/901], Loss: 0.4264\n",
      "Epoch [34/50], Step [390/901], Loss: 0.4274\n",
      "Epoch [34/50], Step [400/901], Loss: 0.4078\n",
      "Epoch [34/50], Step [410/901], Loss: 0.4415\n",
      "Epoch [34/50], Step [420/901], Loss: 0.3879\n",
      "Epoch [34/50], Step [430/901], Loss: 0.4854\n",
      "Epoch [34/50], Step [440/901], Loss: 0.4400\n",
      "Epoch [34/50], Step [450/901], Loss: 0.4362\n",
      "Epoch [34/50], Step [460/901], Loss: 0.4535\n",
      "Epoch [34/50], Step [470/901], Loss: 0.4551\n",
      "Epoch [34/50], Step [480/901], Loss: 0.4544\n",
      "Epoch [34/50], Step [490/901], Loss: 0.4129\n",
      "Epoch [34/50], Step [500/901], Loss: 0.4539\n",
      "Epoch [34/50], Step [510/901], Loss: 0.4313\n",
      "Epoch [34/50], Step [520/901], Loss: 0.4346\n",
      "Epoch [34/50], Step [530/901], Loss: 0.4355\n",
      "Epoch [34/50], Step [540/901], Loss: 0.4495\n",
      "Epoch [34/50], Step [550/901], Loss: 0.4399\n",
      "Epoch [34/50], Step [560/901], Loss: 0.4152\n",
      "Epoch [34/50], Step [570/901], Loss: 0.4579\n",
      "Epoch [34/50], Step [580/901], Loss: 0.3994\n",
      "Epoch [34/50], Step [590/901], Loss: 0.4051\n",
      "Epoch [34/50], Step [600/901], Loss: 0.4225\n",
      "Epoch [34/50], Step [610/901], Loss: 0.4046\n",
      "Epoch [34/50], Step [620/901], Loss: 0.4415\n",
      "Epoch [34/50], Step [630/901], Loss: 0.4311\n",
      "Epoch [34/50], Step [640/901], Loss: 0.4410\n",
      "Epoch [34/50], Step [650/901], Loss: 0.4374\n",
      "Epoch [34/50], Step [660/901], Loss: 0.4501\n",
      "Epoch [34/50], Step [670/901], Loss: 0.4604\n",
      "Epoch [34/50], Step [680/901], Loss: 0.4593\n",
      "Epoch [34/50], Step [690/901], Loss: 0.4377\n",
      "Epoch [34/50], Step [700/901], Loss: 0.5374\n",
      "Epoch [34/50], Step [710/901], Loss: 0.4361\n",
      "Epoch [34/50], Step [720/901], Loss: 0.4503\n",
      "Epoch [34/50], Step [730/901], Loss: 0.4663\n",
      "Epoch [34/50], Step [740/901], Loss: 0.4277\n",
      "Epoch [34/50], Step [750/901], Loss: 0.5148\n",
      "Epoch [34/50], Step [760/901], Loss: 0.4880\n",
      "Epoch [34/50], Step [770/901], Loss: 0.4487\n",
      "Epoch [34/50], Step [780/901], Loss: 0.4521\n",
      "Epoch [34/50], Step [790/901], Loss: 0.4555\n",
      "Epoch [34/50], Step [800/901], Loss: 0.4196\n",
      "Epoch [34/50], Step [810/901], Loss: 0.4102\n",
      "Epoch [34/50], Step [820/901], Loss: 0.4585\n",
      "Epoch [34/50], Step [830/901], Loss: 0.4828\n",
      "Epoch [34/50], Step [840/901], Loss: 0.4881\n",
      "Epoch [34/50], Step [850/901], Loss: 0.4283\n",
      "Epoch [34/50], Step [860/901], Loss: 0.4648\n",
      "Epoch [34/50], Step [870/901], Loss: 0.4015\n",
      "Epoch [34/50], Step [880/901], Loss: 0.4319\n",
      "Epoch [34/50], Step [890/901], Loss: 0.4429\n",
      "Epoch [34/50], Step [900/901], Loss: 0.4285\n",
      "\n",
      "train loss: 0.4488, train acc: 88.4740\n",
      "validation loss: 0.7056, validation acc: 97.7377\n",
      "\n",
      "Epoch 35\n",
      "\n",
      "Epoch [35/50], Step [0/901], Loss: 0.4375\n",
      "Epoch [35/50], Step [10/901], Loss: 0.4675\n",
      "Epoch [35/50], Step [20/901], Loss: 0.4570\n",
      "Epoch [35/50], Step [30/901], Loss: 0.4467\n",
      "Epoch [35/50], Step [40/901], Loss: 0.5085\n",
      "Epoch [35/50], Step [50/901], Loss: 0.4263\n",
      "Epoch [35/50], Step [60/901], Loss: 0.4493\n",
      "Epoch [35/50], Step [70/901], Loss: 0.4159\n",
      "Epoch [35/50], Step [80/901], Loss: 0.4320\n",
      "Epoch [35/50], Step [90/901], Loss: 0.4012\n",
      "Epoch [35/50], Step [100/901], Loss: 0.4649\n",
      "Epoch [35/50], Step [110/901], Loss: 0.4024\n",
      "Epoch [35/50], Step [120/901], Loss: 0.4336\n",
      "Epoch [35/50], Step [130/901], Loss: 0.4219\n",
      "Epoch [35/50], Step [140/901], Loss: 0.4478\n",
      "Epoch [35/50], Step [150/901], Loss: 0.4299\n",
      "Epoch [35/50], Step [160/901], Loss: 0.4134\n",
      "Epoch [35/50], Step [170/901], Loss: 0.4263\n",
      "Epoch [35/50], Step [180/901], Loss: 0.4077\n",
      "Epoch [35/50], Step [190/901], Loss: 0.4398\n",
      "Epoch [35/50], Step [200/901], Loss: 0.4343\n",
      "Epoch [35/50], Step [210/901], Loss: 0.4485\n",
      "Epoch [35/50], Step [220/901], Loss: 0.3979\n",
      "Epoch [35/50], Step [230/901], Loss: 0.4765\n",
      "Epoch [35/50], Step [240/901], Loss: 0.4342\n",
      "Epoch [35/50], Step [250/901], Loss: 0.4138\n",
      "Epoch [35/50], Step [260/901], Loss: 0.4235\n",
      "Epoch [35/50], Step [270/901], Loss: 0.4604\n",
      "Epoch [35/50], Step [280/901], Loss: 0.4382\n",
      "Epoch [35/50], Step [290/901], Loss: 0.4328\n",
      "Epoch [35/50], Step [300/901], Loss: 0.4435\n",
      "Epoch [35/50], Step [310/901], Loss: 0.4444\n",
      "Epoch [35/50], Step [320/901], Loss: 0.3924\n",
      "Epoch [35/50], Step [330/901], Loss: 0.4481\n",
      "Epoch [35/50], Step [340/901], Loss: 0.4162\n",
      "Epoch [35/50], Step [350/901], Loss: 0.4410\n",
      "Epoch [35/50], Step [360/901], Loss: 0.4660\n",
      "Epoch [35/50], Step [370/901], Loss: 0.3953\n",
      "Epoch [35/50], Step [380/901], Loss: 0.4314\n",
      "Epoch [35/50], Step [390/901], Loss: 0.4868\n",
      "Epoch [35/50], Step [400/901], Loss: 0.4469\n",
      "Epoch [35/50], Step [410/901], Loss: 0.4572\n",
      "Epoch [35/50], Step [420/901], Loss: 0.4463\n",
      "Epoch [35/50], Step [430/901], Loss: 0.4315\n",
      "Epoch [35/50], Step [440/901], Loss: 0.4376\n",
      "Epoch [35/50], Step [450/901], Loss: 0.4193\n",
      "Epoch [35/50], Step [460/901], Loss: 0.4322\n",
      "Epoch [35/50], Step [470/901], Loss: 0.3795\n",
      "Epoch [35/50], Step [480/901], Loss: 0.4778\n",
      "Epoch [35/50], Step [490/901], Loss: 0.4114\n",
      "Epoch [35/50], Step [500/901], Loss: 0.4307\n",
      "Epoch [35/50], Step [510/901], Loss: 0.4517\n",
      "Epoch [35/50], Step [520/901], Loss: 0.4328\n",
      "Epoch [35/50], Step [530/901], Loss: 0.4116\n",
      "Epoch [35/50], Step [540/901], Loss: 0.4352\n",
      "Epoch [35/50], Step [550/901], Loss: 0.4658\n",
      "Epoch [35/50], Step [560/901], Loss: 0.4608\n",
      "Epoch [35/50], Step [570/901], Loss: 0.4427\n",
      "Epoch [35/50], Step [580/901], Loss: 0.4343\n",
      "Epoch [35/50], Step [590/901], Loss: 0.4161\n",
      "Epoch [35/50], Step [600/901], Loss: 0.4501\n",
      "Epoch [35/50], Step [610/901], Loss: 0.4074\n",
      "Epoch [35/50], Step [620/901], Loss: 0.4768\n",
      "Epoch [35/50], Step [630/901], Loss: 0.4645\n",
      "Epoch [35/50], Step [640/901], Loss: 0.4507\n",
      "Epoch [35/50], Step [650/901], Loss: 0.3994\n",
      "Epoch [35/50], Step [660/901], Loss: 0.4346\n",
      "Epoch [35/50], Step [670/901], Loss: 0.4404\n",
      "Epoch [35/50], Step [680/901], Loss: 0.4698\n",
      "Epoch [35/50], Step [690/901], Loss: 0.4318\n",
      "Epoch [35/50], Step [700/901], Loss: 0.4441\n",
      "Epoch [35/50], Step [710/901], Loss: 0.4207\n",
      "Epoch [35/50], Step [720/901], Loss: 0.4665\n",
      "Epoch [35/50], Step [730/901], Loss: 0.4681\n",
      "Epoch [35/50], Step [740/901], Loss: 0.4541\n",
      "Epoch [35/50], Step [750/901], Loss: 0.4092\n",
      "Epoch [35/50], Step [760/901], Loss: 0.4340\n",
      "Epoch [35/50], Step [770/901], Loss: 0.4330\n",
      "Epoch [35/50], Step [780/901], Loss: 0.4271\n",
      "Epoch [35/50], Step [790/901], Loss: 0.4155\n",
      "Epoch [35/50], Step [800/901], Loss: 0.4506\n",
      "Epoch [35/50], Step [810/901], Loss: 0.4037\n",
      "Epoch [35/50], Step [820/901], Loss: 0.4279\n",
      "Epoch [35/50], Step [830/901], Loss: 0.4226\n",
      "Epoch [35/50], Step [840/901], Loss: 0.4529\n",
      "Epoch [35/50], Step [850/901], Loss: 0.4212\n",
      "Epoch [35/50], Step [860/901], Loss: 0.4255\n",
      "Epoch [35/50], Step [870/901], Loss: 0.4420\n",
      "Epoch [35/50], Step [880/901], Loss: 0.4336\n",
      "Epoch [35/50], Step [890/901], Loss: 0.4259\n",
      "Epoch [35/50], Step [900/901], Loss: 0.3661\n",
      "\n",
      "train loss: 0.4485, train acc: 88.5069\n",
      "validation loss: 0.7053, validation acc: 97.9847\n",
      "\n",
      "Epoch 36\n",
      "\n",
      "Epoch [36/50], Step [0/901], Loss: 0.4211\n",
      "Epoch [36/50], Step [10/901], Loss: 0.4296\n",
      "Epoch [36/50], Step [20/901], Loss: 0.4205\n",
      "Epoch [36/50], Step [30/901], Loss: 0.4067\n",
      "Epoch [36/50], Step [40/901], Loss: 0.4057\n",
      "Epoch [36/50], Step [50/901], Loss: 0.4636\n",
      "Epoch [36/50], Step [60/901], Loss: 0.4012\n",
      "Epoch [36/50], Step [70/901], Loss: 0.4279\n",
      "Epoch [36/50], Step [80/901], Loss: 0.4367\n",
      "Epoch [36/50], Step [90/901], Loss: 0.4364\n",
      "Epoch [36/50], Step [100/901], Loss: 0.4277\n",
      "Epoch [36/50], Step [110/901], Loss: 0.4354\n",
      "Epoch [36/50], Step [120/901], Loss: 0.4247\n",
      "Epoch [36/50], Step [130/901], Loss: 0.4226\n",
      "Epoch [36/50], Step [140/901], Loss: 0.4167\n",
      "Epoch [36/50], Step [150/901], Loss: 0.4453\n",
      "Epoch [36/50], Step [160/901], Loss: 0.4609\n",
      "Epoch [36/50], Step [170/901], Loss: 0.4062\n",
      "Epoch [36/50], Step [180/901], Loss: 0.4525\n",
      "Epoch [36/50], Step [190/901], Loss: 0.4235\n",
      "Epoch [36/50], Step [200/901], Loss: 0.4034\n",
      "Epoch [36/50], Step [210/901], Loss: 0.4475\n",
      "Epoch [36/50], Step [220/901], Loss: 0.4248\n",
      "Epoch [36/50], Step [230/901], Loss: 0.4149\n",
      "Epoch [36/50], Step [240/901], Loss: 0.4659\n",
      "Epoch [36/50], Step [250/901], Loss: 0.4480\n",
      "Epoch [36/50], Step [260/901], Loss: 0.4383\n",
      "Epoch [36/50], Step [270/901], Loss: 0.4247\n",
      "Epoch [36/50], Step [280/901], Loss: 0.4654\n",
      "Epoch [36/50], Step [290/901], Loss: 0.4004\n",
      "Epoch [36/50], Step [300/901], Loss: 0.4672\n",
      "Epoch [36/50], Step [310/901], Loss: 0.4212\n",
      "Epoch [36/50], Step [320/901], Loss: 0.4764\n",
      "Epoch [36/50], Step [330/901], Loss: 0.4307\n",
      "Epoch [36/50], Step [340/901], Loss: 0.4729\n",
      "Epoch [36/50], Step [350/901], Loss: 0.4379\n",
      "Epoch [36/50], Step [360/901], Loss: 0.4461\n",
      "Epoch [36/50], Step [370/901], Loss: 0.4426\n",
      "Epoch [36/50], Step [380/901], Loss: 0.4221\n",
      "Epoch [36/50], Step [390/901], Loss: 0.4596\n",
      "Epoch [36/50], Step [400/901], Loss: 0.4343\n",
      "Epoch [36/50], Step [410/901], Loss: 0.4928\n",
      "Epoch [36/50], Step [420/901], Loss: 0.4362\n",
      "Epoch [36/50], Step [430/901], Loss: 0.4280\n",
      "Epoch [36/50], Step [440/901], Loss: 0.4423\n",
      "Epoch [36/50], Step [450/901], Loss: 0.4620\n",
      "Epoch [36/50], Step [460/901], Loss: 0.4444\n",
      "Epoch [36/50], Step [470/901], Loss: 0.4270\n",
      "Epoch [36/50], Step [480/901], Loss: 0.4237\n",
      "Epoch [36/50], Step [490/901], Loss: 0.4489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/50], Step [500/901], Loss: 0.4331\n",
      "Epoch [36/50], Step [510/901], Loss: 0.4719\n",
      "Epoch [36/50], Step [520/901], Loss: 0.4145\n",
      "Epoch [36/50], Step [530/901], Loss: 0.4101\n",
      "Epoch [36/50], Step [540/901], Loss: 0.4465\n",
      "Epoch [36/50], Step [550/901], Loss: 0.4198\n",
      "Epoch [36/50], Step [560/901], Loss: 0.4522\n",
      "Epoch [36/50], Step [570/901], Loss: 0.4103\n",
      "Epoch [36/50], Step [580/901], Loss: 0.4678\n",
      "Epoch [36/50], Step [590/901], Loss: 0.4040\n",
      "Epoch [36/50], Step [600/901], Loss: 0.4187\n",
      "Epoch [36/50], Step [610/901], Loss: 0.4115\n",
      "Epoch [36/50], Step [620/901], Loss: 0.4374\n",
      "Epoch [36/50], Step [630/901], Loss: 0.4417\n",
      "Epoch [36/50], Step [640/901], Loss: 0.4092\n",
      "Epoch [36/50], Step [650/901], Loss: 0.4523\n",
      "Epoch [36/50], Step [660/901], Loss: 0.4200\n",
      "Epoch [36/50], Step [670/901], Loss: 0.4289\n",
      "Epoch [36/50], Step [680/901], Loss: 0.4183\n",
      "Epoch [36/50], Step [690/901], Loss: 0.4301\n",
      "Epoch [36/50], Step [700/901], Loss: 0.4740\n",
      "Epoch [36/50], Step [710/901], Loss: 0.4439\n",
      "Epoch [36/50], Step [720/901], Loss: 0.3957\n",
      "Epoch [36/50], Step [730/901], Loss: 0.4211\n",
      "Epoch [36/50], Step [740/901], Loss: 0.4329\n",
      "Epoch [36/50], Step [750/901], Loss: 0.3988\n",
      "Epoch [36/50], Step [760/901], Loss: 0.4522\n",
      "Epoch [36/50], Step [770/901], Loss: 0.4330\n",
      "Epoch [36/50], Step [780/901], Loss: 0.4524\n",
      "Epoch [36/50], Step [790/901], Loss: 0.4308\n",
      "Epoch [36/50], Step [800/901], Loss: 0.4089\n",
      "Epoch [36/50], Step [810/901], Loss: 0.4465\n",
      "Epoch [36/50], Step [820/901], Loss: 0.4372\n",
      "Epoch [36/50], Step [830/901], Loss: 0.4671\n",
      "Epoch [36/50], Step [840/901], Loss: 0.5016\n",
      "Epoch [36/50], Step [850/901], Loss: 0.4134\n",
      "Epoch [36/50], Step [860/901], Loss: 0.4002\n",
      "Epoch [36/50], Step [870/901], Loss: 0.4494\n",
      "Epoch [36/50], Step [880/901], Loss: 0.4611\n",
      "Epoch [36/50], Step [890/901], Loss: 0.4260\n",
      "Epoch [36/50], Step [900/901], Loss: 0.4629\n",
      "\n",
      "train loss: 0.4482, train acc: 88.4532\n",
      "validation loss: 0.7051, validation acc: 97.4775\n",
      "\n",
      "Epoch 37\n",
      "\n",
      "Epoch [37/50], Step [0/901], Loss: 0.4393\n",
      "Epoch [37/50], Step [10/901], Loss: 0.4115\n",
      "Epoch [37/50], Step [20/901], Loss: 0.4617\n",
      "Epoch [37/50], Step [30/901], Loss: 0.4360\n",
      "Epoch [37/50], Step [40/901], Loss: 0.4744\n",
      "Epoch [37/50], Step [50/901], Loss: 0.4677\n",
      "Epoch [37/50], Step [60/901], Loss: 0.4293\n",
      "Epoch [37/50], Step [70/901], Loss: 0.4654\n",
      "Epoch [37/50], Step [80/901], Loss: 0.4305\n",
      "Epoch [37/50], Step [90/901], Loss: 0.4380\n",
      "Epoch [37/50], Step [100/901], Loss: 0.4032\n",
      "Epoch [37/50], Step [110/901], Loss: 0.4952\n",
      "Epoch [37/50], Step [120/901], Loss: 0.4626\n",
      "Epoch [37/50], Step [130/901], Loss: 0.4710\n",
      "Epoch [37/50], Step [140/901], Loss: 0.4326\n",
      "Epoch [37/50], Step [150/901], Loss: 0.4384\n",
      "Epoch [37/50], Step [160/901], Loss: 0.4213\n",
      "Epoch [37/50], Step [170/901], Loss: 0.4908\n",
      "Epoch [37/50], Step [180/901], Loss: 0.4176\n",
      "Epoch [37/50], Step [190/901], Loss: 0.4542\n",
      "Epoch [37/50], Step [200/901], Loss: 0.4080\n",
      "Epoch [37/50], Step [210/901], Loss: 0.4621\n",
      "Epoch [37/50], Step [220/901], Loss: 0.4627\n",
      "Epoch [37/50], Step [230/901], Loss: 0.4557\n",
      "Epoch [37/50], Step [240/901], Loss: 0.4322\n",
      "Epoch [37/50], Step [250/901], Loss: 0.5016\n",
      "Epoch [37/50], Step [260/901], Loss: 0.4503\n",
      "Epoch [37/50], Step [270/901], Loss: 0.4010\n",
      "Epoch [37/50], Step [280/901], Loss: 0.4595\n",
      "Epoch [37/50], Step [290/901], Loss: 0.4108\n",
      "Epoch [37/50], Step [300/901], Loss: 0.4770\n",
      "Epoch [37/50], Step [310/901], Loss: 0.4224\n",
      "Epoch [37/50], Step [320/901], Loss: 0.4423\n",
      "Epoch [37/50], Step [330/901], Loss: 0.4437\n",
      "Epoch [37/50], Step [340/901], Loss: 0.4159\n",
      "Epoch [37/50], Step [350/901], Loss: 0.4570\n",
      "Epoch [37/50], Step [360/901], Loss: 0.4403\n",
      "Epoch [37/50], Step [370/901], Loss: 0.4315\n",
      "Epoch [37/50], Step [380/901], Loss: 0.3890\n",
      "Epoch [37/50], Step [390/901], Loss: 0.4410\n",
      "Epoch [37/50], Step [400/901], Loss: 0.4078\n",
      "Epoch [37/50], Step [410/901], Loss: 0.4426\n",
      "Epoch [37/50], Step [420/901], Loss: 0.4469\n",
      "Epoch [37/50], Step [430/901], Loss: 0.4158\n",
      "Epoch [37/50], Step [440/901], Loss: 0.4314\n",
      "Epoch [37/50], Step [450/901], Loss: 0.4347\n",
      "Epoch [37/50], Step [460/901], Loss: 0.4449\n",
      "Epoch [37/50], Step [470/901], Loss: 0.4151\n",
      "Epoch [37/50], Step [480/901], Loss: 0.4443\n",
      "Epoch [37/50], Step [490/901], Loss: 0.4274\n",
      "Epoch [37/50], Step [500/901], Loss: 0.4262\n",
      "Epoch [37/50], Step [510/901], Loss: 0.4353\n",
      "Epoch [37/50], Step [520/901], Loss: 0.4374\n",
      "Epoch [37/50], Step [530/901], Loss: 0.4346\n",
      "Epoch [37/50], Step [540/901], Loss: 0.4643\n",
      "Epoch [37/50], Step [550/901], Loss: 0.3871\n",
      "Epoch [37/50], Step [560/901], Loss: 0.3894\n",
      "Epoch [37/50], Step [570/901], Loss: 0.4215\n",
      "Epoch [37/50], Step [580/901], Loss: 0.4710\n",
      "Epoch [37/50], Step [590/901], Loss: 0.4395\n",
      "Epoch [37/50], Step [600/901], Loss: 0.4093\n",
      "Epoch [37/50], Step [610/901], Loss: 0.4417\n",
      "Epoch [37/50], Step [620/901], Loss: 0.4512\n",
      "Epoch [37/50], Step [630/901], Loss: 0.4027\n",
      "Epoch [37/50], Step [640/901], Loss: 0.4287\n",
      "Epoch [37/50], Step [650/901], Loss: 0.4630\n",
      "Epoch [37/50], Step [660/901], Loss: 0.4199\n",
      "Epoch [37/50], Step [670/901], Loss: 0.4736\n",
      "Epoch [37/50], Step [680/901], Loss: 0.4450\n",
      "Epoch [37/50], Step [690/901], Loss: 0.3999\n",
      "Epoch [37/50], Step [700/901], Loss: 0.4654\n",
      "Epoch [37/50], Step [710/901], Loss: 0.4274\n",
      "Epoch [37/50], Step [720/901], Loss: 0.4634\n",
      "Epoch [37/50], Step [730/901], Loss: 0.4456\n",
      "Epoch [37/50], Step [740/901], Loss: 0.4242\n",
      "Epoch [37/50], Step [750/901], Loss: 0.4324\n",
      "Epoch [37/50], Step [760/901], Loss: 0.4135\n",
      "Epoch [37/50], Step [770/901], Loss: 0.4282\n",
      "Epoch [37/50], Step [780/901], Loss: 0.4683\n",
      "Epoch [37/50], Step [790/901], Loss: 0.4223\n",
      "Epoch [37/50], Step [800/901], Loss: 0.4601\n",
      "Epoch [37/50], Step [810/901], Loss: 0.4013\n",
      "Epoch [37/50], Step [820/901], Loss: 0.4379\n",
      "Epoch [37/50], Step [830/901], Loss: 0.4387\n",
      "Epoch [37/50], Step [840/901], Loss: 0.4827\n",
      "Epoch [37/50], Step [850/901], Loss: 0.4263\n",
      "Epoch [37/50], Step [860/901], Loss: 0.4557\n",
      "Epoch [37/50], Step [870/901], Loss: 0.4339\n",
      "Epoch [37/50], Step [880/901], Loss: 0.4822\n",
      "Epoch [37/50], Step [890/901], Loss: 0.4501\n",
      "Epoch [37/50], Step [900/901], Loss: 0.4686\n",
      "\n",
      "train loss: 0.4480, train acc: 88.4493\n",
      "validation loss: 0.7049, validation acc: 97.8312\n",
      "\n",
      "Epoch 38\n",
      "\n",
      "Epoch [38/50], Step [0/901], Loss: 0.4796\n",
      "Epoch [38/50], Step [10/901], Loss: 0.4582\n",
      "Epoch [38/50], Step [20/901], Loss: 0.4263\n",
      "Epoch [38/50], Step [30/901], Loss: 0.4053\n",
      "Epoch [38/50], Step [40/901], Loss: 0.4650\n",
      "Epoch [38/50], Step [50/901], Loss: 0.4408\n",
      "Epoch [38/50], Step [60/901], Loss: 0.4674\n",
      "Epoch [38/50], Step [70/901], Loss: 0.4468\n",
      "Epoch [38/50], Step [80/901], Loss: 0.4531\n",
      "Epoch [38/50], Step [90/901], Loss: 0.4606\n",
      "Epoch [38/50], Step [100/901], Loss: 0.4300\n",
      "Epoch [38/50], Step [110/901], Loss: 0.4134\n",
      "Epoch [38/50], Step [120/901], Loss: 0.4629\n",
      "Epoch [38/50], Step [130/901], Loss: 0.4270\n",
      "Epoch [38/50], Step [140/901], Loss: 0.4423\n",
      "Epoch [38/50], Step [150/901], Loss: 0.4336\n",
      "Epoch [38/50], Step [160/901], Loss: 0.4748\n",
      "Epoch [38/50], Step [170/901], Loss: 0.4394\n",
      "Epoch [38/50], Step [180/901], Loss: 0.4198\n",
      "Epoch [38/50], Step [190/901], Loss: 0.4236\n",
      "Epoch [38/50], Step [200/901], Loss: 0.4170\n",
      "Epoch [38/50], Step [210/901], Loss: 0.3927\n",
      "Epoch [38/50], Step [220/901], Loss: 0.4273\n",
      "Epoch [38/50], Step [230/901], Loss: 0.4166\n",
      "Epoch [38/50], Step [240/901], Loss: 0.4560\n",
      "Epoch [38/50], Step [250/901], Loss: 0.4420\n",
      "Epoch [38/50], Step [260/901], Loss: 0.4293\n",
      "Epoch [38/50], Step [270/901], Loss: 0.4817\n",
      "Epoch [38/50], Step [280/901], Loss: 0.4374\n",
      "Epoch [38/50], Step [290/901], Loss: 0.4559\n",
      "Epoch [38/50], Step [300/901], Loss: 0.4375\n",
      "Epoch [38/50], Step [310/901], Loss: 0.4238\n",
      "Epoch [38/50], Step [320/901], Loss: 0.4075\n",
      "Epoch [38/50], Step [330/901], Loss: 0.4388\n",
      "Epoch [38/50], Step [340/901], Loss: 0.4714\n",
      "Epoch [38/50], Step [350/901], Loss: 0.4479\n",
      "Epoch [38/50], Step [360/901], Loss: 0.3972\n",
      "Epoch [38/50], Step [370/901], Loss: 0.4466\n",
      "Epoch [38/50], Step [380/901], Loss: 0.4286\n",
      "Epoch [38/50], Step [390/901], Loss: 0.4669\n",
      "Epoch [38/50], Step [400/901], Loss: 0.4151\n",
      "Epoch [38/50], Step [410/901], Loss: 0.4886\n",
      "Epoch [38/50], Step [420/901], Loss: 0.4748\n",
      "Epoch [38/50], Step [430/901], Loss: 0.4021\n",
      "Epoch [38/50], Step [440/901], Loss: 0.4656\n",
      "Epoch [38/50], Step [450/901], Loss: 0.4407\n",
      "Epoch [38/50], Step [460/901], Loss: 0.4511\n",
      "Epoch [38/50], Step [470/901], Loss: 0.4275\n",
      "Epoch [38/50], Step [480/901], Loss: 0.4175\n",
      "Epoch [38/50], Step [490/901], Loss: 0.4272\n",
      "Epoch [38/50], Step [500/901], Loss: 0.4255\n",
      "Epoch [38/50], Step [510/901], Loss: 0.4078\n",
      "Epoch [38/50], Step [520/901], Loss: 0.4608\n",
      "Epoch [38/50], Step [530/901], Loss: 0.4257\n",
      "Epoch [38/50], Step [540/901], Loss: 0.4221\n",
      "Epoch [38/50], Step [550/901], Loss: 0.4229\n",
      "Epoch [38/50], Step [560/901], Loss: 0.4315\n",
      "Epoch [38/50], Step [570/901], Loss: 0.4308\n",
      "Epoch [38/50], Step [580/901], Loss: 0.4908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/50], Step [590/901], Loss: 0.4510\n",
      "Epoch [38/50], Step [600/901], Loss: 0.4519\n",
      "Epoch [38/50], Step [610/901], Loss: 0.4087\n",
      "Epoch [38/50], Step [620/901], Loss: 0.4648\n",
      "Epoch [38/50], Step [630/901], Loss: 0.4652\n",
      "Epoch [38/50], Step [640/901], Loss: 0.4654\n",
      "Epoch [38/50], Step [650/901], Loss: 0.4404\n",
      "Epoch [38/50], Step [660/901], Loss: 0.4111\n",
      "Epoch [38/50], Step [670/901], Loss: 0.4015\n",
      "Epoch [38/50], Step [680/901], Loss: 0.4329\n",
      "Epoch [38/50], Step [690/901], Loss: 0.4290\n",
      "Epoch [38/50], Step [700/901], Loss: 0.4423\n",
      "Epoch [38/50], Step [710/901], Loss: 0.4310\n",
      "Epoch [38/50], Step [720/901], Loss: 0.4354\n",
      "Epoch [38/50], Step [730/901], Loss: 0.4528\n",
      "Epoch [38/50], Step [740/901], Loss: 0.4350\n",
      "Epoch [38/50], Step [750/901], Loss: 0.4279\n",
      "Epoch [38/50], Step [760/901], Loss: 0.3944\n",
      "Epoch [38/50], Step [770/901], Loss: 0.4241\n",
      "Epoch [38/50], Step [780/901], Loss: 0.4184\n",
      "Epoch [38/50], Step [790/901], Loss: 0.4398\n",
      "Epoch [38/50], Step [800/901], Loss: 0.4647\n",
      "Epoch [38/50], Step [810/901], Loss: 0.4006\n",
      "Epoch [38/50], Step [820/901], Loss: 0.4937\n",
      "Epoch [38/50], Step [830/901], Loss: 0.4702\n",
      "Epoch [38/50], Step [840/901], Loss: 0.4410\n",
      "Epoch [38/50], Step [850/901], Loss: 0.4703\n",
      "Epoch [38/50], Step [860/901], Loss: 0.4668\n",
      "Epoch [38/50], Step [870/901], Loss: 0.4305\n",
      "Epoch [38/50], Step [880/901], Loss: 0.4152\n",
      "Epoch [38/50], Step [890/901], Loss: 0.4631\n",
      "Epoch [38/50], Step [900/901], Loss: 0.4263\n",
      "\n",
      "train loss: 0.4478, train acc: 88.4822\n",
      "validation loss: 0.7049, validation acc: 97.6844\n",
      "\n",
      "Epoch 39\n",
      "\n",
      "Epoch [39/50], Step [0/901], Loss: 0.4211\n",
      "Epoch [39/50], Step [10/901], Loss: 0.4056\n",
      "Epoch [39/50], Step [20/901], Loss: 0.4526\n",
      "Epoch [39/50], Step [30/901], Loss: 0.4336\n",
      "Epoch [39/50], Step [40/901], Loss: 0.3863\n",
      "Epoch [39/50], Step [50/901], Loss: 0.4360\n",
      "Epoch [39/50], Step [60/901], Loss: 0.4305\n",
      "Epoch [39/50], Step [70/901], Loss: 0.4217\n",
      "Epoch [39/50], Step [80/901], Loss: 0.4628\n",
      "Epoch [39/50], Step [90/901], Loss: 0.4196\n",
      "Epoch [39/50], Step [100/901], Loss: 0.4360\n",
      "Epoch [39/50], Step [110/901], Loss: 0.4145\n",
      "Epoch [39/50], Step [120/901], Loss: 0.4221\n",
      "Epoch [39/50], Step [130/901], Loss: 0.4620\n",
      "Epoch [39/50], Step [140/901], Loss: 0.4171\n",
      "Epoch [39/50], Step [150/901], Loss: 0.4418\n",
      "Epoch [39/50], Step [160/901], Loss: 0.4164\n",
      "Epoch [39/50], Step [170/901], Loss: 0.4612\n",
      "Epoch [39/50], Step [180/901], Loss: 0.4349\n",
      "Epoch [39/50], Step [190/901], Loss: 0.4319\n",
      "Epoch [39/50], Step [200/901], Loss: 0.4642\n",
      "Epoch [39/50], Step [210/901], Loss: 0.4287\n",
      "Epoch [39/50], Step [220/901], Loss: 0.4267\n",
      "Epoch [39/50], Step [230/901], Loss: 0.4070\n",
      "Epoch [39/50], Step [240/901], Loss: 0.4384\n",
      "Epoch [39/50], Step [250/901], Loss: 0.4265\n",
      "Epoch [39/50], Step [260/901], Loss: 0.4305\n",
      "Epoch [39/50], Step [270/901], Loss: 0.4388\n",
      "Epoch [39/50], Step [280/901], Loss: 0.4239\n",
      "Epoch [39/50], Step [290/901], Loss: 0.4596\n",
      "Epoch [39/50], Step [300/901], Loss: 0.4231\n",
      "Epoch [39/50], Step [310/901], Loss: 0.4415\n",
      "Epoch [39/50], Step [320/901], Loss: 0.4235\n",
      "Epoch [39/50], Step [330/901], Loss: 0.4421\n",
      "Epoch [39/50], Step [340/901], Loss: 0.4626\n",
      "Epoch [39/50], Step [350/901], Loss: 0.4452\n",
      "Epoch [39/50], Step [360/901], Loss: 0.4281\n",
      "Epoch [39/50], Step [370/901], Loss: 0.4233\n",
      "Epoch [39/50], Step [380/901], Loss: 0.4655\n",
      "Epoch [39/50], Step [390/901], Loss: 0.4338\n",
      "Epoch [39/50], Step [400/901], Loss: 0.4728\n",
      "Epoch [39/50], Step [410/901], Loss: 0.4139\n",
      "Epoch [39/50], Step [420/901], Loss: 0.4813\n",
      "Epoch [39/50], Step [430/901], Loss: 0.4406\n",
      "Epoch [39/50], Step [440/901], Loss: 0.4341\n",
      "Epoch [39/50], Step [450/901], Loss: 0.3874\n",
      "Epoch [39/50], Step [460/901], Loss: 0.4144\n",
      "Epoch [39/50], Step [470/901], Loss: 0.4382\n",
      "Epoch [39/50], Step [480/901], Loss: 0.4055\n",
      "Epoch [39/50], Step [490/901], Loss: 0.4463\n",
      "Epoch [39/50], Step [500/901], Loss: 0.4551\n",
      "Epoch [39/50], Step [510/901], Loss: 0.4875\n",
      "Epoch [39/50], Step [520/901], Loss: 0.4556\n",
      "Epoch [39/50], Step [530/901], Loss: 0.4771\n",
      "Epoch [39/50], Step [540/901], Loss: 0.4433\n",
      "Epoch [39/50], Step [550/901], Loss: 0.4337\n",
      "Epoch [39/50], Step [560/901], Loss: 0.4168\n",
      "Epoch [39/50], Step [570/901], Loss: 0.4335\n",
      "Epoch [39/50], Step [580/901], Loss: 0.4072\n",
      "Epoch [39/50], Step [590/901], Loss: 0.4734\n",
      "Epoch [39/50], Step [600/901], Loss: 0.4242\n",
      "Epoch [39/50], Step [610/901], Loss: 0.4439\n",
      "Epoch [39/50], Step [620/901], Loss: 0.3940\n",
      "Epoch [39/50], Step [630/901], Loss: 0.4391\n",
      "Epoch [39/50], Step [640/901], Loss: 0.4948\n",
      "Epoch [39/50], Step [650/901], Loss: 0.3998\n",
      "Epoch [39/50], Step [660/901], Loss: 0.4406\n",
      "Epoch [39/50], Step [670/901], Loss: 0.4400\n",
      "Epoch [39/50], Step [680/901], Loss: 0.4829\n",
      "Epoch [39/50], Step [690/901], Loss: 0.4532\n",
      "Epoch [39/50], Step [700/901], Loss: 0.4139\n",
      "Epoch [39/50], Step [710/901], Loss: 0.4404\n",
      "Epoch [39/50], Step [720/901], Loss: 0.4300\n",
      "Epoch [39/50], Step [730/901], Loss: 0.4455\n",
      "Epoch [39/50], Step [740/901], Loss: 0.4175\n",
      "Epoch [39/50], Step [750/901], Loss: 0.4428\n",
      "Epoch [39/50], Step [760/901], Loss: 0.4767\n",
      "Epoch [39/50], Step [770/901], Loss: 0.3938\n",
      "Epoch [39/50], Step [780/901], Loss: 0.4138\n",
      "Epoch [39/50], Step [790/901], Loss: 0.3891\n",
      "Epoch [39/50], Step [800/901], Loss: 0.4165\n",
      "Epoch [39/50], Step [810/901], Loss: 0.5083\n",
      "Epoch [39/50], Step [820/901], Loss: 0.4223\n",
      "Epoch [39/50], Step [830/901], Loss: 0.4162\n",
      "Epoch [39/50], Step [840/901], Loss: 0.4255\n",
      "Epoch [39/50], Step [850/901], Loss: 0.4577\n",
      "Epoch [39/50], Step [860/901], Loss: 0.4374\n",
      "Epoch [39/50], Step [870/901], Loss: 0.4060\n",
      "Epoch [39/50], Step [880/901], Loss: 0.4392\n",
      "Epoch [39/50], Step [890/901], Loss: 0.4134\n",
      "Epoch [39/50], Step [900/901], Loss: 0.4448\n",
      "\n",
      "train loss: 0.4475, train acc: 88.5395\n",
      "validation loss: 0.7048, validation acc: 97.4775\n",
      "\n",
      "Epoch 40\n",
      "\n",
      "Epoch [40/50], Step [0/901], Loss: 0.4411\n",
      "Epoch [40/50], Step [10/901], Loss: 0.4373\n",
      "Epoch [40/50], Step [20/901], Loss: 0.3932\n",
      "Epoch [40/50], Step [30/901], Loss: 0.4355\n",
      "Epoch [40/50], Step [40/901], Loss: 0.4403\n",
      "Epoch [40/50], Step [50/901], Loss: 0.4649\n",
      "Epoch [40/50], Step [60/901], Loss: 0.4349\n",
      "Epoch [40/50], Step [70/901], Loss: 0.4443\n",
      "Epoch [40/50], Step [80/901], Loss: 0.4174\n",
      "Epoch [40/50], Step [90/901], Loss: 0.4399\n",
      "Epoch [40/50], Step [100/901], Loss: 0.4536\n",
      "Epoch [40/50], Step [110/901], Loss: 0.4071\n",
      "Epoch [40/50], Step [120/901], Loss: 0.4185\n",
      "Epoch [40/50], Step [130/901], Loss: 0.4348\n",
      "Epoch [40/50], Step [140/901], Loss: 0.4113\n",
      "Epoch [40/50], Step [150/901], Loss: 0.4638\n",
      "Epoch [40/50], Step [160/901], Loss: 0.4311\n",
      "Epoch [40/50], Step [170/901], Loss: 0.4379\n",
      "Epoch [40/50], Step [180/901], Loss: 0.4621\n",
      "Epoch [40/50], Step [190/901], Loss: 0.4394\n",
      "Epoch [40/50], Step [200/901], Loss: 0.4222\n",
      "Epoch [40/50], Step [210/901], Loss: 0.4014\n",
      "Epoch [40/50], Step [220/901], Loss: 0.4558\n",
      "Epoch [40/50], Step [230/901], Loss: 0.4071\n",
      "Epoch [40/50], Step [240/901], Loss: 0.4511\n",
      "Epoch [40/50], Step [250/901], Loss: 0.4143\n",
      "Epoch [40/50], Step [260/901], Loss: 0.4192\n",
      "Epoch [40/50], Step [270/901], Loss: 0.4375\n",
      "Epoch [40/50], Step [280/901], Loss: 0.4253\n",
      "Epoch [40/50], Step [290/901], Loss: 0.4292\n",
      "Epoch [40/50], Step [300/901], Loss: 0.4481\n",
      "Epoch [40/50], Step [310/901], Loss: 0.4339\n",
      "Epoch [40/50], Step [320/901], Loss: 0.3941\n",
      "Epoch [40/50], Step [330/901], Loss: 0.3956\n",
      "Epoch [40/50], Step [340/901], Loss: 0.4534\n",
      "Epoch [40/50], Step [350/901], Loss: 0.4085\n",
      "Epoch [40/50], Step [360/901], Loss: 0.4053\n",
      "Epoch [40/50], Step [370/901], Loss: 0.4596\n",
      "Epoch [40/50], Step [380/901], Loss: 0.4476\n",
      "Epoch [40/50], Step [390/901], Loss: 0.4808\n",
      "Epoch [40/50], Step [400/901], Loss: 0.4288\n",
      "Epoch [40/50], Step [410/901], Loss: 0.4352\n",
      "Epoch [40/50], Step [420/901], Loss: 0.4169\n",
      "Epoch [40/50], Step [430/901], Loss: 0.4854\n",
      "Epoch [40/50], Step [440/901], Loss: 0.4414\n",
      "Epoch [40/50], Step [450/901], Loss: 0.4606\n",
      "Epoch [40/50], Step [460/901], Loss: 0.4523\n",
      "Epoch [40/50], Step [470/901], Loss: 0.4376\n",
      "Epoch [40/50], Step [480/901], Loss: 0.4326\n",
      "Epoch [40/50], Step [490/901], Loss: 0.4638\n",
      "Epoch [40/50], Step [500/901], Loss: 0.4889\n",
      "Epoch [40/50], Step [510/901], Loss: 0.4153\n",
      "Epoch [40/50], Step [520/901], Loss: 0.4559\n",
      "Epoch [40/50], Step [530/901], Loss: 0.4270\n",
      "Epoch [40/50], Step [540/901], Loss: 0.4630\n",
      "Epoch [40/50], Step [550/901], Loss: 0.4760\n",
      "Epoch [40/50], Step [560/901], Loss: 0.4510\n",
      "Epoch [40/50], Step [570/901], Loss: 0.4149\n",
      "Epoch [40/50], Step [580/901], Loss: 0.4403\n",
      "Epoch [40/50], Step [590/901], Loss: 0.4570\n",
      "Epoch [40/50], Step [600/901], Loss: 0.4657\n",
      "Epoch [40/50], Step [610/901], Loss: 0.4210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/50], Step [620/901], Loss: 0.4492\n",
      "Epoch [40/50], Step [630/901], Loss: 0.4443\n",
      "Epoch [40/50], Step [640/901], Loss: 0.4330\n",
      "Epoch [40/50], Step [650/901], Loss: 0.4146\n",
      "Epoch [40/50], Step [660/901], Loss: 0.4237\n",
      "Epoch [40/50], Step [670/901], Loss: 0.4308\n",
      "Epoch [40/50], Step [680/901], Loss: 0.4483\n",
      "Epoch [40/50], Step [690/901], Loss: 0.4594\n",
      "Epoch [40/50], Step [700/901], Loss: 0.4600\n",
      "Epoch [40/50], Step [710/901], Loss: 0.4248\n",
      "Epoch [40/50], Step [720/901], Loss: 0.4223\n",
      "Epoch [40/50], Step [730/901], Loss: 0.4392\n",
      "Epoch [40/50], Step [740/901], Loss: 0.4520\n",
      "Epoch [40/50], Step [750/901], Loss: 0.4271\n",
      "Epoch [40/50], Step [760/901], Loss: 0.4229\n",
      "Epoch [40/50], Step [770/901], Loss: 0.4368\n",
      "Epoch [40/50], Step [780/901], Loss: 0.4366\n",
      "Epoch [40/50], Step [790/901], Loss: 0.4427\n",
      "Epoch [40/50], Step [800/901], Loss: 0.4472\n",
      "Epoch [40/50], Step [810/901], Loss: 0.4345\n",
      "Epoch [40/50], Step [820/901], Loss: 0.4456\n",
      "Epoch [40/50], Step [830/901], Loss: 0.4482\n",
      "Epoch [40/50], Step [840/901], Loss: 0.4421\n",
      "Epoch [40/50], Step [850/901], Loss: 0.4448\n",
      "Epoch [40/50], Step [860/901], Loss: 0.4138\n",
      "Epoch [40/50], Step [870/901], Loss: 0.4063\n",
      "Epoch [40/50], Step [880/901], Loss: 0.4201\n",
      "Epoch [40/50], Step [890/901], Loss: 0.4484\n",
      "Epoch [40/50], Step [900/901], Loss: 0.4083\n",
      "\n",
      "train loss: 0.4473, train acc: 88.4991\n",
      "validation loss: 0.7049, validation acc: 97.4975\n",
      "\n",
      "Epoch 41\n",
      "\n",
      "Epoch [41/50], Step [0/901], Loss: 0.4266\n",
      "Epoch [41/50], Step [10/901], Loss: 0.4016\n",
      "Epoch [41/50], Step [20/901], Loss: 0.4253\n",
      "Epoch [41/50], Step [30/901], Loss: 0.4261\n",
      "Epoch [41/50], Step [40/901], Loss: 0.4280\n",
      "Epoch [41/50], Step [50/901], Loss: 0.4456\n",
      "Epoch [41/50], Step [60/901], Loss: 0.4021\n",
      "Epoch [41/50], Step [70/901], Loss: 0.4453\n",
      "Epoch [41/50], Step [80/901], Loss: 0.4168\n",
      "Epoch [41/50], Step [90/901], Loss: 0.4643\n",
      "Epoch [41/50], Step [100/901], Loss: 0.4069\n",
      "Epoch [41/50], Step [110/901], Loss: 0.4798\n",
      "Epoch [41/50], Step [120/901], Loss: 0.4955\n",
      "Epoch [41/50], Step [130/901], Loss: 0.4360\n",
      "Epoch [41/50], Step [140/901], Loss: 0.4480\n",
      "Epoch [41/50], Step [150/901], Loss: 0.4400\n",
      "Epoch [41/50], Step [160/901], Loss: 0.4722\n",
      "Epoch [41/50], Step [170/901], Loss: 0.4370\n",
      "Epoch [41/50], Step [180/901], Loss: 0.4359\n",
      "Epoch [41/50], Step [190/901], Loss: 0.4446\n",
      "Epoch [41/50], Step [200/901], Loss: 0.4543\n",
      "Epoch [41/50], Step [210/901], Loss: 0.4095\n",
      "Epoch [41/50], Step [220/901], Loss: 0.4165\n",
      "Epoch [41/50], Step [230/901], Loss: 0.4186\n",
      "Epoch [41/50], Step [240/901], Loss: 0.4151\n",
      "Epoch [41/50], Step [250/901], Loss: 0.4256\n",
      "Epoch [41/50], Step [260/901], Loss: 0.4223\n",
      "Epoch [41/50], Step [270/901], Loss: 0.4691\n",
      "Epoch [41/50], Step [280/901], Loss: 0.3889\n",
      "Epoch [41/50], Step [290/901], Loss: 0.4798\n",
      "Epoch [41/50], Step [300/901], Loss: 0.4570\n",
      "Epoch [41/50], Step [310/901], Loss: 0.4097\n",
      "Epoch [41/50], Step [320/901], Loss: 0.4590\n",
      "Epoch [41/50], Step [330/901], Loss: 0.4624\n",
      "Epoch [41/50], Step [340/901], Loss: 0.4534\n",
      "Epoch [41/50], Step [350/901], Loss: 0.4510\n",
      "Epoch [41/50], Step [360/901], Loss: 0.4285\n",
      "Epoch [41/50], Step [370/901], Loss: 0.4705\n",
      "Epoch [41/50], Step [380/901], Loss: 0.4171\n",
      "Epoch [41/50], Step [390/901], Loss: 0.4144\n",
      "Epoch [41/50], Step [400/901], Loss: 0.4563\n",
      "Epoch [41/50], Step [410/901], Loss: 0.4429\n",
      "Epoch [41/50], Step [420/901], Loss: 0.4444\n",
      "Epoch [41/50], Step [430/901], Loss: 0.4156\n",
      "Epoch [41/50], Step [440/901], Loss: 0.4675\n",
      "Epoch [41/50], Step [450/901], Loss: 0.4912\n",
      "Epoch [41/50], Step [460/901], Loss: 0.4358\n",
      "Epoch [41/50], Step [470/901], Loss: 0.4345\n",
      "Epoch [41/50], Step [480/901], Loss: 0.4349\n",
      "Epoch [41/50], Step [490/901], Loss: 0.4233\n",
      "Epoch [41/50], Step [500/901], Loss: 0.3842\n",
      "Epoch [41/50], Step [510/901], Loss: 0.4201\n",
      "Epoch [41/50], Step [520/901], Loss: 0.4213\n",
      "Epoch [41/50], Step [530/901], Loss: 0.4452\n",
      "Epoch [41/50], Step [540/901], Loss: 0.4479\n",
      "Epoch [41/50], Step [550/901], Loss: 0.4351\n",
      "Epoch [41/50], Step [560/901], Loss: 0.4052\n",
      "Epoch [41/50], Step [570/901], Loss: 0.4083\n",
      "Epoch [41/50], Step [580/901], Loss: 0.4017\n",
      "Epoch [41/50], Step [590/901], Loss: 0.4437\n",
      "Epoch [41/50], Step [600/901], Loss: 0.4627\n",
      "Epoch [41/50], Step [610/901], Loss: 0.3830\n",
      "Epoch [41/50], Step [620/901], Loss: 0.4165\n",
      "Epoch [41/50], Step [630/901], Loss: 0.4500\n",
      "Epoch [41/50], Step [640/901], Loss: 0.4203\n",
      "Epoch [41/50], Step [650/901], Loss: 0.4166\n",
      "Epoch [41/50], Step [660/901], Loss: 0.4009\n",
      "Epoch [41/50], Step [670/901], Loss: 0.4119\n",
      "Epoch [41/50], Step [680/901], Loss: 0.4720\n",
      "Epoch [41/50], Step [690/901], Loss: 0.4261\n",
      "Epoch [41/50], Step [700/901], Loss: 0.4396\n",
      "Epoch [41/50], Step [710/901], Loss: 0.4660\n",
      "Epoch [41/50], Step [720/901], Loss: 0.4389\n",
      "Epoch [41/50], Step [730/901], Loss: 0.4475\n",
      "Epoch [41/50], Step [740/901], Loss: 0.4561\n",
      "Epoch [41/50], Step [750/901], Loss: 0.4865\n",
      "Epoch [41/50], Step [760/901], Loss: 0.4236\n",
      "Epoch [41/50], Step [770/901], Loss: 0.4425\n",
      "Epoch [41/50], Step [780/901], Loss: 0.4229\n",
      "Epoch [41/50], Step [790/901], Loss: 0.4193\n",
      "Epoch [41/50], Step [800/901], Loss: 0.4747\n",
      "Epoch [41/50], Step [810/901], Loss: 0.4856\n",
      "Epoch [41/50], Step [820/901], Loss: 0.4030\n",
      "Epoch [41/50], Step [830/901], Loss: 0.4316\n",
      "Epoch [41/50], Step [840/901], Loss: 0.4407\n",
      "Epoch [41/50], Step [850/901], Loss: 0.4219\n",
      "Epoch [41/50], Step [860/901], Loss: 0.4899\n",
      "Epoch [41/50], Step [870/901], Loss: 0.4520\n",
      "Epoch [41/50], Step [880/901], Loss: 0.4626\n",
      "Epoch [41/50], Step [890/901], Loss: 0.4097\n",
      "Epoch [41/50], Step [900/901], Loss: 0.4457\n",
      "\n",
      "train loss: 0.4470, train acc: 88.5234\n",
      "validation loss: 0.7049, validation acc: 97.6577\n",
      "\n",
      "Epoch 42\n",
      "\n",
      "Epoch [42/50], Step [0/901], Loss: 0.4455\n",
      "Epoch [42/50], Step [10/901], Loss: 0.3933\n",
      "Epoch [42/50], Step [20/901], Loss: 0.4173\n",
      "Epoch [42/50], Step [30/901], Loss: 0.4744\n",
      "Epoch [42/50], Step [40/901], Loss: 0.4698\n",
      "Epoch [42/50], Step [50/901], Loss: 0.4603\n",
      "Epoch [42/50], Step [60/901], Loss: 0.4606\n",
      "Epoch [42/50], Step [70/901], Loss: 0.4499\n",
      "Epoch [42/50], Step [80/901], Loss: 0.4297\n",
      "Epoch [42/50], Step [90/901], Loss: 0.4136\n",
      "Epoch [42/50], Step [100/901], Loss: 0.4223\n",
      "Epoch [42/50], Step [110/901], Loss: 0.4494\n",
      "Epoch [42/50], Step [120/901], Loss: 0.4176\n",
      "Epoch [42/50], Step [130/901], Loss: 0.4245\n",
      "Epoch [42/50], Step [140/901], Loss: 0.3831\n",
      "Epoch [42/50], Step [150/901], Loss: 0.4171\n",
      "Epoch [42/50], Step [160/901], Loss: 0.4143\n",
      "Epoch [42/50], Step [170/901], Loss: 0.4317\n",
      "Epoch [42/50], Step [180/901], Loss: 0.4692\n",
      "Epoch [42/50], Step [190/901], Loss: 0.4058\n",
      "Epoch [42/50], Step [200/901], Loss: 0.4345\n",
      "Epoch [42/50], Step [210/901], Loss: 0.4837\n",
      "Epoch [42/50], Step [220/901], Loss: 0.4260\n",
      "Epoch [42/50], Step [230/901], Loss: 0.4245\n",
      "Epoch [42/50], Step [240/901], Loss: 0.4421\n",
      "Epoch [42/50], Step [250/901], Loss: 0.4389\n",
      "Epoch [42/50], Step [260/901], Loss: 0.4262\n",
      "Epoch [42/50], Step [270/901], Loss: 0.4313\n",
      "Epoch [42/50], Step [280/901], Loss: 0.4173\n",
      "Epoch [42/50], Step [290/901], Loss: 0.4033\n",
      "Epoch [42/50], Step [300/901], Loss: 0.4131\n",
      "Epoch [42/50], Step [310/901], Loss: 0.4601\n",
      "Epoch [42/50], Step [320/901], Loss: 0.4030\n",
      "Epoch [42/50], Step [330/901], Loss: 0.4517\n",
      "Epoch [42/50], Step [340/901], Loss: 0.4382\n",
      "Epoch [42/50], Step [350/901], Loss: 0.3967\n",
      "Epoch [42/50], Step [360/901], Loss: 0.4425\n",
      "Epoch [42/50], Step [370/901], Loss: 0.4885\n",
      "Epoch [42/50], Step [380/901], Loss: 0.4156\n",
      "Epoch [42/50], Step [390/901], Loss: 0.3930\n",
      "Epoch [42/50], Step [400/901], Loss: 0.4425\n",
      "Epoch [42/50], Step [410/901], Loss: 0.4217\n",
      "Epoch [42/50], Step [420/901], Loss: 0.4400\n",
      "Epoch [42/50], Step [430/901], Loss: 0.4112\n",
      "Epoch [42/50], Step [440/901], Loss: 0.4593\n",
      "Epoch [42/50], Step [450/901], Loss: 0.4436\n",
      "Epoch [42/50], Step [460/901], Loss: 0.4090\n",
      "Epoch [42/50], Step [470/901], Loss: 0.4305\n",
      "Epoch [42/50], Step [480/901], Loss: 0.4505\n",
      "Epoch [42/50], Step [490/901], Loss: 0.4467\n",
      "Epoch [42/50], Step [500/901], Loss: 0.4166\n",
      "Epoch [42/50], Step [510/901], Loss: 0.4541\n",
      "Epoch [42/50], Step [520/901], Loss: 0.4559\n",
      "Epoch [42/50], Step [530/901], Loss: 0.4728\n",
      "Epoch [42/50], Step [540/901], Loss: 0.4696\n",
      "Epoch [42/50], Step [550/901], Loss: 0.3864\n",
      "Epoch [42/50], Step [560/901], Loss: 0.4468\n",
      "Epoch [42/50], Step [570/901], Loss: 0.4115\n",
      "Epoch [42/50], Step [580/901], Loss: 0.4751\n",
      "Epoch [42/50], Step [590/901], Loss: 0.4464\n",
      "Epoch [42/50], Step [600/901], Loss: 0.4308\n",
      "Epoch [42/50], Step [610/901], Loss: 0.4481\n",
      "Epoch [42/50], Step [620/901], Loss: 0.4512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/50], Step [630/901], Loss: 0.4219\n",
      "Epoch [42/50], Step [640/901], Loss: 0.4100\n",
      "Epoch [42/50], Step [650/901], Loss: 0.4661\n",
      "Epoch [42/50], Step [660/901], Loss: 0.3899\n",
      "Epoch [42/50], Step [670/901], Loss: 0.3950\n",
      "Epoch [42/50], Step [680/901], Loss: 0.4670\n",
      "Epoch [42/50], Step [690/901], Loss: 0.4429\n",
      "Epoch [42/50], Step [700/901], Loss: 0.4710\n",
      "Epoch [42/50], Step [710/901], Loss: 0.4239\n",
      "Epoch [42/50], Step [720/901], Loss: 0.4362\n",
      "Epoch [42/50], Step [730/901], Loss: 0.4672\n",
      "Epoch [42/50], Step [740/901], Loss: 0.4548\n",
      "Epoch [42/50], Step [750/901], Loss: 0.4133\n",
      "Epoch [42/50], Step [760/901], Loss: 0.4833\n",
      "Epoch [42/50], Step [770/901], Loss: 0.4473\n",
      "Epoch [42/50], Step [780/901], Loss: 0.4082\n",
      "Epoch [42/50], Step [790/901], Loss: 0.4581\n",
      "Epoch [42/50], Step [800/901], Loss: 0.4657\n",
      "Epoch [42/50], Step [810/901], Loss: 0.4681\n",
      "Epoch [42/50], Step [820/901], Loss: 0.4329\n",
      "Epoch [42/50], Step [830/901], Loss: 0.4359\n",
      "Epoch [42/50], Step [840/901], Loss: 0.4444\n",
      "Epoch [42/50], Step [850/901], Loss: 0.4168\n",
      "Epoch [42/50], Step [860/901], Loss: 0.4173\n",
      "Epoch [42/50], Step [870/901], Loss: 0.4569\n",
      "Epoch [42/50], Step [880/901], Loss: 0.4268\n",
      "Epoch [42/50], Step [890/901], Loss: 0.4671\n",
      "Epoch [42/50], Step [900/901], Loss: 0.4398\n",
      "\n",
      "train loss: 0.4468, train acc: 88.4623\n",
      "validation loss: 0.7049, validation acc: 97.8111\n",
      "\n",
      "Epoch 43\n",
      "\n",
      "Epoch [43/50], Step [0/901], Loss: 0.4123\n",
      "Epoch [43/50], Step [10/901], Loss: 0.4269\n",
      "Epoch [43/50], Step [20/901], Loss: 0.4391\n",
      "Epoch [43/50], Step [30/901], Loss: 0.4277\n",
      "Epoch [43/50], Step [40/901], Loss: 0.4580\n",
      "Epoch [43/50], Step [50/901], Loss: 0.4480\n",
      "Epoch [43/50], Step [60/901], Loss: 0.4671\n",
      "Epoch [43/50], Step [70/901], Loss: 0.4077\n",
      "Epoch [43/50], Step [80/901], Loss: 0.4598\n",
      "Epoch [43/50], Step [90/901], Loss: 0.4534\n",
      "Epoch [43/50], Step [100/901], Loss: 0.3892\n",
      "Epoch [43/50], Step [110/901], Loss: 0.4275\n",
      "Epoch [43/50], Step [120/901], Loss: 0.4861\n",
      "Epoch [43/50], Step [130/901], Loss: 0.4750\n",
      "Epoch [43/50], Step [140/901], Loss: 0.4846\n",
      "Epoch [43/50], Step [150/901], Loss: 0.4244\n",
      "Epoch [43/50], Step [160/901], Loss: 0.4259\n",
      "Epoch [43/50], Step [170/901], Loss: 0.4690\n",
      "Epoch [43/50], Step [180/901], Loss: 0.4265\n",
      "Epoch [43/50], Step [190/901], Loss: 0.4189\n",
      "Epoch [43/50], Step [200/901], Loss: 0.4597\n",
      "Epoch [43/50], Step [210/901], Loss: 0.4399\n",
      "Epoch [43/50], Step [220/901], Loss: 0.4291\n",
      "Epoch [43/50], Step [230/901], Loss: 0.4380\n",
      "Epoch [43/50], Step [240/901], Loss: 0.4182\n",
      "Epoch [43/50], Step [250/901], Loss: 0.4111\n",
      "Epoch [43/50], Step [260/901], Loss: 0.4329\n",
      "Epoch [43/50], Step [270/901], Loss: 0.4262\n",
      "Epoch [43/50], Step [280/901], Loss: 0.4389\n",
      "Epoch [43/50], Step [290/901], Loss: 0.4525\n",
      "Epoch [43/50], Step [300/901], Loss: 0.4145\n",
      "Epoch [43/50], Step [310/901], Loss: 0.4087\n",
      "Epoch [43/50], Step [320/901], Loss: 0.4497\n",
      "Epoch [43/50], Step [330/901], Loss: 0.4086\n",
      "Epoch [43/50], Step [340/901], Loss: 0.4170\n",
      "Epoch [43/50], Step [350/901], Loss: 0.3880\n",
      "Epoch [43/50], Step [360/901], Loss: 0.4296\n",
      "Epoch [43/50], Step [370/901], Loss: 0.4361\n",
      "Epoch [43/50], Step [380/901], Loss: 0.4644\n",
      "Epoch [43/50], Step [390/901], Loss: 0.3958\n",
      "Epoch [43/50], Step [400/901], Loss: 0.3842\n",
      "Epoch [43/50], Step [410/901], Loss: 0.4326\n",
      "Epoch [43/50], Step [420/901], Loss: 0.4617\n",
      "Epoch [43/50], Step [430/901], Loss: 0.4246\n",
      "Epoch [43/50], Step [440/901], Loss: 0.4513\n",
      "Epoch [43/50], Step [450/901], Loss: 0.4402\n",
      "Epoch [43/50], Step [460/901], Loss: 0.4668\n",
      "Epoch [43/50], Step [470/901], Loss: 0.4465\n",
      "Epoch [43/50], Step [480/901], Loss: 0.4140\n",
      "Epoch [43/50], Step [490/901], Loss: 0.4161\n",
      "Epoch [43/50], Step [500/901], Loss: 0.4162\n",
      "Epoch [43/50], Step [510/901], Loss: 0.4571\n",
      "Epoch [43/50], Step [520/901], Loss: 0.4845\n",
      "Epoch [43/50], Step [530/901], Loss: 0.4639\n",
      "Epoch [43/50], Step [540/901], Loss: 0.4439\n",
      "Epoch [43/50], Step [550/901], Loss: 0.4217\n",
      "Epoch [43/50], Step [560/901], Loss: 0.4805\n",
      "Epoch [43/50], Step [570/901], Loss: 0.4594\n",
      "Epoch [43/50], Step [580/901], Loss: 0.4441\n",
      "Epoch [43/50], Step [590/901], Loss: 0.4417\n",
      "Epoch [43/50], Step [600/901], Loss: 0.4252\n",
      "Epoch [43/50], Step [610/901], Loss: 0.4600\n",
      "Epoch [43/50], Step [620/901], Loss: 0.4039\n",
      "Epoch [43/50], Step [630/901], Loss: 0.4710\n",
      "Epoch [43/50], Step [640/901], Loss: 0.4168\n",
      "Epoch [43/50], Step [650/901], Loss: 0.4246\n",
      "Epoch [43/50], Step [660/901], Loss: 0.4286\n",
      "Epoch [43/50], Step [670/901], Loss: 0.4259\n",
      "Epoch [43/50], Step [680/901], Loss: 0.4603\n",
      "Epoch [43/50], Step [690/901], Loss: 0.4444\n",
      "Epoch [43/50], Step [700/901], Loss: 0.4460\n",
      "Epoch [43/50], Step [710/901], Loss: 0.4542\n",
      "Epoch [43/50], Step [720/901], Loss: 0.4542\n",
      "Epoch [43/50], Step [730/901], Loss: 0.4410\n",
      "Epoch [43/50], Step [740/901], Loss: 0.4635\n",
      "Epoch [43/50], Step [750/901], Loss: 0.4655\n",
      "Epoch [43/50], Step [760/901], Loss: 0.4541\n",
      "Epoch [43/50], Step [770/901], Loss: 0.4178\n",
      "Epoch [43/50], Step [780/901], Loss: 0.4576\n",
      "Epoch [43/50], Step [790/901], Loss: 0.5002\n",
      "Epoch [43/50], Step [800/901], Loss: 0.4597\n",
      "Epoch [43/50], Step [810/901], Loss: 0.4431\n",
      "Epoch [43/50], Step [820/901], Loss: 0.4214\n",
      "Epoch [43/50], Step [830/901], Loss: 0.4401\n",
      "Epoch [43/50], Step [840/901], Loss: 0.5032\n",
      "Epoch [43/50], Step [850/901], Loss: 0.4478\n",
      "Epoch [43/50], Step [860/901], Loss: 0.4460\n",
      "Epoch [43/50], Step [870/901], Loss: 0.4602\n",
      "Epoch [43/50], Step [880/901], Loss: 0.4293\n",
      "Epoch [43/50], Step [890/901], Loss: 0.4544\n",
      "Epoch [43/50], Step [900/901], Loss: 0.4512\n",
      "\n",
      "train loss: 0.4466, train acc: 88.4809\n",
      "validation loss: 0.7047, validation acc: 97.9980\n",
      "\n",
      "Epoch 44\n",
      "\n",
      "Epoch [44/50], Step [0/901], Loss: 0.4478\n",
      "Epoch [44/50], Step [10/901], Loss: 0.4361\n",
      "Epoch [44/50], Step [20/901], Loss: 0.4406\n",
      "Epoch [44/50], Step [30/901], Loss: 0.4429\n",
      "Epoch [44/50], Step [40/901], Loss: 0.4501\n",
      "Epoch [44/50], Step [50/901], Loss: 0.4232\n",
      "Epoch [44/50], Step [60/901], Loss: 0.4687\n",
      "Epoch [44/50], Step [70/901], Loss: 0.4160\n",
      "Epoch [44/50], Step [80/901], Loss: 0.4305\n",
      "Epoch [44/50], Step [90/901], Loss: 0.4270\n",
      "Epoch [44/50], Step [100/901], Loss: 0.4364\n",
      "Epoch [44/50], Step [110/901], Loss: 0.4169\n",
      "Epoch [44/50], Step [120/901], Loss: 0.4307\n",
      "Epoch [44/50], Step [130/901], Loss: 0.4619\n",
      "Epoch [44/50], Step [140/901], Loss: 0.4531\n",
      "Epoch [44/50], Step [150/901], Loss: 0.4395\n",
      "Epoch [44/50], Step [160/901], Loss: 0.4409\n",
      "Epoch [44/50], Step [170/901], Loss: 0.4227\n",
      "Epoch [44/50], Step [180/901], Loss: 0.4491\n",
      "Epoch [44/50], Step [190/901], Loss: 0.4711\n",
      "Epoch [44/50], Step [200/901], Loss: 0.4379\n",
      "Epoch [44/50], Step [210/901], Loss: 0.4465\n",
      "Epoch [44/50], Step [220/901], Loss: 0.4553\n",
      "Epoch [44/50], Step [230/901], Loss: 0.4470\n",
      "Epoch [44/50], Step [240/901], Loss: 0.4536\n",
      "Epoch [44/50], Step [250/901], Loss: 0.4676\n",
      "Epoch [44/50], Step [260/901], Loss: 0.4474\n",
      "Epoch [44/50], Step [270/901], Loss: 0.4144\n",
      "Epoch [44/50], Step [280/901], Loss: 0.4297\n",
      "Epoch [44/50], Step [290/901], Loss: 0.4272\n",
      "Epoch [44/50], Step [300/901], Loss: 0.4215\n",
      "Epoch [44/50], Step [310/901], Loss: 0.4309\n",
      "Epoch [44/50], Step [320/901], Loss: 0.4230\n",
      "Epoch [44/50], Step [330/901], Loss: 0.4517\n",
      "Epoch [44/50], Step [340/901], Loss: 0.4355\n",
      "Epoch [44/50], Step [350/901], Loss: 0.4397\n",
      "Epoch [44/50], Step [360/901], Loss: 0.4147\n",
      "Epoch [44/50], Step [370/901], Loss: 0.4341\n",
      "Epoch [44/50], Step [380/901], Loss: 0.4479\n",
      "Epoch [44/50], Step [390/901], Loss: 0.4640\n",
      "Epoch [44/50], Step [400/901], Loss: 0.4735\n",
      "Epoch [44/50], Step [410/901], Loss: 0.4361\n",
      "Epoch [44/50], Step [420/901], Loss: 0.4134\n",
      "Epoch [44/50], Step [430/901], Loss: 0.4713\n",
      "Epoch [44/50], Step [440/901], Loss: 0.3898\n",
      "Epoch [44/50], Step [450/901], Loss: 0.4507\n",
      "Epoch [44/50], Step [460/901], Loss: 0.4141\n",
      "Epoch [44/50], Step [470/901], Loss: 0.4510\n",
      "Epoch [44/50], Step [480/901], Loss: 0.4597\n",
      "Epoch [44/50], Step [490/901], Loss: 0.4346\n",
      "Epoch [44/50], Step [500/901], Loss: 0.4276\n",
      "Epoch [44/50], Step [510/901], Loss: 0.4513\n",
      "Epoch [44/50], Step [520/901], Loss: 0.4410\n",
      "Epoch [44/50], Step [530/901], Loss: 0.4399\n",
      "Epoch [44/50], Step [540/901], Loss: 0.4104\n",
      "Epoch [44/50], Step [550/901], Loss: 0.4489\n",
      "Epoch [44/50], Step [560/901], Loss: 0.4355\n",
      "Epoch [44/50], Step [570/901], Loss: 0.4181\n",
      "Epoch [44/50], Step [580/901], Loss: 0.4269\n",
      "Epoch [44/50], Step [590/901], Loss: 0.4660\n",
      "Epoch [44/50], Step [600/901], Loss: 0.4324\n",
      "Epoch [44/50], Step [610/901], Loss: 0.4010\n",
      "Epoch [44/50], Step [620/901], Loss: 0.4634\n",
      "Epoch [44/50], Step [630/901], Loss: 0.4333\n",
      "Epoch [44/50], Step [640/901], Loss: 0.5007\n",
      "Epoch [44/50], Step [650/901], Loss: 0.4385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/50], Step [660/901], Loss: 0.4672\n",
      "Epoch [44/50], Step [670/901], Loss: 0.4337\n",
      "Epoch [44/50], Step [680/901], Loss: 0.4631\n",
      "Epoch [44/50], Step [690/901], Loss: 0.4283\n",
      "Epoch [44/50], Step [700/901], Loss: 0.4344\n",
      "Epoch [44/50], Step [710/901], Loss: 0.4872\n",
      "Epoch [44/50], Step [720/901], Loss: 0.4515\n",
      "Epoch [44/50], Step [730/901], Loss: 0.4489\n",
      "Epoch [44/50], Step [740/901], Loss: 0.4974\n",
      "Epoch [44/50], Step [750/901], Loss: 0.4310\n",
      "Epoch [44/50], Step [760/901], Loss: 0.4166\n",
      "Epoch [44/50], Step [770/901], Loss: 0.4106\n",
      "Epoch [44/50], Step [780/901], Loss: 0.4890\n",
      "Epoch [44/50], Step [790/901], Loss: 0.4327\n",
      "Epoch [44/50], Step [800/901], Loss: 0.4610\n",
      "Epoch [44/50], Step [810/901], Loss: 0.4484\n",
      "Epoch [44/50], Step [820/901], Loss: 0.4246\n",
      "Epoch [44/50], Step [830/901], Loss: 0.5327\n",
      "Epoch [44/50], Step [840/901], Loss: 0.4776\n",
      "Epoch [44/50], Step [850/901], Loss: 0.4320\n",
      "Epoch [44/50], Step [860/901], Loss: 0.4439\n",
      "Epoch [44/50], Step [870/901], Loss: 0.4377\n",
      "Epoch [44/50], Step [880/901], Loss: 0.4396\n",
      "Epoch [44/50], Step [890/901], Loss: 0.4364\n",
      "Epoch [44/50], Step [900/901], Loss: 0.4668\n",
      "\n",
      "train loss: 0.4464, train acc: 88.5256\n",
      "validation loss: 0.7046, validation acc: 97.9112\n",
      "\n",
      "Epoch 45\n",
      "\n",
      "Epoch [45/50], Step [0/901], Loss: 0.4882\n",
      "Epoch [45/50], Step [10/901], Loss: 0.4421\n",
      "Epoch [45/50], Step [20/901], Loss: 0.4302\n",
      "Epoch [45/50], Step [30/901], Loss: 0.4184\n",
      "Epoch [45/50], Step [40/901], Loss: 0.4706\n",
      "Epoch [45/50], Step [50/901], Loss: 0.4406\n",
      "Epoch [45/50], Step [60/901], Loss: 0.4220\n",
      "Epoch [45/50], Step [70/901], Loss: 0.4361\n",
      "Epoch [45/50], Step [80/901], Loss: 0.4089\n",
      "Epoch [45/50], Step [90/901], Loss: 0.3976\n",
      "Epoch [45/50], Step [100/901], Loss: 0.4253\n",
      "Epoch [45/50], Step [110/901], Loss: 0.4442\n",
      "Epoch [45/50], Step [120/901], Loss: 0.4003\n",
      "Epoch [45/50], Step [130/901], Loss: 0.4027\n",
      "Epoch [45/50], Step [140/901], Loss: 0.3999\n",
      "Epoch [45/50], Step [150/901], Loss: 0.4493\n",
      "Epoch [45/50], Step [160/901], Loss: 0.4338\n",
      "Epoch [45/50], Step [170/901], Loss: 0.4678\n",
      "Epoch [45/50], Step [180/901], Loss: 0.3964\n",
      "Epoch [45/50], Step [190/901], Loss: 0.5088\n",
      "Epoch [45/50], Step [200/901], Loss: 0.4387\n",
      "Epoch [45/50], Step [210/901], Loss: 0.4496\n",
      "Epoch [45/50], Step [220/901], Loss: 0.4117\n",
      "Epoch [45/50], Step [230/901], Loss: 0.4746\n",
      "Epoch [45/50], Step [240/901], Loss: 0.4319\n",
      "Epoch [45/50], Step [250/901], Loss: 0.4632\n",
      "Epoch [45/50], Step [260/901], Loss: 0.4090\n",
      "Epoch [45/50], Step [270/901], Loss: 0.3909\n",
      "Epoch [45/50], Step [280/901], Loss: 0.4238\n",
      "Epoch [45/50], Step [290/901], Loss: 0.4478\n",
      "Epoch [45/50], Step [300/901], Loss: 0.4297\n",
      "Epoch [45/50], Step [310/901], Loss: 0.4765\n",
      "Epoch [45/50], Step [320/901], Loss: 0.4278\n",
      "Epoch [45/50], Step [330/901], Loss: 0.4368\n",
      "Epoch [45/50], Step [340/901], Loss: 0.4561\n",
      "Epoch [45/50], Step [350/901], Loss: 0.4703\n",
      "Epoch [45/50], Step [360/901], Loss: 0.4599\n",
      "Epoch [45/50], Step [370/901], Loss: 0.4179\n",
      "Epoch [45/50], Step [380/901], Loss: 0.4632\n",
      "Epoch [45/50], Step [390/901], Loss: 0.4255\n",
      "Epoch [45/50], Step [400/901], Loss: 0.4010\n",
      "Epoch [45/50], Step [410/901], Loss: 0.4895\n",
      "Epoch [45/50], Step [420/901], Loss: 0.4689\n",
      "Epoch [45/50], Step [430/901], Loss: 0.4537\n",
      "Epoch [45/50], Step [440/901], Loss: 0.4290\n",
      "Epoch [45/50], Step [450/901], Loss: 0.3970\n",
      "Epoch [45/50], Step [460/901], Loss: 0.4514\n",
      "Epoch [45/50], Step [470/901], Loss: 0.4686\n",
      "Epoch [45/50], Step [480/901], Loss: 0.4093\n",
      "Epoch [45/50], Step [490/901], Loss: 0.4724\n",
      "Epoch [45/50], Step [500/901], Loss: 0.4240\n",
      "Epoch [45/50], Step [510/901], Loss: 0.4196\n",
      "Epoch [45/50], Step [520/901], Loss: 0.4662\n",
      "Epoch [45/50], Step [530/901], Loss: 0.4205\n",
      "Epoch [45/50], Step [540/901], Loss: 0.4475\n",
      "Epoch [45/50], Step [550/901], Loss: 0.4261\n",
      "Epoch [45/50], Step [560/901], Loss: 0.4578\n",
      "Epoch [45/50], Step [570/901], Loss: 0.4032\n",
      "Epoch [45/50], Step [580/901], Loss: 0.4460\n",
      "Epoch [45/50], Step [590/901], Loss: 0.4168\n",
      "Epoch [45/50], Step [600/901], Loss: 0.4268\n",
      "Epoch [45/50], Step [610/901], Loss: 0.4553\n",
      "Epoch [45/50], Step [620/901], Loss: 0.4174\n",
      "Epoch [45/50], Step [630/901], Loss: 0.4419\n",
      "Epoch [45/50], Step [640/901], Loss: 0.4394\n",
      "Epoch [45/50], Step [650/901], Loss: 0.4527\n",
      "Epoch [45/50], Step [660/901], Loss: 0.4104\n",
      "Epoch [45/50], Step [670/901], Loss: 0.4258\n",
      "Epoch [45/50], Step [680/901], Loss: 0.4498\n",
      "Epoch [45/50], Step [690/901], Loss: 0.4279\n",
      "Epoch [45/50], Step [700/901], Loss: 0.4536\n",
      "Epoch [45/50], Step [710/901], Loss: 0.4175\n",
      "Epoch [45/50], Step [720/901], Loss: 0.4226\n",
      "Epoch [45/50], Step [730/901], Loss: 0.4393\n",
      "Epoch [45/50], Step [740/901], Loss: 0.3877\n",
      "Epoch [45/50], Step [750/901], Loss: 0.4515\n",
      "Epoch [45/50], Step [760/901], Loss: 0.3951\n",
      "Epoch [45/50], Step [770/901], Loss: 0.4198\n",
      "Epoch [45/50], Step [780/901], Loss: 0.4428\n",
      "Epoch [45/50], Step [790/901], Loss: 0.4583\n",
      "Epoch [45/50], Step [800/901], Loss: 0.4735\n",
      "Epoch [45/50], Step [810/901], Loss: 0.4770\n",
      "Epoch [45/50], Step [820/901], Loss: 0.4602\n",
      "Epoch [45/50], Step [830/901], Loss: 0.4311\n",
      "Epoch [45/50], Step [840/901], Loss: 0.3958\n",
      "Epoch [45/50], Step [850/901], Loss: 0.4056\n",
      "Epoch [45/50], Step [860/901], Loss: 0.4584\n",
      "Epoch [45/50], Step [870/901], Loss: 0.4925\n",
      "Epoch [45/50], Step [880/901], Loss: 0.4299\n",
      "Epoch [45/50], Step [890/901], Loss: 0.4308\n",
      "Epoch [45/50], Step [900/901], Loss: 0.4257\n",
      "\n",
      "train loss: 0.4462, train acc: 88.5789\n",
      "validation loss: 0.7044, validation acc: 97.9513\n",
      "\n",
      "Epoch 46\n",
      "\n",
      "Epoch [46/50], Step [0/901], Loss: 0.4289\n",
      "Epoch [46/50], Step [10/901], Loss: 0.4366\n",
      "Epoch [46/50], Step [20/901], Loss: 0.3959\n",
      "Epoch [46/50], Step [30/901], Loss: 0.4024\n",
      "Epoch [46/50], Step [40/901], Loss: 0.4444\n",
      "Epoch [46/50], Step [50/901], Loss: 0.4595\n",
      "Epoch [46/50], Step [60/901], Loss: 0.4112\n",
      "Epoch [46/50], Step [70/901], Loss: 0.4802\n",
      "Epoch [46/50], Step [80/901], Loss: 0.4703\n",
      "Epoch [46/50], Step [90/901], Loss: 0.4487\n",
      "Epoch [46/50], Step [100/901], Loss: 0.3829\n",
      "Epoch [46/50], Step [110/901], Loss: 0.4360\n",
      "Epoch [46/50], Step [120/901], Loss: 0.4535\n",
      "Epoch [46/50], Step [130/901], Loss: 0.4788\n",
      "Epoch [46/50], Step [140/901], Loss: 0.4249\n",
      "Epoch [46/50], Step [150/901], Loss: 0.4435\n",
      "Epoch [46/50], Step [160/901], Loss: 0.4311\n",
      "Epoch [46/50], Step [170/901], Loss: 0.4256\n",
      "Epoch [46/50], Step [180/901], Loss: 0.4567\n",
      "Epoch [46/50], Step [190/901], Loss: 0.4393\n",
      "Epoch [46/50], Step [200/901], Loss: 0.4398\n",
      "Epoch [46/50], Step [210/901], Loss: 0.4582\n",
      "Epoch [46/50], Step [220/901], Loss: 0.4656\n",
      "Epoch [46/50], Step [230/901], Loss: 0.4478\n",
      "Epoch [46/50], Step [240/901], Loss: 0.4282\n",
      "Epoch [46/50], Step [250/901], Loss: 0.4026\n",
      "Epoch [46/50], Step [260/901], Loss: 0.4811\n",
      "Epoch [46/50], Step [270/901], Loss: 0.4465\n",
      "Epoch [46/50], Step [280/901], Loss: 0.4410\n",
      "Epoch [46/50], Step [290/901], Loss: 0.4266\n",
      "Epoch [46/50], Step [300/901], Loss: 0.4700\n",
      "Epoch [46/50], Step [310/901], Loss: 0.4477\n",
      "Epoch [46/50], Step [320/901], Loss: 0.4413\n",
      "Epoch [46/50], Step [330/901], Loss: 0.3983\n",
      "Epoch [46/50], Step [340/901], Loss: 0.4248\n",
      "Epoch [46/50], Step [350/901], Loss: 0.4505\n",
      "Epoch [46/50], Step [360/901], Loss: 0.4480\n",
      "Epoch [46/50], Step [370/901], Loss: 0.4469\n",
      "Epoch [46/50], Step [380/901], Loss: 0.3965\n",
      "Epoch [46/50], Step [390/901], Loss: 0.4534\n",
      "Epoch [46/50], Step [400/901], Loss: 0.4377\n",
      "Epoch [46/50], Step [410/901], Loss: 0.4467\n",
      "Epoch [46/50], Step [420/901], Loss: 0.4571\n",
      "Epoch [46/50], Step [430/901], Loss: 0.4475\n",
      "Epoch [46/50], Step [440/901], Loss: 0.4514\n",
      "Epoch [46/50], Step [450/901], Loss: 0.4674\n",
      "Epoch [46/50], Step [460/901], Loss: 0.4232\n",
      "Epoch [46/50], Step [470/901], Loss: 0.4049\n",
      "Epoch [46/50], Step [480/901], Loss: 0.4459\n",
      "Epoch [46/50], Step [490/901], Loss: 0.4363\n",
      "Epoch [46/50], Step [500/901], Loss: 0.4114\n",
      "Epoch [46/50], Step [510/901], Loss: 0.4235\n",
      "Epoch [46/50], Step [520/901], Loss: 0.4239\n",
      "Epoch [46/50], Step [530/901], Loss: 0.4093\n",
      "Epoch [46/50], Step [540/901], Loss: 0.3851\n",
      "Epoch [46/50], Step [550/901], Loss: 0.4786\n",
      "Epoch [46/50], Step [560/901], Loss: 0.4807\n",
      "Epoch [46/50], Step [570/901], Loss: 0.4099\n",
      "Epoch [46/50], Step [580/901], Loss: 0.3949\n",
      "Epoch [46/50], Step [590/901], Loss: 0.4174\n",
      "Epoch [46/50], Step [600/901], Loss: 0.4309\n",
      "Epoch [46/50], Step [610/901], Loss: 0.4508\n",
      "Epoch [46/50], Step [620/901], Loss: 0.4111\n",
      "Epoch [46/50], Step [630/901], Loss: 0.4717\n",
      "Epoch [46/50], Step [640/901], Loss: 0.4597\n",
      "Epoch [46/50], Step [650/901], Loss: 0.4513\n",
      "Epoch [46/50], Step [660/901], Loss: 0.4725\n",
      "Epoch [46/50], Step [670/901], Loss: 0.4302\n",
      "Epoch [46/50], Step [680/901], Loss: 0.4027\n",
      "Epoch [46/50], Step [690/901], Loss: 0.4114\n",
      "Epoch [46/50], Step [700/901], Loss: 0.4283\n",
      "Epoch [46/50], Step [710/901], Loss: 0.4256\n",
      "Epoch [46/50], Step [720/901], Loss: 0.4820\n",
      "Epoch [46/50], Step [730/901], Loss: 0.4740\n",
      "Epoch [46/50], Step [740/901], Loss: 0.4695\n",
      "Epoch [46/50], Step [750/901], Loss: 0.4192\n",
      "Epoch [46/50], Step [760/901], Loss: 0.4445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/50], Step [770/901], Loss: 0.4136\n",
      "Epoch [46/50], Step [780/901], Loss: 0.4622\n",
      "Epoch [46/50], Step [790/901], Loss: 0.4236\n",
      "Epoch [46/50], Step [800/901], Loss: 0.4344\n",
      "Epoch [46/50], Step [810/901], Loss: 0.4371\n",
      "Epoch [46/50], Step [820/901], Loss: 0.4297\n",
      "Epoch [46/50], Step [830/901], Loss: 0.4049\n",
      "Epoch [46/50], Step [840/901], Loss: 0.4178\n",
      "Epoch [46/50], Step [850/901], Loss: 0.4226\n",
      "Epoch [46/50], Step [860/901], Loss: 0.4271\n",
      "Epoch [46/50], Step [870/901], Loss: 0.4334\n",
      "Epoch [46/50], Step [880/901], Loss: 0.4693\n",
      "Epoch [46/50], Step [890/901], Loss: 0.4725\n",
      "Epoch [46/50], Step [900/901], Loss: 0.4387\n",
      "\n",
      "train loss: 0.4460, train acc: 88.5260\n",
      "validation loss: 0.7042, validation acc: 97.8512\n",
      "\n",
      "Epoch 47\n",
      "\n",
      "Epoch [47/50], Step [0/901], Loss: 0.4273\n",
      "Epoch [47/50], Step [10/901], Loss: 0.4540\n",
      "Epoch [47/50], Step [20/901], Loss: 0.4111\n",
      "Epoch [47/50], Step [30/901], Loss: 0.4712\n",
      "Epoch [47/50], Step [40/901], Loss: 0.4139\n",
      "Epoch [47/50], Step [50/901], Loss: 0.4358\n",
      "Epoch [47/50], Step [60/901], Loss: 0.3937\n",
      "Epoch [47/50], Step [70/901], Loss: 0.4048\n",
      "Epoch [47/50], Step [80/901], Loss: 0.3974\n",
      "Epoch [47/50], Step [90/901], Loss: 0.4863\n",
      "Epoch [47/50], Step [100/901], Loss: 0.4497\n",
      "Epoch [47/50], Step [110/901], Loss: 0.4331\n",
      "Epoch [47/50], Step [120/901], Loss: 0.4167\n",
      "Epoch [47/50], Step [130/901], Loss: 0.4310\n",
      "Epoch [47/50], Step [140/901], Loss: 0.4173\n",
      "Epoch [47/50], Step [150/901], Loss: 0.3692\n",
      "Epoch [47/50], Step [160/901], Loss: 0.4376\n",
      "Epoch [47/50], Step [170/901], Loss: 0.4268\n",
      "Epoch [47/50], Step [180/901], Loss: 0.4471\n",
      "Epoch [47/50], Step [190/901], Loss: 0.4372\n",
      "Epoch [47/50], Step [200/901], Loss: 0.4196\n",
      "Epoch [47/50], Step [210/901], Loss: 0.4461\n",
      "Epoch [47/50], Step [220/901], Loss: 0.4302\n",
      "Epoch [47/50], Step [230/901], Loss: 0.4754\n",
      "Epoch [47/50], Step [240/901], Loss: 0.4236\n",
      "Epoch [47/50], Step [250/901], Loss: 0.4160\n",
      "Epoch [47/50], Step [260/901], Loss: 0.4165\n",
      "Epoch [47/50], Step [270/901], Loss: 0.4493\n",
      "Epoch [47/50], Step [280/901], Loss: 0.4792\n",
      "Epoch [47/50], Step [290/901], Loss: 0.4695\n",
      "Epoch [47/50], Step [300/901], Loss: 0.3900\n",
      "Epoch [47/50], Step [310/901], Loss: 0.4216\n",
      "Epoch [47/50], Step [320/901], Loss: 0.4414\n",
      "Epoch [47/50], Step [330/901], Loss: 0.4466\n",
      "Epoch [47/50], Step [340/901], Loss: 0.4547\n",
      "Epoch [47/50], Step [350/901], Loss: 0.4283\n",
      "Epoch [47/50], Step [360/901], Loss: 0.4365\n",
      "Epoch [47/50], Step [370/901], Loss: 0.4841\n",
      "Epoch [47/50], Step [380/901], Loss: 0.4409\n",
      "Epoch [47/50], Step [390/901], Loss: 0.4360\n",
      "Epoch [47/50], Step [400/901], Loss: 0.4785\n",
      "Epoch [47/50], Step [410/901], Loss: 0.4324\n",
      "Epoch [47/50], Step [420/901], Loss: 0.4564\n",
      "Epoch [47/50], Step [430/901], Loss: 0.4691\n",
      "Epoch [47/50], Step [440/901], Loss: 0.4653\n",
      "Epoch [47/50], Step [450/901], Loss: 0.4349\n",
      "Epoch [47/50], Step [460/901], Loss: 0.4484\n",
      "Epoch [47/50], Step [470/901], Loss: 0.4431\n",
      "Epoch [47/50], Step [480/901], Loss: 0.4144\n",
      "Epoch [47/50], Step [490/901], Loss: 0.4331\n",
      "Epoch [47/50], Step [500/901], Loss: 0.4202\n",
      "Epoch [47/50], Step [510/901], Loss: 0.4558\n",
      "Epoch [47/50], Step [520/901], Loss: 0.4397\n",
      "Epoch [47/50], Step [530/901], Loss: 0.4220\n",
      "Epoch [47/50], Step [540/901], Loss: 0.4138\n",
      "Epoch [47/50], Step [550/901], Loss: 0.4387\n",
      "Epoch [47/50], Step [560/901], Loss: 0.3886\n",
      "Epoch [47/50], Step [570/901], Loss: 0.4289\n",
      "Epoch [47/50], Step [580/901], Loss: 0.4270\n",
      "Epoch [47/50], Step [590/901], Loss: 0.4052\n",
      "Epoch [47/50], Step [600/901], Loss: 0.4023\n",
      "Epoch [47/50], Step [610/901], Loss: 0.4009\n",
      "Epoch [47/50], Step [620/901], Loss: 0.4480\n",
      "Epoch [47/50], Step [630/901], Loss: 0.4363\n",
      "Epoch [47/50], Step [640/901], Loss: 0.4805\n",
      "Epoch [47/50], Step [650/901], Loss: 0.4286\n",
      "Epoch [47/50], Step [660/901], Loss: 0.4766\n",
      "Epoch [47/50], Step [670/901], Loss: 0.4089\n",
      "Epoch [47/50], Step [680/901], Loss: 0.4654\n",
      "Epoch [47/50], Step [690/901], Loss: 0.4359\n",
      "Epoch [47/50], Step [700/901], Loss: 0.4134\n",
      "Epoch [47/50], Step [710/901], Loss: 0.4112\n",
      "Epoch [47/50], Step [720/901], Loss: 0.4268\n",
      "Epoch [47/50], Step [730/901], Loss: 0.4568\n",
      "Epoch [47/50], Step [740/901], Loss: 0.4220\n",
      "Epoch [47/50], Step [750/901], Loss: 0.4538\n",
      "Epoch [47/50], Step [760/901], Loss: 0.4767\n",
      "Epoch [47/50], Step [770/901], Loss: 0.4540\n",
      "Epoch [47/50], Step [780/901], Loss: 0.4234\n",
      "Epoch [47/50], Step [790/901], Loss: 0.4719\n",
      "Epoch [47/50], Step [800/901], Loss: 0.4308\n",
      "Epoch [47/50], Step [810/901], Loss: 0.4885\n",
      "Epoch [47/50], Step [820/901], Loss: 0.4325\n",
      "Epoch [47/50], Step [830/901], Loss: 0.4612\n",
      "Epoch [47/50], Step [840/901], Loss: 0.3941\n",
      "Epoch [47/50], Step [850/901], Loss: 0.4463\n",
      "Epoch [47/50], Step [860/901], Loss: 0.4117\n",
      "Epoch [47/50], Step [870/901], Loss: 0.4715\n",
      "Epoch [47/50], Step [880/901], Loss: 0.4311\n",
      "Epoch [47/50], Step [890/901], Loss: 0.4336\n",
      "Epoch [47/50], Step [900/901], Loss: 0.4701\n",
      "\n",
      "train loss: 0.4458, train acc: 88.5681\n",
      "validation loss: 0.7043, validation acc: 97.6710\n",
      "\n",
      "Epoch 48\n",
      "\n",
      "Epoch [48/50], Step [0/901], Loss: 0.4369\n",
      "Epoch [48/50], Step [10/901], Loss: 0.4570\n",
      "Epoch [48/50], Step [20/901], Loss: 0.4445\n",
      "Epoch [48/50], Step [30/901], Loss: 0.4224\n",
      "Epoch [48/50], Step [40/901], Loss: 0.4350\n",
      "Epoch [48/50], Step [50/901], Loss: 0.4954\n",
      "Epoch [48/50], Step [60/901], Loss: 0.4716\n",
      "Epoch [48/50], Step [70/901], Loss: 0.4673\n",
      "Epoch [48/50], Step [80/901], Loss: 0.4368\n",
      "Epoch [48/50], Step [90/901], Loss: 0.4623\n",
      "Epoch [48/50], Step [100/901], Loss: 0.4815\n",
      "Epoch [48/50], Step [110/901], Loss: 0.4564\n",
      "Epoch [48/50], Step [120/901], Loss: 0.4340\n",
      "Epoch [48/50], Step [130/901], Loss: 0.4485\n",
      "Epoch [48/50], Step [140/901], Loss: 0.4062\n",
      "Epoch [48/50], Step [150/901], Loss: 0.4402\n",
      "Epoch [48/50], Step [160/901], Loss: 0.4357\n",
      "Epoch [48/50], Step [170/901], Loss: 0.4299\n",
      "Epoch [48/50], Step [180/901], Loss: 0.3963\n",
      "Epoch [48/50], Step [190/901], Loss: 0.4352\n",
      "Epoch [48/50], Step [200/901], Loss: 0.4615\n",
      "Epoch [48/50], Step [210/901], Loss: 0.4407\n",
      "Epoch [48/50], Step [220/901], Loss: 0.4552\n",
      "Epoch [48/50], Step [230/901], Loss: 0.4431\n",
      "Epoch [48/50], Step [240/901], Loss: 0.4498\n",
      "Epoch [48/50], Step [250/901], Loss: 0.4583\n",
      "Epoch [48/50], Step [260/901], Loss: 0.4353\n",
      "Epoch [48/50], Step [270/901], Loss: 0.4557\n",
      "Epoch [48/50], Step [280/901], Loss: 0.4022\n",
      "Epoch [48/50], Step [290/901], Loss: 0.4389\n",
      "Epoch [48/50], Step [300/901], Loss: 0.4031\n",
      "Epoch [48/50], Step [310/901], Loss: 0.4136\n",
      "Epoch [48/50], Step [320/901], Loss: 0.4203\n",
      "Epoch [48/50], Step [330/901], Loss: 0.4199\n",
      "Epoch [48/50], Step [340/901], Loss: 0.4535\n",
      "Epoch [48/50], Step [350/901], Loss: 0.4454\n",
      "Epoch [48/50], Step [360/901], Loss: 0.4237\n",
      "Epoch [48/50], Step [370/901], Loss: 0.4494\n",
      "Epoch [48/50], Step [380/901], Loss: 0.3958\n",
      "Epoch [48/50], Step [390/901], Loss: 0.4356\n",
      "Epoch [48/50], Step [400/901], Loss: 0.4291\n",
      "Epoch [48/50], Step [410/901], Loss: 0.4540\n",
      "Epoch [48/50], Step [420/901], Loss: 0.4005\n",
      "Epoch [48/50], Step [430/901], Loss: 0.4358\n",
      "Epoch [48/50], Step [440/901], Loss: 0.4816\n",
      "Epoch [48/50], Step [450/901], Loss: 0.4307\n",
      "Epoch [48/50], Step [460/901], Loss: 0.4708\n",
      "Epoch [48/50], Step [470/901], Loss: 0.4250\n",
      "Epoch [48/50], Step [480/901], Loss: 0.3835\n",
      "Epoch [48/50], Step [490/901], Loss: 0.4145\n",
      "Epoch [48/50], Step [500/901], Loss: 0.4325\n",
      "Epoch [48/50], Step [510/901], Loss: 0.4058\n",
      "Epoch [48/50], Step [520/901], Loss: 0.4178\n",
      "Epoch [48/50], Step [530/901], Loss: 0.4032\n",
      "Epoch [48/50], Step [540/901], Loss: 0.4350\n",
      "Epoch [48/50], Step [550/901], Loss: 0.4133\n",
      "Epoch [48/50], Step [560/901], Loss: 0.4380\n",
      "Epoch [48/50], Step [570/901], Loss: 0.4513\n",
      "Epoch [48/50], Step [580/901], Loss: 0.4421\n",
      "Epoch [48/50], Step [590/901], Loss: 0.3922\n",
      "Epoch [48/50], Step [600/901], Loss: 0.4773\n",
      "Epoch [48/50], Step [610/901], Loss: 0.4842\n",
      "Epoch [48/50], Step [620/901], Loss: 0.4590\n",
      "Epoch [48/50], Step [630/901], Loss: 0.4561\n",
      "Epoch [48/50], Step [640/901], Loss: 0.4265\n",
      "Epoch [48/50], Step [650/901], Loss: 0.4554\n",
      "Epoch [48/50], Step [660/901], Loss: 0.4362\n",
      "Epoch [48/50], Step [670/901], Loss: 0.4497\n",
      "Epoch [48/50], Step [680/901], Loss: 0.4618\n",
      "Epoch [48/50], Step [690/901], Loss: 0.4301\n",
      "Epoch [48/50], Step [700/901], Loss: 0.4268\n",
      "Epoch [48/50], Step [710/901], Loss: 0.4431\n",
      "Epoch [48/50], Step [720/901], Loss: 0.4588\n",
      "Epoch [48/50], Step [730/901], Loss: 0.4669\n",
      "Epoch [48/50], Step [740/901], Loss: 0.4585\n",
      "Epoch [48/50], Step [750/901], Loss: 0.4188\n",
      "Epoch [48/50], Step [760/901], Loss: 0.4485\n",
      "Epoch [48/50], Step [770/901], Loss: 0.4509\n",
      "Epoch [48/50], Step [780/901], Loss: 0.4155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/50], Step [790/901], Loss: 0.4257\n",
      "Epoch [48/50], Step [800/901], Loss: 0.4567\n",
      "Epoch [48/50], Step [810/901], Loss: 0.4243\n",
      "Epoch [48/50], Step [820/901], Loss: 0.4463\n",
      "Epoch [48/50], Step [830/901], Loss: 0.4246\n",
      "Epoch [48/50], Step [840/901], Loss: 0.4125\n",
      "Epoch [48/50], Step [850/901], Loss: 0.4540\n",
      "Epoch [48/50], Step [860/901], Loss: 0.4435\n",
      "Epoch [48/50], Step [870/901], Loss: 0.4506\n",
      "Epoch [48/50], Step [880/901], Loss: 0.4444\n",
      "Epoch [48/50], Step [890/901], Loss: 0.3926\n",
      "Epoch [48/50], Step [900/901], Loss: 0.4014\n",
      "\n",
      "train loss: 0.4456, train acc: 88.6262\n",
      "validation loss: 0.7043, validation acc: 97.6710\n",
      "\n",
      "Epoch 49\n",
      "\n",
      "Epoch [49/50], Step [0/901], Loss: 0.4052\n",
      "Epoch [49/50], Step [10/901], Loss: 0.4504\n",
      "Epoch [49/50], Step [20/901], Loss: 0.4416\n",
      "Epoch [49/50], Step [30/901], Loss: 0.4179\n",
      "Epoch [49/50], Step [40/901], Loss: 0.4318\n",
      "Epoch [49/50], Step [50/901], Loss: 0.4277\n",
      "Epoch [49/50], Step [60/901], Loss: 0.4285\n",
      "Epoch [49/50], Step [70/901], Loss: 0.4202\n",
      "Epoch [49/50], Step [80/901], Loss: 0.4017\n",
      "Epoch [49/50], Step [90/901], Loss: 0.5135\n",
      "Epoch [49/50], Step [100/901], Loss: 0.4351\n",
      "Epoch [49/50], Step [110/901], Loss: 0.4665\n",
      "Epoch [49/50], Step [120/901], Loss: 0.4218\n",
      "Epoch [49/50], Step [130/901], Loss: 0.4192\n",
      "Epoch [49/50], Step [140/901], Loss: 0.4768\n",
      "Epoch [49/50], Step [150/901], Loss: 0.4885\n",
      "Epoch [49/50], Step [160/901], Loss: 0.4217\n",
      "Epoch [49/50], Step [170/901], Loss: 0.4445\n",
      "Epoch [49/50], Step [180/901], Loss: 0.4380\n",
      "Epoch [49/50], Step [190/901], Loss: 0.4234\n",
      "Epoch [49/50], Step [200/901], Loss: 0.4551\n",
      "Epoch [49/50], Step [210/901], Loss: 0.4421\n",
      "Epoch [49/50], Step [220/901], Loss: 0.4246\n",
      "Epoch [49/50], Step [230/901], Loss: 0.4325\n",
      "Epoch [49/50], Step [240/901], Loss: 0.4273\n",
      "Epoch [49/50], Step [250/901], Loss: 0.4579\n",
      "Epoch [49/50], Step [260/901], Loss: 0.4359\n",
      "Epoch [49/50], Step [270/901], Loss: 0.4544\n",
      "Epoch [49/50], Step [280/901], Loss: 0.4333\n",
      "Epoch [49/50], Step [290/901], Loss: 0.4302\n",
      "Epoch [49/50], Step [300/901], Loss: 0.4283\n",
      "Epoch [49/50], Step [310/901], Loss: 0.4380\n",
      "Epoch [49/50], Step [320/901], Loss: 0.4130\n",
      "Epoch [49/50], Step [330/901], Loss: 0.4157\n",
      "Epoch [49/50], Step [340/901], Loss: 0.4347\n",
      "Epoch [49/50], Step [350/901], Loss: 0.4343\n",
      "Epoch [49/50], Step [360/901], Loss: 0.4614\n",
      "Epoch [49/50], Step [370/901], Loss: 0.4271\n",
      "Epoch [49/50], Step [380/901], Loss: 0.4558\n",
      "Epoch [49/50], Step [390/901], Loss: 0.4257\n",
      "Epoch [49/50], Step [400/901], Loss: 0.5055\n",
      "Epoch [49/50], Step [410/901], Loss: 0.4585\n",
      "Epoch [49/50], Step [420/901], Loss: 0.4093\n",
      "Epoch [49/50], Step [430/901], Loss: 0.4062\n",
      "Epoch [49/50], Step [440/901], Loss: 0.4320\n",
      "Epoch [49/50], Step [450/901], Loss: 0.4056\n",
      "Epoch [49/50], Step [460/901], Loss: 0.4079\n",
      "Epoch [49/50], Step [470/901], Loss: 0.4520\n",
      "Epoch [49/50], Step [480/901], Loss: 0.4729\n",
      "Epoch [49/50], Step [490/901], Loss: 0.4187\n",
      "Epoch [49/50], Step [500/901], Loss: 0.3836\n",
      "Epoch [49/50], Step [510/901], Loss: 0.3995\n",
      "Epoch [49/50], Step [520/901], Loss: 0.4070\n",
      "Epoch [49/50], Step [530/901], Loss: 0.4665\n",
      "Epoch [49/50], Step [540/901], Loss: 0.4386\n",
      "Epoch [49/50], Step [550/901], Loss: 0.4460\n",
      "Epoch [49/50], Step [560/901], Loss: 0.4444\n",
      "Epoch [49/50], Step [570/901], Loss: 0.3977\n",
      "Epoch [49/50], Step [580/901], Loss: 0.4266\n",
      "Epoch [49/50], Step [590/901], Loss: 0.4173\n",
      "Epoch [49/50], Step [600/901], Loss: 0.4044\n",
      "Epoch [49/50], Step [610/901], Loss: 0.4229\n",
      "Epoch [49/50], Step [620/901], Loss: 0.4420\n",
      "Epoch [49/50], Step [630/901], Loss: 0.4282\n",
      "Epoch [49/50], Step [640/901], Loss: 0.3855\n",
      "Epoch [49/50], Step [650/901], Loss: 0.4612\n",
      "Epoch [49/50], Step [660/901], Loss: 0.4087\n",
      "Epoch [49/50], Step [670/901], Loss: 0.4300\n",
      "Epoch [49/50], Step [680/901], Loss: 0.4624\n",
      "Epoch [49/50], Step [690/901], Loss: 0.4640\n",
      "Epoch [49/50], Step [700/901], Loss: 0.4273\n",
      "Epoch [49/50], Step [710/901], Loss: 0.4451\n",
      "Epoch [49/50], Step [720/901], Loss: 0.4681\n",
      "Epoch [49/50], Step [730/901], Loss: 0.4078\n",
      "Epoch [49/50], Step [740/901], Loss: 0.4217\n",
      "Epoch [49/50], Step [750/901], Loss: 0.4205\n",
      "Epoch [49/50], Step [760/901], Loss: 0.4373\n",
      "Epoch [49/50], Step [770/901], Loss: 0.4337\n",
      "Epoch [49/50], Step [780/901], Loss: 0.4267\n",
      "Epoch [49/50], Step [790/901], Loss: 0.4656\n",
      "Epoch [49/50], Step [800/901], Loss: 0.4328\n",
      "Epoch [49/50], Step [810/901], Loss: 0.4296\n",
      "Epoch [49/50], Step [820/901], Loss: 0.4175\n",
      "Epoch [49/50], Step [830/901], Loss: 0.4239\n",
      "Epoch [49/50], Step [840/901], Loss: 0.4223\n",
      "Epoch [49/50], Step [850/901], Loss: 0.4514\n",
      "Epoch [49/50], Step [860/901], Loss: 0.4016\n",
      "Epoch [49/50], Step [870/901], Loss: 0.4124\n",
      "Epoch [49/50], Step [880/901], Loss: 0.4353\n",
      "Epoch [49/50], Step [890/901], Loss: 0.4009\n",
      "Epoch [49/50], Step [900/901], Loss: 0.4405\n",
      "\n",
      "train loss: 0.4454, train acc: 88.6218\n",
      "validation loss: 0.7044, validation acc: 97.6243\n",
      "\n",
      "Epoch 50\n",
      "\n",
      "Epoch [50/50], Step [0/901], Loss: 0.4151\n",
      "Epoch [50/50], Step [10/901], Loss: 0.4797\n",
      "Epoch [50/50], Step [20/901], Loss: 0.4491\n",
      "Epoch [50/50], Step [30/901], Loss: 0.4344\n",
      "Epoch [50/50], Step [40/901], Loss: 0.4323\n",
      "Epoch [50/50], Step [50/901], Loss: 0.4435\n",
      "Epoch [50/50], Step [60/901], Loss: 0.4586\n",
      "Epoch [50/50], Step [70/901], Loss: 0.4513\n",
      "Epoch [50/50], Step [80/901], Loss: 0.4154\n",
      "Epoch [50/50], Step [90/901], Loss: 0.4447\n",
      "Epoch [50/50], Step [100/901], Loss: 0.4055\n",
      "Epoch [50/50], Step [110/901], Loss: 0.4344\n",
      "Epoch [50/50], Step [120/901], Loss: 0.4545\n",
      "Epoch [50/50], Step [130/901], Loss: 0.4304\n",
      "Epoch [50/50], Step [140/901], Loss: 0.4654\n",
      "Epoch [50/50], Step [150/901], Loss: 0.4559\n",
      "Epoch [50/50], Step [160/901], Loss: 0.4231\n",
      "Epoch [50/50], Step [170/901], Loss: 0.4441\n",
      "Epoch [50/50], Step [180/901], Loss: 0.4587\n",
      "Epoch [50/50], Step [190/901], Loss: 0.4674\n",
      "Epoch [50/50], Step [200/901], Loss: 0.4442\n",
      "Epoch [50/50], Step [210/901], Loss: 0.4522\n",
      "Epoch [50/50], Step [220/901], Loss: 0.4037\n",
      "Epoch [50/50], Step [230/901], Loss: 0.4651\n",
      "Epoch [50/50], Step [240/901], Loss: 0.4045\n",
      "Epoch [50/50], Step [250/901], Loss: 0.4267\n",
      "Epoch [50/50], Step [260/901], Loss: 0.4385\n",
      "Epoch [50/50], Step [270/901], Loss: 0.4606\n",
      "Epoch [50/50], Step [280/901], Loss: 0.4287\n",
      "Epoch [50/50], Step [290/901], Loss: 0.4546\n",
      "Epoch [50/50], Step [300/901], Loss: 0.3803\n",
      "Epoch [50/50], Step [310/901], Loss: 0.4497\n",
      "Epoch [50/50], Step [320/901], Loss: 0.3673\n",
      "Epoch [50/50], Step [330/901], Loss: 0.4040\n",
      "Epoch [50/50], Step [340/901], Loss: 0.4472\n",
      "Epoch [50/50], Step [350/901], Loss: 0.4645\n",
      "Epoch [50/50], Step [360/901], Loss: 0.4270\n",
      "Epoch [50/50], Step [370/901], Loss: 0.4604\n",
      "Epoch [50/50], Step [380/901], Loss: 0.4161\n",
      "Epoch [50/50], Step [390/901], Loss: 0.4269\n",
      "Epoch [50/50], Step [400/901], Loss: 0.4596\n",
      "Epoch [50/50], Step [410/901], Loss: 0.4229\n",
      "Epoch [50/50], Step [420/901], Loss: 0.4199\n",
      "Epoch [50/50], Step [430/901], Loss: 0.4975\n",
      "Epoch [50/50], Step [440/901], Loss: 0.4016\n",
      "Epoch [50/50], Step [450/901], Loss: 0.4045\n",
      "Epoch [50/50], Step [460/901], Loss: 0.4664\n",
      "Epoch [50/50], Step [470/901], Loss: 0.3878\n",
      "Epoch [50/50], Step [480/901], Loss: 0.4230\n",
      "Epoch [50/50], Step [490/901], Loss: 0.4165\n",
      "Epoch [50/50], Step [500/901], Loss: 0.4275\n",
      "Epoch [50/50], Step [510/901], Loss: 0.4243\n",
      "Epoch [50/50], Step [520/901], Loss: 0.4317\n",
      "Epoch [50/50], Step [530/901], Loss: 0.4370\n",
      "Epoch [50/50], Step [540/901], Loss: 0.4279\n",
      "Epoch [50/50], Step [550/901], Loss: 0.3881\n",
      "Epoch [50/50], Step [560/901], Loss: 0.4065\n",
      "Epoch [50/50], Step [570/901], Loss: 0.4120\n",
      "Epoch [50/50], Step [580/901], Loss: 0.4594\n",
      "Epoch [50/50], Step [590/901], Loss: 0.3959\n",
      "Epoch [50/50], Step [600/901], Loss: 0.4259\n",
      "Epoch [50/50], Step [610/901], Loss: 0.4290\n",
      "Epoch [50/50], Step [620/901], Loss: 0.4399\n",
      "Epoch [50/50], Step [630/901], Loss: 0.4592\n",
      "Epoch [50/50], Step [640/901], Loss: 0.4409\n",
      "Epoch [50/50], Step [650/901], Loss: 0.4067\n",
      "Epoch [50/50], Step [660/901], Loss: 0.4196\n",
      "Epoch [50/50], Step [670/901], Loss: 0.4388\n",
      "Epoch [50/50], Step [680/901], Loss: 0.4314\n",
      "Epoch [50/50], Step [690/901], Loss: 0.4180\n",
      "Epoch [50/50], Step [700/901], Loss: 0.4339\n",
      "Epoch [50/50], Step [710/901], Loss: 0.4237\n",
      "Epoch [50/50], Step [720/901], Loss: 0.4422\n",
      "Epoch [50/50], Step [730/901], Loss: 0.4309\n",
      "Epoch [50/50], Step [740/901], Loss: 0.4289\n",
      "Epoch [50/50], Step [750/901], Loss: 0.4493\n",
      "Epoch [50/50], Step [760/901], Loss: 0.4714\n",
      "Epoch [50/50], Step [770/901], Loss: 0.4470\n",
      "Epoch [50/50], Step [780/901], Loss: 0.4532\n",
      "Epoch [50/50], Step [790/901], Loss: 0.4591\n",
      "Epoch [50/50], Step [800/901], Loss: 0.4215\n",
      "Epoch [50/50], Step [810/901], Loss: 0.4767\n",
      "Epoch [50/50], Step [820/901], Loss: 0.4071\n",
      "Epoch [50/50], Step [830/901], Loss: 0.4014\n",
      "Epoch [50/50], Step [840/901], Loss: 0.4614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50], Step [850/901], Loss: 0.4320\n",
      "Epoch [50/50], Step [860/901], Loss: 0.4927\n",
      "Epoch [50/50], Step [870/901], Loss: 0.4439\n",
      "Epoch [50/50], Step [880/901], Loss: 0.4528\n",
      "Epoch [50/50], Step [890/901], Loss: 0.4533\n",
      "Epoch [50/50], Step [900/901], Loss: 0.4561\n",
      "\n",
      "train loss: 0.4452, train acc: 88.7922\n",
      "validation loss: 0.7042, validation acc: 97.9580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "print_every = 10\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "total_step = len(my_dataloader)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    # scheduler.step(epoch)\n",
    "    correct = 0\n",
    "    total=0\n",
    "    print(f'Epoch {epoch}\\n')\n",
    "    for batch_idx, (data_, target_) in enumerate(my_dataloader):\n",
    "        #data_, target_ = data_.to(device), target_.to(device)# on GPU\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(data_)\n",
    "        loss = criterion(outputs, target_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        #_,pred = torch.max(outputs)#, dim=1)\n",
    "        pred = torch.round(outputs)\n",
    "        correct += torch.sum(pred==target_).item()\n",
    "        total += target_.size(0)\n",
    "        if (batch_idx) % print_every == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain loss: {np.mean(train_loss):.4f}, train acc: {(100 * correct / total):.4f}')\n",
    "    batch_loss = 0\n",
    "    total_t=0\n",
    "    correct_t=0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data_t, target_t in (validation_loader):\n",
    "            #data_t, target_t = data_t.to(device), target_t.to(device)# on GPU\n",
    "            outputs_t = model(data_t)\n",
    "            loss_t = criterion(outputs_t, target_t)\n",
    "            batch_loss += loss_t.item()\n",
    "            pred_t = torch.round(outputs_t)\n",
    "            correct_t += torch.sum(pred_t==target_t).item()\n",
    "            total_t += target_t.size(0)\n",
    "        val_acc.append(100 * correct_t / total_t)\n",
    "        val_loss.append(batch_loss/len(validation_loader))\n",
    "        network_learned = batch_loss < valid_loss_min\n",
    "        print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t / total_t):.4f}\\n')\n",
    "        # Saving the best weight \n",
    "        #if network_learned:\n",
    "        #    valid_loss_min = batch_loss\n",
    "        #    torch.save(model.state_dict(), 'model_classification_tutorial.pt')\n",
    "        #    print('Detected network improvement, saving current model')\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0964edf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f62e081ca30>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.title(\"Train - Validation Accuracy\")\n",
    "plt.plot(train_acc, label='train')\n",
    "plt.plot(val_acc, label='validation')\n",
    "plt.xlabel('num_epochs', fontsize=12)\n",
    "plt.ylabel('accuracy', fontsize=12)\n",
    "plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4bed18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
